# 9장 서포트 벡터 머신

## (예 9.1)

#### 패키지 로드

```{r}
library(kernlab)
```

#### 데이터 로드

```{r}
dat <- read.csv("data/ch9_dat1.csv")
dat$class <- factor(dat$class)
dat
```

#### SVM 학습

```{r}
model <- ksvm(
  class ~ x1 + x2,
  data = dat,
  scaled = FALSE,
  kernel = "vanilladot"
)
```

#### 서포트 벡터 객체

```{r}
model@alphaindex[[1]]
```

#### 최적해: $\alpha$값 (서포트 벡터)

```{r}
model@alpha[[1]]
```

#### 목적함수값

```{r}
-model@obj
```


#### 하이퍼플레인 추정

계수 (w)

```{r}
w <- model@coef[[1]] %*% model@xmatrix[[1]]
w
```

절편 (b)

```{r}
-model@b
```


#### 시각화

```{r}
plot(model, data = dat)
```



## (예 9.2)

#### 패키지 로드

```{r}
library(kernlab)
```


#### 데이터 로드

```{r}
dat <- read.csv("data/ch9_dat2.csv")
dat$class <- factor(dat$class)
dat
```


#### SVM 학습

```{r}
model <- ksvm(
  class ~ x1 + x2,
  data = dat,
  scaled = FALSE,
  kernel = "vanilladot",
  C = 1
)
```


#### 서포트 벡터 객체

```{r}
model@alphaindex[[1]]
```


#### 최적해: $\alpha$값 (서포트 벡터)

```{r}
model@alpha[[1]]
```

#### 목적함수값

```{r}
-model@obj
```

#### 하이퍼플레인 추정

계수 (w)

```{r}
w <- model@coef[[1]] %*% model@xmatrix[[1]]
w
```

절편 (b)

```{r}
-model@b
```

#### 오분류 객체

```{r}
which(model@ymatrix != as.integer(model@fitted))
```

#### 시각화

```{r}
plot(model, data = dat)
```


#### 패널티 단가(C) 변경: 1, 5, 100

```{r}
# Try with C = 1, C = 5 and C = 100
Cs <- c(1, 5, 100)
models <- vector("list", length = length(Cs))

for (i in seq_along(Cs)) {
  message(paste0("-------\nC = ", Cs[i], "\n-------"))

  # SVM with 2nd order polynomial kernel
  models[[i]] <- ksvm(
    class ~ x1 + x2,
    data = dat,
    scaled = FALSE,
    kernel = "vanilladot",
    C = Cs[i]
  )

  # support vectors
  message("Support vectors:")
  print(models[[i]]@alphaindex[[1]])

  # alpha values for support vectors
  message("Alpha values for support vectors:")
  print(round(models[[i]]@alpha[[1]], 4))

  # objective value
  # note that we placed minus(-) sign
  message("Objective value:")
  print(-models[[i]]@obj)

  # hyperplane coefficient vector w and intercept b
  message("Hyperplane coefficients (w):")
  w <- models[[i]]@coef[[1]] %*% models[[i]]@xmatrix[[1]]
  print(w)

  # hyperplane's intercept b
  # Note that we placed minus(-) sign
  message("Hyperplane intercept (b):")
  print(-models[[i]]@b)

  # misclassified objects
  message("Misclassified instances:")
  print(which(models[[i]]@ymatrix != as.integer(models[[i]]@fitted)))
}
```


## (예 9.3)

#### 패키지 로드

```{r}
library(kernlab)
```

#### 데이터 로드

```{r}
dat <- matrix(c(1, 2, 2, 2, 2, -1), nrow = 3, byrow = TRUE)
dat
```

#### 가우시안 커널

```{r}
kernelMatrix(rbfdot(sigma = 1 / 2), dat)
```

#### 이차 커널

```{r}
kernelMatrix(polydot(degree = 2), dat)
```

#### 시그모이드 커널 (하이퍼볼릭 탄젠트 커널)

```{r}
kernelMatrix(tanhdot(offset = 0), dat)
```

#### 선형 커널

```{r}
kernelMatrix(vanilladot(), dat)
```


## (예 9.6)

#### 패키지 로드

```{r}
library(kernlab)
```

#### 데이터 로드

```{r}
dat <- data.frame(
  x1 = c(-1, -1, 1, 1),
  x2 = c(-1, 1, -1, 1),
  class = factor(c(-1, 1, 1, -1))
)
dat
```

#### 이차 커널

```{r}
X <- as.matrix(dat[, 1:2])
K <- kernelMatrix(polydot(degree = 2), X)
K
```

#### SVM 학습

```{r}
model <- ksvm(
  class ~ x1 + x2,
  data = dat,
  scaled = FALSE,
  kernel = polydot(degree = 2)
)
```

#### 서포트 벡터 객체

```{r}
model@alphaindex[[1]]
```


#### 최적해: $\alpha$값 (서포트 벡터)

```{r}
model@alpha[[1]]
```


#### 하이퍼플레인 추정

계수 (beta)

```{r}
beta1 <- 2 * sum(model@coef[[1]] * model@xmatrix[[1]][, "x1"])
beta2 <- 2 * sum(model@coef[[1]] * model@xmatrix[[1]][, "x2"])
beta11 <- sum(model@coef[[1]] * model@xmatrix[[1]][, "x1"]^2)
beta22 <- sum(model@coef[[1]] * model@xmatrix[[1]][, "x2"]^2)
beta12 <- 2 * sum(model@coef[[1]] * apply(model@xmatrix[[1]], 1, prod))
betas <- c(beta1, beta2, beta11, beta22, beta12)
names(betas) <- c("beta1", "beta2", "beta11", "beta22", "beta12")
round(betas, 4)
```


절편 (b)

```{r}
-model@b
```


## (예 9.7)

#### 패키지 로드

```{r}
library(kernlab)
```


#### 데이터 로드

```{r}
dat <- read.csv("data/ch9_dat3.csv")
dat$class <- factor(dat$class)
dat
```


#### SVM 학습

```{r}
model <- ksvm(
  class ~ x1 + x2,
  data = dat,
  scaled = FALSE,
  kernel = polydot(degree = 2),
  C = 1
)
```


#### 서포트 벡터 객체

```{r}
model@alphaindex[[1]]
```


#### 최적해: $\alpha$값 (서포트 벡터)

```{r}
model@alpha[[1]]
```


#### 하이퍼플레인 추정

```{r}
beta1 <- 2 * sum(model@coef[[1]] * model@xmatrix[[1]][, "x1"])
beta2 <- 2 * sum(model@coef[[1]] * model@xmatrix[[1]][, "x2"])
beta11 <- sum(model@coef[[1]] * model@xmatrix[[1]][, "x1"]^2)
beta22 <- sum(model@coef[[1]] * model@xmatrix[[1]][, "x2"]^2)
beta12 <- 2 * sum(model@coef[[1]] * apply(model@xmatrix[[1]], 1, prod))
hyperplane <- c(-model@b, beta1, beta2, beta11, beta22, beta12)
names(hyperplane) <- c("b", "beta1", "beta2", "beta11", "beta22", "beta12")
round(hyperplane, 4)
```

#### 오분류 객체

```{r}
which(model@ymatrix != as.integer(model@fitted))
```


#### 패널티 단가(C) 변경: 1, 5, 100

```{r}
# Try with C = 1, C = 5 and C = 100
Cs <- c(1, 5, 100)
models <- vector("list", length = length(Cs))

for (i in seq_along(Cs)) {
  message(paste0("-------\nC = ", Cs[i], "\n-------"))

  # SVM with 2nd order polynomial kernel
  models[[i]] <- ksvm(
    class ~ x1 + x2,
    data = dat,
    scaled = FALSE,
    kernel = polydot(degree = 2),
    C = Cs[i]
  )

  # support vectors
  message("Support vectors:")
  print(models[[i]]@alphaindex[[1]])

  # alpha values for support vectors
  message("Alpha values for support vectors:")
  print(round(models[[i]]@alpha[[1]], 4))

  # hyperplane
  beta1 <- 2 * sum(models[[i]]@coef[[1]] * models[[i]]@xmatrix[[1]][, "x1"])
  beta2 <- 2 * sum(models[[i]]@coef[[1]] * models[[i]]@xmatrix[[1]][, "x2"])
  beta11 <- sum(models[[i]]@coef[[1]] * models[[i]]@xmatrix[[1]][, "x1"]^2)
  beta22 <- sum(models[[i]]@coef[[1]] * models[[i]]@xmatrix[[1]][, "x2"]^2)
  beta12 <- 2 * sum(models[[i]]@coef[[1]] * apply(models[[i]]@xmatrix[[1]], 1, prod))
  hyperplane <- c(-models[[i]]@b, beta1, beta2, beta11, beta22, beta12)
  names(hyperplane) <- c("b", "beta1", "beta2", "beta11", "beta22", "beta12")
  message("Hyperplane coefficients:")
  print(round(hyperplane, 4))

  # misclassified objects
  message("Misclassified instances:")
  print(which(models[[i]]@ymatrix != as.integer(models[[i]]@fitted)))
}
```


## (예 9.8)

#### 패키지 로드

```{r}
library(kernlab)
library(yardstick)
```

#### 데이터 로드

파일 읽기

```{r}
dat <- read.csv("data/breast-cancer-wisconsin.csv")
dat$class <- factor(dat$class)
```

ID 변수 제외

```{r}
dat <- dat[, -1]
```


결측값 존재 객체 제외

```{r}
dat <- na.omit(dat)
```

학습표본 / 테스트 표본 분리

```{r}
set.seed(578925)
n <- nrow(dat)
train_idx <- sample(1:n, size = n * 2 / 3, replace = FALSE)
train_dat <- dat[train_idx, ]
test_dat <- dat[-train_idx, ]
```


##### SVM (가우시안 커널, $\sigma = 0.5$, $C = 10$)

모형 학습

```{r}
model1 <- ksvm(
  class ~ .,
  data = train_dat,
  kernel = "rbfdot",
  kpar = list(sigma = 0.5),
  C = 10
)

print(model1)
```


학습표본 정오분류표

```{r}
train_results1 <- data.frame(
  class = train_dat$class,
  pred_class = predict(model1, train_dat)
)
conf_mat(train_results1, truth = "class", estimate = "pred_class")
```


테스트 표본 정오분류표 

```{r}
test_results1 <- data.frame(
  class = test_dat$class,
  pred_class = predict(model1, test_dat)
)
conf_mat(test_results1, truth = "class", estimate = "pred_class")
```


##### SVM (가우시안 커널, $\sigma = 2$, $C = 10$)

모형 학습

```{r}
model2 <- ksvm(
  class ~ .,
  data = train_dat,
  kernel = "rbfdot",
  kpar = list(sigma = 2),
  C = 10
)

print(model2)
```

학습표본 정오분류표

```{r}
train_results2 <- data.frame(
  class = train_dat$class,
  pred_class = predict(model2, train_dat)
)
conf_mat(train_results2, truth = "class", estimate = "pred_class")
```


테스트 표본 정오분류표

```{r}
test_results2 <- data.frame(
  class = test_dat$class,
  pred_class = predict(model2, test_dat)
)
conf_mat(test_results2, truth = "class", estimate = "pred_class")
```



