# 10장 앙상블 기법

## (예 10.2 - 10.3)

#### 패키지 로드

```{r}
library(randomForestSRC)
library(yardstick)
```


#### 데이터 로드

```{r}
dat <- read.csv("data/ch10_dat1.csv")
dat$X1 <- factor(dat$X1)
dat$X2 <- factor(dat$X2)
dat$X3 <- factor(dat$X3)
dat$Y <- factor(dat$Y)
```


#### 예 10.2

랜덤 포레스트 학습

- 트리 수: 4
- 각 분지 당 고려 변수 수: 2

```{r}
set.seed(5678)
rf <- rfsrc(
  Y ~ .,
  data = dat,
  ntree = 4, # 4 trees
  mtry = 2, # 2 candidate variables at each split
  samptype = "swr" # sampling with replacement
)
```

각 트리에 사용된 학습 데이터

```{r}
matrix(
  rf$forest$nativeArrayTNDS$tnRMBR,
  ncol = 4
)
```

학습된 트리

```{r}
plot(get.tree(rf, 1))
plot(get.tree(rf, 2))
plot(get.tree(rf, 3))
plot(get.tree(rf, 4))
```


#### 예 10.3

Out-of-bag(OOB) 예측: 확률

```{r}
rf$predicted.oob
```

OOB 예측: 범주

```{r}
rf$class.oob
```

OOB 예측 성능: 정오분류표

```{r}
results <- cbind(dat, pred_class = rf$class.oob)
cm <- conf_mat(results, truth = "Y", estimate = "pred_class")
cm
```

OOB 예측 성능: 여러 척도

```{r}
summary(cm)
```


## (예 10.4)

#### 패키지 로드

```{r}
library(JOUSBoost)
library(rpart)
library(rpart.plot)
```


#### 데이터 로드

```{r}
dat <- read.csv("data/ch10_dat1.csv")
dat$X1 <- factor(dat$X1)
dat$X2 <- factor(dat$X2)
dat$X3 <- factor(dat$X3)
dat$Y <- factor(dat$Y)
```


#### 학습 데이터 변환

- 예측변수 $\mathbf{x} \in \mathbb{R}^d$
- 범주 $y \in {1, -1}$

```{r}
X <- model.matrix(Y ~ . - 1, dat) # create predictor variable matrix
y <- ifelse(dat$Y == "yes", 1, -1)
```

#### 아다부스트 학습

- 트리 수: 3
- 트리 당 분지 수: 1

```{r}
fit <- adaboost(X, y, tree_depth = 1, n_rounds = 3)
```


#### 학습된 트리

```{r}
for (i in seq_along(fit$trees)) {
  rpart.plot(fit$trees[[i]], roundint = FALSE)
}
```


#### 각 트리에 적용되는 가중치 ($\alpha$)

```{r}
fit$alphas
```


#### 예측

확률: $P(y = 1 | x)$

```{r}
posterior1 <- predict(fit, X = X, type = "prob")
round(posterior1, 2)
```

범주: $y \in {1, -1}$

```{r}
pred_class <- predict(fit, X = X, type = "response")
pred_class
```

예측성능 - 정오분류표

```{r}
fit$confusion_matrix
```


## (예 10.5)

#### 패키지 로드

```{r}
library(gbm)
library(rpart)
library(rpart.plot)
```

#### 데이터 로드

```{r}
dat <- read.csv("data/ch10_dat3.csv")
```


#### Approach 1: `{gbm}` 패키지 사용

모형 학습

- 트리 수: 5
- 트리 당 분지 수: 1
- 트리 당 가중치 $alpha$: 1

```{r}
gbm_fit <- gbm(
  Y ~ X,
  data = dat,
  distribution = "gaussian",
  n.trees = 5,
  interaction.depth = 1,
  n.minobsinnode = 1,
  shrinkage = 1, # step size = 1
  bag.fraction = 1 # no subsampling of training data
)
```

모형에 기반한 예측

```{r}
dat_p <- data.frame(
  X = seq(min(dat$X), max(dat$X), length = 1000)
)

plot(dat$X, dat$Y,
     pch = 16, xlab = "X", ylab = "Y",
     main = paste0("Observed vs Prediction: {gbm} package")
)
lines(dat_p$X, predict(gbm_fit, newdata = dat_p), col = "red")
```


#### Approach 2: `{rpart}`와 loop 사용

```{r}
# initial model
dat_m <- dat
dat_m$pred <- mean(dat$Y)
dat_p$pred <- mean(dat$Y)

iter <- 5

for (i in 1:iter) {
  # negative gradients
  dat_m$ngrad <- dat$Y - dat_m$pred

  # plot layout
  layout(matrix(c(1, 2, 3, 3), nrow = 2, byrow = TRUE))

  # plot observed point and prediction
  plot(dat$X, dat$Y,
    pch = 16, xlab = "X", ylab = "Y",
    main = paste0("Observed vs Prediction: Iteration ", i)
  )
  lines(dat_p$X, dat_p$pred, col = "red")

  # plot residuals
  plot(dat_m$X, dat_m$ngrad,
    pch = 16, xlab = "X", ylab = "Residual",
    main = paste0("Residual: Iteration ", i)
  )


  # fit C to negative gradients
  fit <- rpart(
    ngrad ~ X,
    data = dat_m,
    control = rpart.control(maxdepth = 1)
  )

  # plot tree
  rpart.plot(fit, main = paste0("Residual estimation: Iteration ", i))

  # update prediction
  dat_m$pred <- dat_m$pred + predict(fit, newdata = dat_m)
  dat_p$pred <- dat_p$pred + predict(fit, newdata = dat_p)
}

# plot final prediction
par(mfrow = c(1, 1))
plot(dat$X, dat$Y,
  pch = 16, xlab = "X", ylab = "Y",
  main = paste0("Observed vs Prediction: Final")
)
lines(dat_p$X, dat_p$pred, col = "red")
```


## (예 10.6)

#### 패키지 로드

```{r}
library(gbm)
library(yardstick)
library(ggplot2)
```


#### 데이터 로드

```{r}
dat <- read.csv("data/ch8_dat1.csv")
dat$class <- dat$class - 1 # set class to be 0 or 1
```

#### Baseline: 평균 기반 추정

모형 평가: ROC curve, AUC

```{r}
result0 <- data.frame(
  class = factor(dat$class, levels = c(1, 0)),
  posterior1 = rep(0.5, nrow(dat))
)
roc_auc(result0, truth = "class", posterior1)
autoplot(roc_curve(result0, truth = "class", posterior1))
```


#### 트리 1개

모형 추정

```{r}
fit1 <- gbm(
  class ~ x1 + x2,
  data = dat,
  distribution = "bernoulli",
  n.trees = 1,
  interaction.depth = 1,
  n.minobsinnode = 1,
  shrinkage = 1, # step size = 1
  bag.fraction = 1 # no subsampling of training data
)
```

모형 평가: ROC curve, AUC

```{r}
result1 <- data.frame(
  class = factor(dat$class, levels = c(1, 0)),
  posterior1 = predict(fit1, type = "response")
)
roc_auc(result1, truth = "class", posterior1)
autoplot(roc_curve(result1, truth = "class", posterior1))
```


#### 트리 2개

모형 추정

```{r}
fit2 <- gbm(
  class ~ x1 + x2,
  data = dat,
  distribution = "bernoulli",
  n.trees = 2,
  interaction.depth = 1,
  n.minobsinnode = 1,
  shrinkage = 1, # step size = 1
  bag.fraction = 1 # no subsampling of training data
)
```


모형 평가: ROC curve, AUC

```{r}
result2 <- data.frame(
  class = factor(dat$class, levels = c(1, 0)),
  posterior1 = predict(fit2, type = "response")
)
roc_auc(result2, truth = "class", posterior1)
autoplot(roc_curve(result2, truth = "class", posterior1))
```


#### 부연설명

위 코드에서 사용한 `{gbm}` 패키지의 결과는 교재의 예 10.6 결과와는 다소 다르다. 이는 `{gbm}`에서 스코어를 업데이트할 때 기존 추정 확률값에 기반하여 gradient에 multiplier를 적용하기 때문이다. 아래에 교재와 비교하여 다른 점을 간략히 설명한다. 자세한 내용은 [패키지 문서](https://cran.r-project.org/web/packages/gbm/vignettes/gbm.pdf)를 참고하자.

우선, 범주 0에 대한 스코어 $F_t^{(0)}(\mathbf{x}_i)$는 언제나 0인 상수로 간주한다.

$$
F_t^{(0)}(\mathbf{x}_i) = 0, \forall t
$$

범주 1에 대한 초기 스코어 $F_1^{(1)}(\mathbf{x}_i)$는 0으로 한다.

$$
F_1^{(1)}(\mathbf{x}_i) = 0
$$

$i$번째 객체가 범주 1에 속할 확률은 반복 $t$에서 아래와 같이 추정된다.

$$
P_t^{(1)}(\mathbf{x}_i) = \frac{\exp\left(F_t^{(1)}(\mathbf{x}_i)\right)}{1 + \exp\left(F_t^{(1)}(\mathbf{x}_i)\right)}
$$


이때 손실함수에 대한 negative gradient는 아래와 같이 간단하게 정리된다.

$$
z_i = y_{1i} - P_t^{(1)}(\mathbf{x}_i)
$$

이 $z_i$를 타겟변수로 하고 $\mathbf{x}_i$를 입력변수로 하여 $E[z_i \, | \, \mathbf{x}_i]$를 추정하는 트리 $T_t$를 학습한다. 이때 $A_{tl}$을 $T_t$의 $l$번째 leaf node에 속하는 객체의 집합이라 하자.

스코어를 업데이트할 수치 $C_{t}^{(1)}(\mathbf{x}_i)$는 $P_t^{(1)}$을 고려하여 아래와 같이 계산된다. 

$$
C_{t}^{(1)}(\mathbf{x}_i) = \sum_{l} \mathbb{I}(i \in A_{tl})  \frac{\sum_{j \in A_{tl}} z_j}{\sum_{j \in A_{tl}} P_t^{(1)}(\mathbf{x}_j) \left(1 - P_t^{(1)}(\mathbf{x}_j)\right)}
$$

위 수식에서 $\mathbb{I}(x)$는 $x$값이 참일 때 1, 거짓일 때 0의 값을 갖는 함수이다. 위 수식에서 객체 $i$가 속한 leaf node에 함께 속한 모든 객체들의 $P_t^{(1)}$값이 고려되어 업데이트 수치가 결정됨을 유의하자.

이후 아래와 같이 스코어를 업데이트하고 과정을 반복한다.

$$
F_{t+1}^{(1)}(\mathbf{x}_i) = F_{t}^{(1)}(\mathbf{x}_i) + C_{t}^{(1)}(\mathbf{x}_i)
$$





