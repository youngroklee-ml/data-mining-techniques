[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "데이터마이닝 기법과 응용: R 예제",
    "section": "",
    "text": "소개\n데이터마이닝 기법과 응용 2판의 예제에 대한 R 코드입니다.",
    "crumbs": [
      "소개"
    ]
  },
  {
    "objectID": "chapters/ch2_regression.html",
    "href": "chapters/ch2_regression.html",
    "title": "2장 회귀분석",
    "section": "",
    "text": "(예 2.3 - 2.5, 2.7, 2.10 - 2.11)",
    "crumbs": [
      "예측",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>2장 회귀분석</span>"
    ]
  },
  {
    "objectID": "chapters/ch2_regression.html#예-2.3---2.5-2.7-2.10---2.11",
    "href": "chapters/ch2_regression.html#예-2.3---2.5-2.7-2.10---2.11",
    "title": "2장 회귀분석",
    "section": "",
    "text": "데이터 로드\n\ndat1 &lt;- read.csv(\"data/ch2_reg1.csv\")\n\n\n\n(예 2.3) 회귀계수 추정\n\nlm_fit &lt;- lm(weight ~ age + height, data = dat1)\ncoef(lm_fit)\n\n (Intercept)          age       height \n-108.1671993    0.3291212    0.9552913 \n\n\n\n\n(예 2.4) 오차항 분산 추정\n\nsum(lm_fit$residuals^2) / lm_fit$df.residual\n\n[1] 7.038464\n\n\n\n\n(예 2.5) 회귀성검정\n\nanova(lm_fit)\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df  Sum Sq Mean Sq F value   Pr(&gt;F)   \nage        1 119.299 119.299   16.95 0.004476 **\nheight     1 107.831 107.831   15.32 0.005793 **\nResiduals  7  49.269   7.038                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n(예 2.7) 회귀계수검정\n\nsummary(lm_fit)\n\n\nCall:\nlm(formula = weight ~ age + height, data = dat1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3398 -1.6623 -0.3084  1.3727  4.1911 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -108.16720   42.12277  -2.568  0.03712 * \nage            0.32912    0.06924   4.753  0.00208 **\nheight         0.95529    0.24406   3.914  0.00579 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.653 on 7 degrees of freedom\nMultiple R-squared:  0.8217,    Adjusted R-squared:  0.7708 \nF-statistic: 16.13 on 2 and 7 DF,  p-value: 0.002391\n\n\n\n\n(예 2.10) 평균반응치 추정\n추정회귀계수벡터의 분산-공분산 행렬\n\nvcov(lm_fit)\n\n             (Intercept)          age        height\n(Intercept) 1774.3280624 -0.671107283 -10.264885141\nage           -0.6711073  0.004794717   0.003035476\nheight       -10.2648851  0.003035476   0.059566804\n\n\n신규 데이터\n\nnewdata &lt;- data.frame(age = 40, height = 170)\n\n신규 데이터에 대한 평균반응치 추정\n\npredict(lm_fit, newdata)\n\n       1 \n67.39718 \n\n\n신규 데이터에 대한 평균반응치의 95% 신뢰구간\n\nconf_interval &lt;- predict(lm_fit, newdata, interval = \"confidence\", level = 0.95)\nconf_interval[, c(\"lwr\", \"upr\")]\n\n     lwr      upr \n65.01701 69.77735 \n\n\n\n\n(예 2.11) 미래반응치 예측\n신규 데이터에 대한 미래반응치의 95% 예측구간\n\npred_interval &lt;- predict(lm_fit, newdata, interval = \"prediction\", level = 0.95)\npred_interval[, c(\"lwr\", \"upr\")]\n\n     lwr      upr \n60.68745 74.10690",
    "crumbs": [
      "예측",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>2장 회귀분석</span>"
    ]
  },
  {
    "objectID": "chapters/ch2_regression.html#예-2.8---2.9-2.13",
    "href": "chapters/ch2_regression.html#예-2.8---2.9-2.13",
    "title": "2장 회귀분석",
    "section": "(예 2.8 - 2.9, 2.13)",
    "text": "(예 2.8 - 2.9, 2.13)\n\n패키지 로드\n\nlibrary(\"olsrr\")\n\n\nAttaching package: 'olsrr'\n\n\nThe following object is masked from 'package:datasets':\n\n    rivers\n\nlibrary(\"car\")\n\nLoading required package: carData\n\n\n\n\n데이터 로드\n\ndat_ba &lt;- read.csv(\"data/Player.csv\")\n\n\n\n다중 회귀모형 추정\n\nlm_model &lt;- lm(Salary ~ Hits + Walks + CRuns + HmRun + CWalks, data = dat_ba)\nsummary(lm_model)\n\n\nCall:\nlm(formula = Salary ~ Hits + Walks + CRuns + HmRun + CWalks, \n    data = dat_ba)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-858.19 -184.25  -40.46  120.95 2183.89 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -61.7447    57.0968  -1.081  0.28053    \nHits          1.3558     0.7568   1.791  0.07441 .  \nWalks         4.9172     1.5771   3.118  0.00203 ** \nCRuns         1.0212     0.2123   4.810 2.58e-06 ***\nHmRun         2.5378     2.9132   0.871  0.38450    \nCWalks       -0.5726     0.2821  -2.030  0.04342 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 341.1 on 257 degrees of freedom\n  (59 observations deleted due to missingness)\nMultiple R-squared:  0.4391,    Adjusted R-squared:  0.4282 \nF-statistic: 40.24 on 5 and 257 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n(예 2.8) 모든 가능한 조합의 회귀분석\n\nk &lt;- ols_step_all_possible(lm_model)\nk\n\n   Index N                    Predictors  R-Square Adj. R-Square Mallow's Cp\n3      1 1                         CRuns 0.3166062     0.3139878   54.140213\n5      2 1                        CWalks 0.2399256     0.2370135   89.276284\n2      3 1                         Walks 0.1970181     0.1939416  108.937069\n1      4 1                          Hits 0.1924355     0.1893414  111.036886\n4      5 1                         HmRun 0.1176683     0.1142877  145.296250\n7      6 2                    Hits CRuns 0.4147791     0.4102774   11.156078\n10     7 2                   Walks CRuns 0.3906078     0.3859201   22.231709\n9      8 2                   Hits CWalks 0.3853422     0.3806140   24.644480\n13     9 2                   CRuns HmRun 0.3584707     0.3535358   36.957357\n14    10 2                  CRuns CWalks 0.3240582     0.3188586   52.725631\n12    11 2                  Walks CWalks 0.3068498     0.3015179   60.610716\n15    12 2                  HmRun CWalks 0.2965559     0.2911448   65.327521\n6     13 2                    Hits Walks 0.2453786     0.2395738   88.777655\n11    14 2                   Walks HmRun 0.2240202     0.2180511   98.564374\n8     15 2                    Hits HmRun 0.2093560     0.2032741  105.283718\n16    16 3              Hits Walks CRuns 0.4287269     0.4221098    6.765004\n23    17 3            Walks CRuns CWalks 0.4269906     0.4203535    7.560585\n19    18 3              Hits CRuns HmRun 0.4178491     0.4111061   11.749353\n20    19 3             Hits CRuns CWalks 0.4147802     0.4080016   13.155578\n22    20 3             Walks CRuns HmRun 0.4021021     0.3951766   18.964848\n21    21 3             Hits HmRun CWalks 0.3874947     0.3804000   25.658175\n18    22 3             Hits Walks CWalks 0.3868621     0.3797601   25.948023\n25    23 3            CRuns HmRun CWalks 0.3647553     0.3573973   36.077637\n24    24 3            Walks HmRun CWalks 0.3290179     0.3212459   52.452998\n17    25 3              Hits Walks HmRun 0.2532017     0.2445515   87.193024\n27    26 4       Hits Walks CRuns CWalks 0.4374699     0.4287485    4.758862\n30    27 4      Walks CRuns HmRun CWalks 0.4321227     0.4233184    7.209002\n26    28 4        Hits Walks CRuns HmRun 0.4301353     0.4213001    8.119679\n29    29 4       Hits CRuns HmRun CWalks 0.4179107     0.4088860   13.721142\n28    30 4       Hits Walks HmRun CWalks 0.3886385     0.3791600   27.134046\n31    31 5 Hits Walks CRuns HmRun CWalks 0.4391260     0.4282141    6.000000\n\n\n\n\n(예 2.9) 단계별방법\n\nols_step_both_p(lm_model, details = TRUE, pent = 0.3, prem = 0.3)\n\nStepwise Selection Method   \n---------------------------\n\nCandidate Terms: \n\n1. Hits \n2. Walks \n3. CRuns \n4. HmRun \n5. CWalks \n\nWe are selecting variables based on p value...\n\n\nStepwise Selection: Step 1 \n\n- CRuns added \n\n                          Model Summary                            \n------------------------------------------------------------------\nR                       0.563       RMSE                  373.643 \nR-Squared               0.317       Coef. Var              69.719 \nAdj. R-Squared          0.314       MSE                139609.007 \nPred R-Squared          0.295       MAE                   260.516 \n------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                                   ANOVA                                    \n---------------------------------------------------------------------------\n                    Sum of                                                 \n                   Squares         DF     Mean Square       F         Sig. \n---------------------------------------------------------------------------\nRegression    16881162.032          1    16881162.032    120.917    0.0000 \nResidual      36437950.757        261      139609.007                      \nTotal         53319112.789        262                                      \n---------------------------------------------------------------------------\n\n                                    Parameter Estimates                                     \n-------------------------------------------------------------------------------------------\n      model       Beta    Std. Error    Std. Beta      t        Sig       lower      upper \n-------------------------------------------------------------------------------------------\n(Intercept)    259.082        34.127                  7.592    0.000    191.882    326.282 \n      CRuns      0.766         0.070        0.563    10.996    0.000      0.629      0.904 \n-------------------------------------------------------------------------------------------\n\n\n\nStepwise Selection: Step 2 \n\n- Hits added \n\n                          Model Summary                            \n------------------------------------------------------------------\nR                       0.644       RMSE                  346.429 \nR-Squared               0.415       Coef. Var              64.641 \nAdj. R-Squared          0.410       MSE                120013.306 \nPred R-Squared          0.389       MAE                   230.183 \n------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                                  ANOVA                                    \n--------------------------------------------------------------------------\n                    Sum of                                                \n                   Squares         DF     Mean Square      F         Sig. \n--------------------------------------------------------------------------\nRegression    22115653.209          2    11057826.604    92.138    0.0000 \nResidual      31203459.580        260      120013.306                     \nTotal         53319112.789        262                                     \n--------------------------------------------------------------------------\n\n                                    Parameter Estimates                                     \n-------------------------------------------------------------------------------------------\n      model       Beta    Std. Error    Std. Beta      t        Sig        lower     upper \n-------------------------------------------------------------------------------------------\n(Intercept)    -50.817        56.596                 -0.898    0.370    -162.262    60.627 \n      CRuns      0.661         0.067        0.486     9.939    0.000       0.530     0.792 \n       Hits      3.226         0.488        0.323     6.604    0.000       2.264     4.188 \n-------------------------------------------------------------------------------------------\n\n                          Model Summary                            \n------------------------------------------------------------------\nR                       0.644       RMSE                  346.429 \nR-Squared               0.415       Coef. Var              64.641 \nAdj. R-Squared          0.410       MSE                120013.306 \nPred R-Squared          0.389       MAE                   230.183 \n------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                                  ANOVA                                    \n--------------------------------------------------------------------------\n                    Sum of                                                \n                   Squares         DF     Mean Square      F         Sig. \n--------------------------------------------------------------------------\nRegression    22115653.209          2    11057826.604    92.138    0.0000 \nResidual      31203459.580        260      120013.306                     \nTotal         53319112.789        262                                     \n--------------------------------------------------------------------------\n\n                                    Parameter Estimates                                     \n-------------------------------------------------------------------------------------------\n      model       Beta    Std. Error    Std. Beta      t        Sig        lower     upper \n-------------------------------------------------------------------------------------------\n(Intercept)    -50.817        56.596                 -0.898    0.370    -162.262    60.627 \n      CRuns      0.661         0.067        0.486     9.939    0.000       0.530     0.792 \n       Hits      3.226         0.488        0.323     6.604    0.000       2.264     4.188 \n-------------------------------------------------------------------------------------------\n\n\n\nStepwise Selection: Step 3 \n\n- Walks added \n\n                          Model Summary                            \n------------------------------------------------------------------\nR                       0.655       RMSE                  342.936 \nR-Squared               0.429       Coef. Var              63.990 \nAdj. R-Squared          0.422       MSE                117605.308 \nPred R-Squared          0.398       MAE                   230.113 \n------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                                  ANOVA                                   \n-------------------------------------------------------------------------\n                    Sum of                                               \n                   Squares         DF    Mean Square      F         Sig. \n-------------------------------------------------------------------------\nRegression    22859338.083          3    7619779.361    64.791    0.0000 \nResidual      30459774.706        259     117605.308                     \nTotal         53319112.789        262                                    \n-------------------------------------------------------------------------\n\n                                    Parameter Estimates                                     \n-------------------------------------------------------------------------------------------\n      model       Beta    Std. Error    Std. Beta      t        Sig        lower     upper \n-------------------------------------------------------------------------------------------\n(Intercept)    -76.854        56.974                 -1.349    0.179    -189.045    35.337 \n      CRuns      0.620         0.068        0.455     9.117    0.000       0.486     0.753 \n       Hits      2.415         0.581        0.242     4.157    0.000       1.271     3.560 \n      Walks      3.126         1.243        0.151     2.515    0.013       0.678     5.575 \n-------------------------------------------------------------------------------------------\n\n                          Model Summary                            \n------------------------------------------------------------------\nR                       0.655       RMSE                  342.936 \nR-Squared               0.429       Coef. Var              63.990 \nAdj. R-Squared          0.422       MSE                117605.308 \nPred R-Squared          0.398       MAE                   230.113 \n------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                                  ANOVA                                   \n-------------------------------------------------------------------------\n                    Sum of                                               \n                   Squares         DF    Mean Square      F         Sig. \n-------------------------------------------------------------------------\nRegression    22859338.083          3    7619779.361    64.791    0.0000 \nResidual      30459774.706        259     117605.308                     \nTotal         53319112.789        262                                    \n-------------------------------------------------------------------------\n\n                                    Parameter Estimates                                     \n-------------------------------------------------------------------------------------------\n      model       Beta    Std. Error    Std. Beta      t        Sig        lower     upper \n-------------------------------------------------------------------------------------------\n(Intercept)    -76.854        56.974                 -1.349    0.179    -189.045    35.337 \n      CRuns      0.620         0.068        0.455     9.117    0.000       0.486     0.753 \n       Hits      2.415         0.581        0.242     4.157    0.000       1.271     3.560 \n      Walks      3.126         1.243        0.151     2.515    0.013       0.678     5.575 \n-------------------------------------------------------------------------------------------\n\n\n\nStepwise Selection: Step 4 \n\n- CWalks added \n\n                          Model Summary                            \n------------------------------------------------------------------\nR                       0.661       RMSE                  340.961 \nR-Squared               0.437       Coef. Var              63.621 \nAdj. R-Squared          0.429       MSE                116254.294 \nPred R-Squared          0.402       MAE                   229.259 \n------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                                  ANOVA                                   \n-------------------------------------------------------------------------\n                    Sum of                                               \n                   Squares         DF    Mean Square      F         Sig. \n-------------------------------------------------------------------------\nRegression    23325504.960          4    5831376.240    50.161    0.0000 \nResidual      29993607.829        258     116254.294                     \nTotal         53319112.789        262                                    \n-------------------------------------------------------------------------\n\n                                    Parameter Estimates                                     \n-------------------------------------------------------------------------------------------\n      model       Beta    Std. Error    Std. Beta      t        Sig        lower     upper \n-------------------------------------------------------------------------------------------\n(Intercept)    -63.651        57.028                 -1.116    0.265    -175.951    48.649 \n      CRuns      1.022         0.212        0.751     4.818    0.000       0.604     1.440 \n       Hits      1.569         0.716        0.157     2.192    0.029       0.160     2.979 \n      Walks      5.058         1.568        0.244     3.226    0.001       1.971     8.146 \n     CWalks     -0.564         0.282       -0.330    -2.002    0.046      -1.119    -0.009 \n-------------------------------------------------------------------------------------------\n\n                          Model Summary                            \n------------------------------------------------------------------\nR                       0.661       RMSE                  340.961 \nR-Squared               0.437       Coef. Var              63.621 \nAdj. R-Squared          0.429       MSE                116254.294 \nPred R-Squared          0.402       MAE                   229.259 \n------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                                  ANOVA                                   \n-------------------------------------------------------------------------\n                    Sum of                                               \n                   Squares         DF    Mean Square      F         Sig. \n-------------------------------------------------------------------------\nRegression    23325504.960          4    5831376.240    50.161    0.0000 \nResidual      29993607.829        258     116254.294                     \nTotal         53319112.789        262                                    \n-------------------------------------------------------------------------\n\n                                    Parameter Estimates                                     \n-------------------------------------------------------------------------------------------\n      model       Beta    Std. Error    Std. Beta      t        Sig        lower     upper \n-------------------------------------------------------------------------------------------\n(Intercept)    -63.651        57.028                 -1.116    0.265    -175.951    48.649 \n      CRuns      1.022         0.212        0.751     4.818    0.000       0.604     1.440 \n       Hits      1.569         0.716        0.157     2.192    0.029       0.160     2.979 \n      Walks      5.058         1.568        0.244     3.226    0.001       1.971     8.146 \n     CWalks     -0.564         0.282       -0.330    -2.002    0.046      -1.119    -0.009 \n-------------------------------------------------------------------------------------------\n\n\n\nNo more variables to be added/removed.\n\n\nFinal Model Output \n------------------\n\n                          Model Summary                            \n------------------------------------------------------------------\nR                       0.661       RMSE                  340.961 \nR-Squared               0.437       Coef. Var              63.621 \nAdj. R-Squared          0.429       MSE                116254.294 \nPred R-Squared          0.402       MAE                   229.259 \n------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                                  ANOVA                                   \n-------------------------------------------------------------------------\n                    Sum of                                               \n                   Squares         DF    Mean Square      F         Sig. \n-------------------------------------------------------------------------\nRegression    23325504.960          4    5831376.240    50.161    0.0000 \nResidual      29993607.829        258     116254.294                     \nTotal         53319112.789        262                                    \n-------------------------------------------------------------------------\n\n                                    Parameter Estimates                                     \n-------------------------------------------------------------------------------------------\n      model       Beta    Std. Error    Std. Beta      t        Sig        lower     upper \n-------------------------------------------------------------------------------------------\n(Intercept)    -63.651        57.028                 -1.116    0.265    -175.951    48.649 \n      CRuns      1.022         0.212        0.751     4.818    0.000       0.604     1.440 \n       Hits      1.569         0.716        0.157     2.192    0.029       0.160     2.979 \n      Walks      5.058         1.568        0.244     3.226    0.001       1.971     8.146 \n     CWalks     -0.564         0.282       -0.330    -2.002    0.046      -1.119    -0.009 \n-------------------------------------------------------------------------------------------\n\n\n\n                               Stepwise Selection Summary                                \n----------------------------------------------------------------------------------------\n                     Added/                   Adj.                                          \nStep    Variable    Removed     R-Square    R-Square     C(p)         AIC         RMSE      \n----------------------------------------------------------------------------------------\n   1     CRuns      addition       0.317       0.314    54.1400    3866.0101    373.6429    \n   2      Hits      addition       0.415       0.410    11.1560    3827.2236    346.4294    \n   3     Walks      addition       0.429       0.422     6.7650    3822.8795    342.9363    \n   4     CWalks     addition       0.437       0.429     4.7590    3820.8233    340.9608    \n----------------------------------------------------------------------------------------\n\n\n\n\n(예 2.13) 다중공선성\n분산팽창계수\n\nvif_value1 &lt;- vif(lm_model)\nvif_value1\n\n     Hits     Walks     CRuns     HmRun    CWalks \n 2.626281  2.641472 11.132539  1.465358 12.496169 \n\n\n변수간 상관계수\n\nvars1 &lt;- c(\"Hits\", \"Walks\", \"CRuns\", \"HmRun\", \"CWalks\")\ncor(dat_ba[vars1])\n\n            Hits     Walks     CRuns     HmRun    CWalks\nHits   1.0000000 0.6412106 0.2617869 0.5621579 0.1518182\nWalks  0.6412106 1.0000000 0.3384780 0.4810143 0.4245071\nCRuns  0.2617869 0.3384780 1.0000000 0.2623606 0.9278069\nHmRun  0.5621579 0.4810143 0.2623606 1.0000000 0.2331537\nCWalks 0.1518182 0.4245071 0.9278069 0.2331537 1.0000000\n\n\n상관계수가 높은 두 변수 간의 선형관계\n\nplot(dat_ba[, c(\"CRuns\", \"CWalks\")])\nabline(lm(CWalks ~ CRuns, data = dat_ba), col = \"red\", lwd = 2, lty = 1)\n\n\n\n\n\n\n\n\n상관계수가 높은 두 변수 중 하나의 변수를 제거\n\nlm2_model &lt;- lm(Salary ~ Hits + Walks + CRuns + HmRun, data = dat_ba)\nsummary(lm2_model)\n\n\nCall:\nlm(formula = Salary ~ Hits + Walks + CRuns + HmRun, data = dat_ba)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-902.84 -189.20  -37.78  104.27 2196.70 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -75.27564   57.04814  -1.320 0.188168    \nHits          2.23021    0.62602   3.563 0.000437 ***\nWalks         2.97024    1.25939   2.358 0.019096 *  \nCRuns         0.61299    0.06849   8.950  &lt; 2e-16 ***\nHmRun         2.33890    2.92909   0.799 0.425310    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 343.2 on 258 degrees of freedom\n  (59 observations deleted due to missingness)\nMultiple R-squared:  0.4301,    Adjusted R-squared:  0.4213 \nF-statistic: 48.68 on 4 and 258 DF,  p-value: &lt; 2.2e-16\n\nvif_value2 &lt;- vif(lm2_model)\nvif_value2\n\n    Hits    Walks    CRuns    HmRun \n1.775329 1.664286 1.144697 1.463700",
    "crumbs": [
      "예측",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>2장 회귀분석</span>"
    ]
  },
  {
    "objectID": "chapters/ch2_regression.html#예-2.14-2.16",
    "href": "chapters/ch2_regression.html#예-2.14-2.16",
    "title": "2장 회귀분석",
    "section": "(예 2.14, 2.16)",
    "text": "(예 2.14, 2.16)\n\n패키지 로드\n\nlibrary(ggplot2)\n\n\n\n데이터 로드\n\ndat1 &lt;- read.csv(\"data/ch2_coil.csv\")\nsummary(dat1)\n\n      temp           thick           y        \n Min.   :530.0   Min.   :2.0   Min.   :48.70  \n 1st Qu.:562.5   1st Qu.:2.0   1st Qu.:49.52  \n Median :605.0   Median :6.0   Median :50.80  \n Mean   :616.0   Mean   :4.4   Mean   :50.54  \n 3rd Qu.:675.0   3rd Qu.:6.0   3rd Qu.:51.27  \n Max.   :710.0   Max.   :6.0   Max.   :52.50  \n\n\n\n\n지시변수 변환\n\ndat1$thick &lt;- factor(dat1$thick, levels = c(6, 2))\nsummary(dat1)\n\n      temp       thick       y        \n Min.   :530.0   6:6   Min.   :48.70  \n 1st Qu.:562.5   2:4   1st Qu.:49.52  \n Median :605.0         Median :50.80  \n Mean   :616.0         Mean   :50.54  \n 3rd Qu.:675.0         3rd Qu.:51.27  \n Max.   :710.0         Max.   :52.50  \n\n\n\n\n(예 2.14) 회귀모형 추정\n\nlm_fit &lt;- lm(y ~ temp + thick, data = dat1)\nsummary(lm_fit)\n\n\nCall:\nlm(formula = y ~ temp + thick, data = dat1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.26076 -0.18481 -0.02085  0.16208  0.29881 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 61.107966   0.703491  86.864 7.06e-12 ***\ntemp        -0.017678   0.001149 -15.380 1.18e-06 ***\nthick2       0.804153   0.149570   5.376  0.00103 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.228 on 7 degrees of freedom\nMultiple R-squared:  0.9721,    Adjusted R-squared:  0.9641 \nF-statistic: 121.8 on 2 and 7 DF,  p-value: 3.641e-06\n\n\n\n\n(예 2.16) 교호작용 추정\n\nlm_fit_interaction &lt;- lm(y ~ temp + thick + temp:thick, data = dat1)\nsummary(lm_fit_interaction)\n\n\nCall:\nlm(formula = y ~ temp + thick + temp:thick, data = dat1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.18286 -0.10707 -0.02152  0.08831  0.30949 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 60.136402   0.749951  80.187 2.53e-10 ***\ntemp        -0.016076   0.001230 -13.074 1.23e-05 ***\nthick2       3.278471   1.210446   2.708   0.0352 *  \ntemp:thick2 -0.003987   0.001940  -2.055   0.0857 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1886 on 6 degrees of freedom\nMultiple R-squared:  0.9836,    Adjusted R-squared:  0.9754 \nF-statistic:   120 on 3 and 6 DF,  p-value: 9.577e-06\n\n\n시각화\n\nggplot(dat1, aes(x = temp, y = y, shape = thick, linetype = thick)) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"grey30\") +\n  geom_point(size = 3) +\n  labs(x = \"X(temp)\", y = \"Y\", shape = \"thickness\", linetype = \"thickness\") +\n  theme_classic() +\n  theme(legend.position = c(0.2, 0.2))\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "예측",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>2장 회귀분석</span>"
    ]
  },
  {
    "objectID": "chapters/ch5_classification.html",
    "href": "chapters/ch5_classification.html",
    "title": "5장 분류분석 개요",
    "section": "",
    "text": "(예 5.2)",
    "crumbs": [
      "분류분석",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>5장 분류분석 개요</span>"
    ]
  },
  {
    "objectID": "chapters/ch5_classification.html#예-5.2",
    "href": "chapters/ch5_classification.html#예-5.2",
    "title": "5장 분류분석 개요",
    "section": "",
    "text": "패키지 로드\n\nlibrary(class) # knn\nlibrary(proxy) # dist - support cross-distance\n\n\nAttaching package: 'proxy'\n\n\nThe following objects are masked from 'package:stats':\n\n    as.dist, dist\n\n\nThe following object is masked from 'package:base':\n\n    as.matrix\n\n\n\n\n데이터 로드\n\ndat1 &lt;- read.csv(\"data/ch7_dat1.csv\")\ndat1$class &lt;- factor(dat1$class)\ntrain_x &lt;- dat1[1:7, 2:3]\ntrain_y &lt;- dat1[1:7, 4]\ntest_x &lt;- dat1[8:9, 2:3]\n\n\n\n학습데이터간 유클리디안 거리\n\ndist(train_x)\n\n         1        2        3        4        5        6\n2 4.123106                                             \n3 2.236068 5.830952                                    \n4 3.162278 5.000000 2.236068                           \n5 2.236068 3.162278 4.472136 5.000000                  \n6 3.605551 2.828427 5.830952 6.082763 1.414214         \n7 1.414214 3.605551 2.236068 2.000000 3.000000 4.123106\n\n\n\n\n3-인접객체법\n\nknn.cv(train_x, train_y, k = 3)\n\n[1] 1 1 1 1 1 1 2\nLevels: 1 2\n\n\n\n\n새로운 객체들과 학습데이터 간 유클리디안 거리\n\ndist(test_x, train_x)\n\n         1        2        3        4        5        6        7\n8 4.123106 5.830952 2.828427 1.000000 6.000000 7.071068 3.000000\n9 3.000000 1.414214 4.472136 3.605551 2.828427 3.162278 2.236068\n\n\n\n\n3-인접객체법\n\nknn(train_x, test_x, train_y, k = 3)\n\n[1] 2 1\nLevels: 1 2",
    "crumbs": [
      "분류분석",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>5장 분류분석 개요</span>"
    ]
  },
  {
    "objectID": "chapters/ch5_classification.html#예-5.3---5.4",
    "href": "chapters/ch5_classification.html#예-5.3---5.4",
    "title": "5장 분류분석 개요",
    "section": "(예 5.3 - 5.4)",
    "text": "(예 5.3 - 5.4)\n\n패키지 로드\n\nlibrary(e1071) # naive bayes\nlibrary(yardstick) # measure performance\n\n\n\n데이터 로드\n\ndat3 &lt;- read.csv(\"data/ch5_dat3.csv\")\ndat3$gender &lt;- factor(dat3$gender)\ndat3$age_gr &lt;- factor(dat3$age_gr)\ndat3$class &lt;- factor(dat3$class)\nsummary(dat3)\n\n       ID    gender age_gr class\n Min.   :1   F:5    1:1    1:5  \n 1st Qu.:3   M:4    2:4    2:4  \n Median :5          3:2         \n Mean   :5          4:2         \n 3rd Qu.:7                      \n Max.   :9                      \n\n\n\n\n(예 5.3) 나이브베이즈 분류분석\n모형 추정\n\nnb_fit &lt;- naiveBayes(class ~ gender + age_gr, data = dat3)\nprint(nb_fit)\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\n        1         2 \n0.5555556 0.4444444 \n\nConditional probabilities:\n   gender\nY      F    M\n  1 0.40 0.60\n  2 0.75 0.25\n\n   age_gr\nY      1    2    3    4\n  1 0.20 0.40 0.20 0.20\n  2 0.00 0.50 0.25 0.25\n\n\n사후확률\n\nnb_posterior &lt;- predict(nb_fit, dat3, type = \"raw\")\nround(nb_posterior, 3)\n\n          1     2\n [1,] 0.706 0.294\n [2,] 0.706 0.294\n [3,] 0.706 0.294\n [4,] 0.706 0.294\n [5,] 0.993 0.007\n [6,] 0.348 0.652\n [7,] 0.348 0.652\n [8,] 0.348 0.652\n [9,] 0.348 0.652\n\n\n추정범주\n\nnb_class &lt;- predict(nb_fit, dat3, type = \"class\")\nnb_class\n\n[1] 1 1 1 1 1 2 2 2 2\nLevels: 1 2\n\n\n추정결과 정리\n\nresults &lt;- cbind(dat3, pred_class = nb_class, posterior = round(nb_posterior, 3))\nresults\n\n  ID gender age_gr class pred_class posterior.1 posterior.2\n1  1      M      2     1          1       0.706       0.294\n2  2      M      2     2          1       0.706       0.294\n3  3      M      3     1          1       0.706       0.294\n4  4      M      4     1          1       0.706       0.294\n5  5      F      1     1          1       0.993       0.007\n6  6      F      2     2          2       0.348       0.652\n7  7      F      2     1          2       0.348       0.652\n8  8      F      3     2          2       0.348       0.652\n9  9      F      4     2          2       0.348       0.652\n\n\n\n\n(예 5.4) 성능 평가\n정오분류표\n\nconf_mat(results, truth = \"class\", estimate = \"pred_class\")\n\n          Truth\nPrediction 1 2\n         1 4 1\n         2 1 3\n\n\n민감도, 특이도 및 F1 척도\n\nmulti_metric &lt;- metric_set(accuracy, sens, spec, f_meas)\nmulti_metric(results, truth = \"class\", estimate = \"pred_class\")\n\n# A tibble: 4 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.778\n2 sens     binary         0.8  \n3 spec     binary         0.75 \n4 f_meas   binary         0.8",
    "crumbs": [
      "분류분석",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>5장 분류분석 개요</span>"
    ]
  },
  {
    "objectID": "chapters/ch5_classification.html#예-5.4",
    "href": "chapters/ch5_classification.html#예-5.4",
    "title": "5장 분류분석 개요",
    "section": "(예 5.4)",
    "text": "(예 5.4)\n\n패키지 로드\n\nlibrary(yardstick)\n\n\n\n데이터 로드\n\ndat &lt;- read.csv(\"data/ch5_cm.csv\")\ndat$pred_y &lt;- factor(dat$pred_y, levels = c(1, 0))\ndat$true_y &lt;- factor(dat$true_y, levels = c(1, 0))\nsummary(dat)\n\n pred_y true_y\n 1:25   1:20  \n 0:75   0:80  \n\ntable(dat)\n\n      true_y\npred_y  1  0\n     1 15 10\n     0  5 70\n\n\n\n\n정오분류표\n\ncm &lt;- conf_mat(dat, truth = \"true_y\", estimate = \"pred_y\")\ncm\n\n          Truth\nPrediction  1  0\n         1 15 10\n         0  5 70\n\n\n\n\n정확도, 민감도, 특이도, F1 척도\n\nmulti_metric &lt;- metric_set(accuracy, sens, spec, f_meas)\nmulti_metric(dat, truth = \"true_y\", estimate = \"pred_y\")\n\n# A tibble: 4 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.85 \n2 sens     binary         0.75 \n3 spec     binary         0.875\n4 f_meas   binary         0.667\n\n\n\n\nOPTIONAL: 보다 다양한 척도들\n\nsummary(cm)\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.85 \n 2 kap                  binary         0.571\n 3 sens                 binary         0.75 \n 4 spec                 binary         0.875\n 5 ppv                  binary         0.6  \n 6 npv                  binary         0.933\n 7 mcc                  binary         0.577\n 8 j_index              binary         0.625\n 9 bal_accuracy         binary         0.812\n10 detection_prevalence binary         0.25 \n11 precision            binary         0.6  \n12 recall               binary         0.75 \n13 f_meas               binary         0.667",
    "crumbs": [
      "분류분석",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>5장 분류분석 개요</span>"
    ]
  },
  {
    "objectID": "chapters/ch5_classification.html#예-5.5",
    "href": "chapters/ch5_classification.html#예-5.5",
    "title": "5장 분류분석 개요",
    "section": "(예 5.5)",
    "text": "(예 5.5)\n\n패키지 로드\n\nlibrary(yardstick)\nlibrary(ggplot2)\n\n\n\n데이터 로드\n\ndat5 &lt;- read.csv(\"data/ch5_dat5.csv\")\ndat5$class &lt;- factor(dat5$class, levels = c(1, 0))\n\n\n\n범주 분류기준: x &gt;= 40\n범주 추정\n\ndat5$pred40 &lt;- factor(ifelse(dat5$x &gt;= 40, 1, 0), levels = c(1, 0))\n\n정오분류표\n\ncm40 &lt;- conf_mat(dat5, truth = \"class\", estimate = \"pred40\")\ncm40\n\n          Truth\nPrediction 1 0\n         1 5 2\n         0 1 2\n\n\n민감도 및 특이도\n\nmulti_metric &lt;- metric_set(sens, spec)\nmetric40 &lt;- multi_metric(dat5, truth = \"class\", estimate = \"pred40\")\nmetric40\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 sens    binary         0.833\n2 spec    binary         0.5  \n\n\n\n\n범주 분류기준: x &gt;= 50\n범주 추정\n\ndat5$pred50 &lt;- factor(ifelse(dat5$x &gt;= 50, 1, 0), levels = c(1, 0))\n\n정오분류표\n\ncm50 &lt;- conf_mat(dat5, truth = \"class\", estimate = \"pred50\")\ncm50\n\n          Truth\nPrediction 1 0\n         1 4 1\n         0 2 3\n\n\n민감도 및 특이도\n\nmetric50 &lt;- multi_metric(dat5, truth = \"class\", estimate = \"pred50\")\nmetric50\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 sens    binary         0.667\n2 spec    binary         0.75",
    "crumbs": [
      "분류분석",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>5장 분류분석 개요</span>"
    ]
  },
  {
    "objectID": "chapters/ch5_classification.html#예-5.6",
    "href": "chapters/ch5_classification.html#예-5.6",
    "title": "5장 분류분석 개요",
    "section": "(예 5.6)",
    "text": "(예 5.6)\n\n패키지 로드\n\nlibrary(yardstick)\nlibrary(ggplot2)\n\n\n\n데이터 로드\n\ndat &lt;- read.csv(\"data/ch5_roc.csv\")\ndat$class &lt;- factor(dat$class)\ndat$pred &lt;- factor(dat$pred)\n\n\n\nROC 곡선 데이터\n\nroc &lt;- roc_curve(dat, truth = \"class\", posterior1)\nroc\n\n# A tibble: 11 × 3\n   .threshold specificity sensitivity\n        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 -Inf               0          1   \n 2    0.00596         0          1   \n 3    0.0264          0.2        1   \n 4    0.0995          0.4        1   \n 5    0.103           0.6        1   \n 6    0.356           0.8        1   \n 7    0.728           0.8        0.75\n 8    0.921           1          0.75\n 9    0.980           1          0.5 \n10    0.981           1          0.25\n11  Inf               1          0   \n\n\n\n\nAUC\n\nauc &lt;- roc_auc(dat, truth = \"class\", posterior1)\nauc\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary          0.95\n\n\n\n\nROC 곡선 시각화\n\nautoplot(roc) +\n  labs(title = paste0(\"ROC Curve: AUC = \", auc[[\".estimate\"]]))",
    "crumbs": [
      "분류분석",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>5장 분류분석 개요</span>"
    ]
  },
  {
    "objectID": "chapters/ch5_classification.html#예-5.7",
    "href": "chapters/ch5_classification.html#예-5.7",
    "title": "5장 분류분석 개요",
    "section": "(예 5.7)",
    "text": "(예 5.7)\n\n패키지 로드\n\nlibrary(yardstick)\nlibrary(ggplot2)\n\n\n\n데이터 로드\n1000개 객체의 실제범주와 범주 1에 속할 사후확률 추정 결과\n\ndat &lt;- read.csv(\"data/ch5_lift_2.csv\")\ndat$class &lt;- factor(dat$class)\nhead(dat)\n\n  class posterior1\n1     1     0.9995\n2     1     0.9985\n3     1     0.9975\n4     1     0.9965\n5     1     0.9955\n6     1     0.9945\n\ntail(dat)\n\n     class posterior1\n995      2     0.0055\n996      2     0.0045\n997      2     0.0035\n998      2     0.0025\n999      2     0.0015\n1000     2     0.0005\n\nsummary(dat)\n\n class     posterior1    \n 1:437   Min.   :0.0005  \n 2:563   1st Qu.:0.2502  \n         Median :0.5000  \n         Mean   :0.5000  \n         3rd Qu.:0.7498  \n         Max.   :0.9995  \n\n\n사후확률 내림차순으로 객체를 100개씩 묶은 10개 집단 각각의 범주 1 빈도\n\ngrouped &lt;- read.csv(\"data/ch5_lift_1.csv\")\ngrouped$group &lt;- factor(grouped$group)\ngrouped\n\n   group   n n1\n1      1 100 92\n2      2 100 78\n3      3 100 64\n4      4 100 57\n5      5 100 43\n6      6 100 35\n7      7 100 29\n8      8 100 22\n9      9 100  7\n10    10 100 10\n\n\n\n\n범주 1\n범주 1에 속한 전체 객체 수\n\ntotal_n1 &lt;- sum(grouped$n1)\nprint(total_n1)\n\n[1] 437\n\n\n전체 데이터에서 범주 1의 비율\n\nprop_n1 &lt;- sum(grouped$n1) / sum(grouped$n)\nprint(prop_n1)\n\n[1] 0.437\n\n\n\n\n이익도표 통계량 산출\n\ngrouped$response_pct &lt;- grouped$n1 / grouped$n * 100\ngrouped$captured_response_pct &lt;- grouped$n1 / total_n1 * 100\ngrouped$gain &lt;- cumsum(grouped$captured_response_pct)\ngrouped$lift &lt;- (cumsum(grouped$n1) / cumsum(grouped$n)) / prop_n1\ngrouped\n\n   group   n n1 response_pct captured_response_pct      gain     lift\n1      1 100 92           92             21.052632  21.05263 2.105263\n2      2 100 78           78             17.848970  38.90160 1.945080\n3      3 100 64           64             14.645309  53.54691 1.784897\n4      4 100 57           57             13.043478  66.59039 1.664760\n5      5 100 43           43              9.839817  76.43021 1.528604\n6      6 100 35           35              8.009153  84.43936 1.407323\n7      7 100 29           29              6.636156  91.07551 1.301079\n8      8 100 22           22              5.034325  96.10984 1.201373\n9      9 100  7            7              1.601831  97.71167 1.085685\n10    10 100 10           10              2.288330 100.00000 1.000000\n\n\n\n\nGain chart\n\nggplot(grouped, aes(x = group, y = gain)) +\n  geom_line(group = 1) +\n  geom_point() +\n  labs(y = \"% Gain\", title = \"Gain Chart for Class 1\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\nLift chart\n\nggplot(grouped, aes(x = group, y = lift)) +\n  geom_hline(yintercept = 1, color = \"grey30\", linetype = \"dashed\") +\n  geom_line(group = 1) +\n  geom_point() +\n  labs(y = \"Lift\", title = \"Lift Chart for Class 1\") +\n  scale_y_continuous(breaks = seq(1, 2.2, by = 0.2)) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\nOPTIONAL: 집단화하기 전 원 데이터(1000개 객체)를 이용한 이익도표\nGain chart\n\ngain &lt;- gain_curve(dat, truth = class, posterior1)\nautoplot(gain)\n\n\n\n\n\n\n\n\n\nlift &lt;- lift_curve(dat, truth = class, posterior1)\nautoplot(lift)",
    "crumbs": [
      "분류분석",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>5장 분류분석 개요</span>"
    ]
  },
  {
    "objectID": "chapters/ch09_svm.html",
    "href": "chapters/ch09_svm.html",
    "title": "9장 서포트 벡터 머신",
    "section": "",
    "text": "(예 9.1)",
    "crumbs": [
      "분류분석",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>9장 서포트 벡터 머신</span>"
    ]
  },
  {
    "objectID": "chapters/ch09_svm.html#예-9.1",
    "href": "chapters/ch09_svm.html#예-9.1",
    "title": "9장 서포트 벡터 머신",
    "section": "",
    "text": "패키지 로드\n\nlibrary(kernlab)\n\n\n\n데이터 로드\n\ndat &lt;- read.csv(\"data/ch9_dat1.csv\")\ndat$class &lt;- factor(dat$class)\ndat\n\n  x1 x2 class\n1  5  7     1\n2  4  3    -1\n3  7  8     1\n4  8  6     1\n5  3  6    -1\n6  2  5    -1\n7  6  6     1\n8  9  6     1\n9  5  4    -1\n\n\n\n\nSVM 학습\n\nmodel &lt;- ksvm(\n  class ~ x1 + x2,\n  data = dat,\n  scaled = FALSE,\n  kernel = \"vanilladot\"\n)\n\n Setting default kernel parameters  \n\n\n\n\n서포트 벡터 객체\n\nmodel@alphaindex[[1]]\n\n[1] 1 5 7 9\n\n\n\n\n최적해: \\(\\alpha\\)값 (서포트 벡터)\n\nmodel@alpha[[1]]\n\n[1] 0.1555556 0.1888889 0.2888889 0.2555556\n\n\n\n\n목적함수값\n\n-model@obj\n\n[1] 0.4444444\n\n\n\n\n하이퍼플레인 추정\n계수 (w)\n\nw &lt;- model@coef[[1]] %*% model@xmatrix[[1]]\nw\n\n            x1        x2\n[1,] 0.6666667 0.6666667\n\n\n절편 (b)\n\n-model@b\n\n[1] -7\n\n\n\n\n시각화\n\nplot(model, data = dat)",
    "crumbs": [
      "분류분석",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>9장 서포트 벡터 머신</span>"
    ]
  },
  {
    "objectID": "chapters/ch09_svm.html#예-9.2",
    "href": "chapters/ch09_svm.html#예-9.2",
    "title": "9장 서포트 벡터 머신",
    "section": "(예 9.2)",
    "text": "(예 9.2)\n\n패키지 로드\n\nlibrary(kernlab)\n\n\n\n데이터 로드\n\ndat &lt;- read.csv(\"data/ch9_dat2.csv\")\ndat$class &lt;- factor(dat$class)\ndat\n\n   x1 x2 class\n1   5  7     1\n2   4  3    -1\n3   7  8     1\n4   8  6     1\n5   3  6    -1\n6   2  5    -1\n7   6  6     1\n8   9  6     1\n9   5  4    -1\n10  7  6    -1\n\n\n\n\nSVM 학습\n\nmodel &lt;- ksvm(\n  class ~ x1 + x2,\n  data = dat,\n  scaled = FALSE,\n  kernel = \"vanilladot\",\n  C = 1\n)\n\n Setting default kernel parameters  \n\n\n\n\n서포트 벡터 객체\n\nmodel@alphaindex[[1]]\n\n[1]  1  5  7 10\n\n\n\n\n최적해: \\(\\alpha\\)값 (서포트 벡터)\n\nmodel@alpha[[1]]\n\n[1] 0.8 0.8 1.0 1.0\n\n\n\n\n목적함수값\n\n-model@obj\n\n[1] 3.1\n\n\n\n\n하이퍼플레인 추정\n계수 (w)\n\nw &lt;- model@coef[[1]] %*% model@xmatrix[[1]]\nw\n\n      x1  x2\n[1,] 0.6 0.8\n\n\n절편 (b)\n\n-model@b\n\n[1] -7.6\n\n\n\n\n오분류 객체\n\nwhich(model@ymatrix != as.integer(model@fitted))\n\n[1] 10\n\n\n\n\n시각화\n\nplot(model, data = dat)\n\n\n\n\n\n\n\n\n\n\n패널티 단가(C) 변경: 1, 5, 100\n\n# Try with C = 1, C = 5 and C = 100\nCs &lt;- c(1, 5, 100)\nmodels &lt;- vector(\"list\", length = length(Cs))\n\nfor (i in seq_along(Cs)) {\n  message(paste0(\"-------\\nC = \", Cs[i], \"\\n-------\"))\n\n  # SVM with 2nd order polynomial kernel\n  models[[i]] &lt;- ksvm(\n    class ~ x1 + x2,\n    data = dat,\n    scaled = FALSE,\n    kernel = \"vanilladot\",\n    C = Cs[i]\n  )\n\n  # support vectors\n  message(\"Support vectors:\")\n  print(models[[i]]@alphaindex[[1]])\n\n  # alpha values for support vectors\n  message(\"Alpha values for support vectors:\")\n  print(round(models[[i]]@alpha[[1]], 4))\n\n  # objective value\n  # note that we placed minus(-) sign\n  message(\"Objective value:\")\n  print(-models[[i]]@obj)\n\n  # hyperplane coefficient vector w and intercept b\n  message(\"Hyperplane coefficients (w):\")\n  w &lt;- models[[i]]@coef[[1]] %*% models[[i]]@xmatrix[[1]]\n  print(w)\n\n  # hyperplane's intercept b\n  # Note that we placed minus(-) sign\n  message(\"Hyperplane intercept (b):\")\n  print(-models[[i]]@b)\n\n  # misclassified objects\n  message(\"Misclassified instances:\")\n  print(which(models[[i]]@ymatrix != as.integer(models[[i]]@fitted)))\n}\n\n-------\nC = 1\n-------\n\n\n Setting default kernel parameters  \n\n\nSupport vectors:\n\n\n[1]  1  5  7 10\n\n\nAlpha values for support vectors:\n\n\n[1] 0.8 0.8 1.0 1.0\n\n\nObjective value:\n\n\n[1] 3.1\n\n\nHyperplane coefficients (w):\n\n\n      x1  x2\n[1,] 0.6 0.8\n\n\nHyperplane intercept (b):\n\n\n[1] -7.6\n\n\nMisclassified instances:\n\n\n[1] 10\n\n\n-------\nC = 5\n-------\n\n\n Setting default kernel parameters  \n\n\nSupport vectors:\n\n\n[1]  1  4  5  7 10\n\n\nAlpha values for support vectors:\n\n\n[1] 1.1991 0.6004 1.7995 5.0000 5.0000\n\n\nObjective value:\n\n\n[1] 12.8\n\n\nHyperplane coefficients (w):\n\n\n      x1       x2\n[1,] 0.4 1.199115\n\n\nHyperplane intercept (b):\n\n\n[1] -9.394396\n\n\nMisclassified instances:\n\n\n[1] 10\n\n\n-------\nC = 100\n-------\n\n\n Setting default kernel parameters  \n\n\nSupport vectors:\n\n\n[1]  1  4  5  7 10\n\n\nAlpha values for support vectors:\n\n\n[1]   1.2004  19.5999  20.8003 100.0000 100.0000\n\n\nObjective value:\n\n\n[1] 240.8\n\n\nHyperplane coefficients (w):\n\n\n            x1       x2\n[1,] 0.4001396 1.200419\n\n\nHyperplane intercept (b):\n\n\n[1] -9.403397\n\n\nMisclassified instances:\n\n\n[1] 10",
    "crumbs": [
      "분류분석",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>9장 서포트 벡터 머신</span>"
    ]
  },
  {
    "objectID": "chapters/ch09_svm.html#예-9.3",
    "href": "chapters/ch09_svm.html#예-9.3",
    "title": "9장 서포트 벡터 머신",
    "section": "(예 9.3)",
    "text": "(예 9.3)\n\n패키지 로드\n\nlibrary(kernlab)\n\n\n\n데이터 로드\n\ndat &lt;- matrix(c(1, 2, 2, 2, 2, -1), nrow = 3, byrow = TRUE)\ndat\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    2    2\n[3,]    2   -1\n\n\n\n\n가우시안 커널\n\nkernelMatrix(rbfdot(sigma = 1 / 2), dat)\n\nAn object of class \"kernelMatrix\"\n            [,1]      [,2]        [,3]\n[1,] 1.000000000 0.6065307 0.006737947\n[2,] 0.606530660 1.0000000 0.011108997\n[3,] 0.006737947 0.0111090 1.000000000\n\n\n\n\n이차 커널\n\nkernelMatrix(polydot(degree = 2), dat)\n\nAn object of class \"kernelMatrix\"\n     [,1] [,2] [,3]\n[1,]   36   49    1\n[2,]   49   81    9\n[3,]    1    9   36\n\n\n\n\n시그모이드 커널 (하이퍼볼릭 탄젠트 커널)\n\nkernelMatrix(tanhdot(offset = 0), dat)\n\nAn object of class \"kernelMatrix\"\n          [,1]      [,2]      [,3]\n[1,] 0.9999092 0.9999877 0.0000000\n[2,] 0.9999877 0.9999998 0.9640276\n[3,] 0.0000000 0.9640276 0.9999092\n\n\n\n\n선형 커널\n\nkernelMatrix(vanilladot(), dat)\n\nAn object of class \"kernelMatrix\"\n     [,1] [,2] [,3]\n[1,]    5    6    0\n[2,]    6    8    2\n[3,]    0    2    5",
    "crumbs": [
      "분류분석",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>9장 서포트 벡터 머신</span>"
    ]
  },
  {
    "objectID": "chapters/ch09_svm.html#예-9.6",
    "href": "chapters/ch09_svm.html#예-9.6",
    "title": "9장 서포트 벡터 머신",
    "section": "(예 9.6)",
    "text": "(예 9.6)\n\n패키지 로드\n\nlibrary(kernlab)\n\n\n\n데이터 로드\n\ndat &lt;- data.frame(\n  x1 = c(-1, -1, 1, 1),\n  x2 = c(-1, 1, -1, 1),\n  class = factor(c(-1, 1, 1, -1))\n)\ndat\n\n  x1 x2 class\n1 -1 -1    -1\n2 -1  1     1\n3  1 -1     1\n4  1  1    -1\n\n\n\n\n이차 커널\n\nX &lt;- as.matrix(dat[, 1:2])\nK &lt;- kernelMatrix(polydot(degree = 2), X)\nK\n\nAn object of class \"kernelMatrix\"\n     [,1] [,2] [,3] [,4]\n[1,]    9    1    1    1\n[2,]    1    9    1    1\n[3,]    1    1    9    1\n[4,]    1    1    1    9\n\n\n\n\nSVM 학습\n\nmodel &lt;- ksvm(\n  class ~ x1 + x2,\n  data = dat,\n  scaled = FALSE,\n  kernel = polydot(degree = 2)\n)\n\n\n\n서포트 벡터 객체\n\nmodel@alphaindex[[1]]\n\n[1] 1 2 3 4\n\n\n\n\n최적해: \\(\\alpha\\)값 (서포트 벡터)\n\nmodel@alpha[[1]]\n\n[1] 0.125 0.125 0.125 0.125\n\n\n\n\n하이퍼플레인 추정\n계수 (beta)\n\nbeta1 &lt;- 2 * sum(model@coef[[1]] * model@xmatrix[[1]][, \"x1\"])\nbeta2 &lt;- 2 * sum(model@coef[[1]] * model@xmatrix[[1]][, \"x2\"])\nbeta11 &lt;- sum(model@coef[[1]] * model@xmatrix[[1]][, \"x1\"]^2)\nbeta22 &lt;- sum(model@coef[[1]] * model@xmatrix[[1]][, \"x2\"]^2)\nbeta12 &lt;- 2 * sum(model@coef[[1]] * apply(model@xmatrix[[1]], 1, prod))\nbetas &lt;- c(beta1, beta2, beta11, beta22, beta12)\nnames(betas) &lt;- c(\"beta1\", \"beta2\", \"beta11\", \"beta22\", \"beta12\")\nround(betas, 4)\n\n beta1  beta2 beta11 beta22 beta12 \n     0      0      0      0     -1 \n\n\n절편 (b)\n\n-model@b\n\n[1] 0",
    "crumbs": [
      "분류분석",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>9장 서포트 벡터 머신</span>"
    ]
  },
  {
    "objectID": "chapters/ch09_svm.html#예-9.7",
    "href": "chapters/ch09_svm.html#예-9.7",
    "title": "9장 서포트 벡터 머신",
    "section": "(예 9.7)",
    "text": "(예 9.7)\n\n패키지 로드\n\nlibrary(kernlab)\n\n\n\n데이터 로드\n\ndat &lt;- read.csv(\"data/ch9_dat3.csv\")\ndat$class &lt;- factor(dat$class)\ndat\n\n  x1 x2 class\n1  5  7     1\n2  4  3    -1\n3  7  8    -1\n4  8  6    -1\n5  3  6     1\n6  2  5     1\n7  6  6     1\n8  9  6    -1\n9  5  4    -1\n\n\n\n\nSVM 학습\n\nmodel &lt;- ksvm(\n  class ~ x1 + x2,\n  data = dat,\n  scaled = FALSE,\n  kernel = polydot(degree = 2),\n  C = 1\n)\n\n\n\n서포트 벡터 객체\n\nmodel@alphaindex[[1]]\n\n[1] 1 2 3 7 9\n\n\n\n\n최적해: \\(\\alpha\\)값 (서포트 벡터)\n\nmodel@alpha[[1]]\n\n[1] 0.23555183 0.50094882 0.65316473 1.00000000 0.08143829\n\n\n\n\n하이퍼플레인 추정\n\nbeta1 &lt;- 2 * sum(model@coef[[1]] * model@xmatrix[[1]][, \"x1\"])\nbeta2 &lt;- 2 * sum(model@coef[[1]] * model@xmatrix[[1]][, \"x2\"])\nbeta11 &lt;- sum(model@coef[[1]] * model@xmatrix[[1]][, \"x1\"]^2)\nbeta22 &lt;- sum(model@coef[[1]] * model@xmatrix[[1]][, \"x2\"]^2)\nbeta12 &lt;- 2 * sum(model@coef[[1]] * apply(model@xmatrix[[1]], 1, prod))\nhyperplane &lt;- c(-model@b, beta1, beta2, beta11, beta22, beta12)\nnames(hyperplane) &lt;- c(\"b\", \"beta1\", \"beta2\", \"beta11\", \"beta22\", \"beta12\")\nround(hyperplane, 4)\n\n      b   beta1   beta2  beta11  beta22  beta12 \n-3.4455  0.3892  1.1899 -0.1674 -0.0721  0.0539 \n\n\n\n\n오분류 객체\n\nwhich(model@ymatrix != as.integer(model@fitted))\n\n[1] 7\n\n\n\n\n패널티 단가(C) 변경: 1, 5, 100\n\n# Try with C = 1, C = 5 and C = 100\nCs &lt;- c(1, 5, 100)\nmodels &lt;- vector(\"list\", length = length(Cs))\n\nfor (i in seq_along(Cs)) {\n  message(paste0(\"-------\\nC = \", Cs[i], \"\\n-------\"))\n\n  # SVM with 2nd order polynomial kernel\n  models[[i]] &lt;- ksvm(\n    class ~ x1 + x2,\n    data = dat,\n    scaled = FALSE,\n    kernel = polydot(degree = 2),\n    C = Cs[i]\n  )\n\n  # support vectors\n  message(\"Support vectors:\")\n  print(models[[i]]@alphaindex[[1]])\n\n  # alpha values for support vectors\n  message(\"Alpha values for support vectors:\")\n  print(round(models[[i]]@alpha[[1]], 4))\n\n  # hyperplane\n  beta1 &lt;- 2 * sum(models[[i]]@coef[[1]] * models[[i]]@xmatrix[[1]][, \"x1\"])\n  beta2 &lt;- 2 * sum(models[[i]]@coef[[1]] * models[[i]]@xmatrix[[1]][, \"x2\"])\n  beta11 &lt;- sum(models[[i]]@coef[[1]] * models[[i]]@xmatrix[[1]][, \"x1\"]^2)\n  beta22 &lt;- sum(models[[i]]@coef[[1]] * models[[i]]@xmatrix[[1]][, \"x2\"]^2)\n  beta12 &lt;- 2 * sum(models[[i]]@coef[[1]] * apply(models[[i]]@xmatrix[[1]], 1, prod))\n  hyperplane &lt;- c(-models[[i]]@b, beta1, beta2, beta11, beta22, beta12)\n  names(hyperplane) &lt;- c(\"b\", \"beta1\", \"beta2\", \"beta11\", \"beta22\", \"beta12\")\n  message(\"Hyperplane coefficients:\")\n  print(round(hyperplane, 4))\n\n  # misclassified objects\n  message(\"Misclassified instances:\")\n  print(which(models[[i]]@ymatrix != as.integer(models[[i]]@fitted)))\n}\n\n-------\nC = 1\n-------\n\n\nSupport vectors:\n\n\n[1] 1 2 3 7 9\n\n\nAlpha values for support vectors:\n\n\n[1] 0.2356 0.5009 0.6532 1.0000 0.0814\n\n\nHyperplane coefficients:\n\n\n      b   beta1   beta2  beta11  beta22  beta12 \n-3.4455  0.3892  1.1899 -0.1674 -0.0721  0.0539 \n\n\nMisclassified instances:\n\n\n[1] 7\n\n\n-------\nC = 5\n-------\nSupport vectors:\n\n\n[1] 2 3 6 7 9\n\n\nAlpha values for support vectors:\n\n\n[1] 0.0865 2.1573 0.2357 5.0000 2.9919\n\n\nHyperplane coefficients:\n\n\n      b   beta1   beta2  beta11  beta22  beta12 \n-5.2716  0.1297  3.3860 -0.9460 -0.8234  1.3451 \n\n\nMisclassified instances:\n\n\ninteger(0)\n\n\n-------\nC = 100\n-------\nSupport vectors:\n\n\n[1] 2 3 6 7 9\n\n\nAlpha values for support vectors:\n\n\n[1] 0.0152 2.7412 0.2814 6.3923 3.9173\n\n\nHyperplane coefficients:\n\n\n      b   beta1   beta2  beta11  beta22  beta12 \n-6.2803  0.1622  4.2330 -1.2444 -1.0915  1.8056 \n\n\nMisclassified instances:\n\n\ninteger(0)",
    "crumbs": [
      "분류분석",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>9장 서포트 벡터 머신</span>"
    ]
  },
  {
    "objectID": "chapters/ch11_cluster.html",
    "href": "chapters/ch11_cluster.html",
    "title": "11장 군집분석 개요",
    "section": "",
    "text": "(예 11.1)\n데이터 읽기\ndat1 &lt;- read.csv(\"data/ch11_dat1.csv\")\n\ndat2 &lt;- dat1[, -1]\n유클리디안 거리\nD1 &lt;- dist(dat2)\nD1 &lt;- round(D1, 2)\nD1\n\n       1     2     3     4     5     6     7     8     9\n2   8.31                                                \n3  24.74 16.76                                          \n4  16.91  9.11  7.87                                    \n5  15.17  9.43 12.08  6.48                              \n6  10.10  3.00 16.55  9.49 11.31                        \n7  27.17 19.05  3.16 10.39 15.17 18.49                  \n8  27.87 20.35 10.05 13.15 19.10 19.52  8.31            \n9  33.03 25.57 14.53 18.57 24.52 24.52 12.04  5.48      \n10 24.37 17.75 11.75 11.66 17.03 17.49 11.49  5.74 10.05\n각 변수의 평균\nmean_vector &lt;- colMeans(dat2)\nmean_vector\n\n  X1   X2   X3 \n36.8  8.5  7.8\n각 변수의 표준편차\ncov_matrix &lt;- cov(dat2)\nsqrt(cov_matrix)\n\nWarning in sqrt(cov_matrix): NaNs produced\n\n\n         X1       X2       X3\nX1 9.738811      NaN      NaN\nX2      NaN 4.478343 1.490712\nX3      NaN 1.490712 4.661902",
    "crumbs": [
      "군집분석",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>11장 군집분석 개요</span>"
    ]
  },
  {
    "objectID": "chapters/ch11_cluster.html#예-11.3",
    "href": "chapters/ch11_cluster.html#예-11.3",
    "title": "11장 군집분석 개요",
    "section": "(예 11.3)",
    "text": "(예 11.3)\n상관계수\n\nrow_cor &lt;- cor(t(dat2), use = \"pairwise.complete.obs\", method = \"pearson\")\nrow_cor\n\n           [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\n [1,] 1.0000000 0.9347195 0.6828741 0.7600371 0.5771595 0.9673518 0.7144034\n [2,] 0.9347195 1.0000000 0.8979182 0.9413798 0.8297018 0.9942708 0.9164427\n [3,] 0.6828741 0.8979182 1.0000000 0.9937702 0.9907060 0.8457246 0.9990286\n [4,] 0.7600371 0.9413798 0.9937702 1.0000000 0.9693748 0.8999271 0.9977160\n [5,] 0.5771595 0.8297018 0.9907060 0.9693748 1.0000000 0.7652812 0.9837496\n [6,] 0.9673518 0.9942708 0.8457246 0.8999271 0.7652812 1.0000000 0.8684181\n [7,] 0.7144034 0.9164427 0.9990286 0.9977160 0.9837496 0.8684181 1.0000000\n [8,] 0.8099343 0.9655028 0.9815576 0.9967479 0.9464324 0.9321377 0.9890283\n [9,] 0.8219949 0.9707253 0.9773556 0.9948498 0.9394895 0.9394895 0.9857309\n[10,] 0.8089800 0.9650783 0.9818670 0.9968776 0.9469560 0.9315479 0.9892671\n           [,8]      [,9]     [,10]\n [1,] 0.8099343 0.8219949 0.8089800\n [2,] 0.9655028 0.9707253 0.9650783\n [3,] 0.9815576 0.9773556 0.9818670\n [4,] 0.9967479 0.9948498 0.9968776\n [5,] 0.9464324 0.9394895 0.9469560\n [6,] 0.9321377 0.9394895 0.9315479\n [7,] 0.9890283 0.9857309 0.9892671\n [8,] 1.0000000 0.9997823 0.9999987\n [9,] 0.9997823 1.0000000 0.9997471\n[10,] 0.9999987 0.9997471 1.0000000\n\n\n표준화한 후 상관계수\n\ns_dat2 &lt;- scale(dat2)\nrow_cor_s &lt;- cor(t(s_dat2), use = \"pairwise.complete.obs\", method = \"pearson\")\nrow_cor_s\n\n            [,1]       [,2]        [,3]        [,4]       [,5]        [,6]\n [1,]  1.0000000  0.9999315 -0.67406561  0.16745765  0.1015587  0.97213475\n [2,]  0.9999315  1.0000000 -0.66537519  0.17898338  0.1131937  0.96932487\n [3,] -0.6740656 -0.6653752  1.00000000  0.61536345  0.6663950 -0.82844400\n [4,]  0.1674577  0.1789834  0.61536345  1.00000000  0.9977886 -0.06832112\n [5,]  0.1015587  0.1131937  0.66639497  0.99778861  1.0000000 -0.13448190\n [6,]  0.9721348  0.9693249 -0.82844400 -0.06832112 -0.1344819  1.00000000\n [7,] -0.8797846 -0.8741613  0.94417672  0.32133338  0.3835649 -0.96670729\n [8,] -0.8006556 -0.8076120  0.09713788 -0.72474074 -0.6773407 -0.63789659\n [9,] -0.7482030 -0.7559160  0.01425163 -0.77939352 -0.7360260 -0.57182171\n[10,] -0.7956097 -0.8026447  0.08879897 -0.73048596 -0.6834783 -0.63142435\n            [,7]        [,8]        [,9]       [,10]\n [1,] -0.8797846 -0.80065556 -0.74820299 -0.79560974\n [2,] -0.8741613 -0.80761196 -0.75591598 -0.80264471\n [3,]  0.9441767  0.09713788  0.01425163  0.08879897\n [4,]  0.3213334 -0.72474074 -0.77939352 -0.73048596\n [5,]  0.3835649 -0.67734074 -0.73602598 -0.68347830\n [6,] -0.9667073 -0.63789659 -0.57182171 -0.63142435\n [7,]  1.0000000  0.41959679  0.34286201  0.41197990\n [8,]  0.4195968  1.00000000  0.99655423  0.99996493\n [9,]  0.3428620  0.99655423  1.00000000  0.99721394\n[10,]  0.4119799  0.99996493  0.99721394  1.00000000",
    "crumbs": [
      "군집분석",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>11장 군집분석 개요</span>"
    ]
  },
  {
    "objectID": "chapters/ch12_cluster.html",
    "href": "chapters/ch12_cluster.html",
    "title": "12장 계층적 군집방법",
    "section": "",
    "text": "(예 12.1)",
    "crumbs": [
      "군집분석",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>12장 계층적 군집방법</span>"
    ]
  },
  {
    "objectID": "chapters/ch12_cluster.html#예-12.1",
    "href": "chapters/ch12_cluster.html#예-12.1",
    "title": "12장 계층적 군집방법",
    "section": "",
    "text": "데이터 로드\n\ndat1 &lt;- read.csv(\"data/ch12_dat1.csv\")\ndat2 &lt;- dat1[, -1]\n\n\n\n유클리디안 거리\n\nD1 &lt;- dist(dat2)\nround(D1, 2)\n\n       1     2     3     4     5     6     7     8     9\n2   2.24                                                \n3  11.31  9.22                                          \n4   7.81  5.83  3.61                                    \n5  11.40  9.22  1.41  4.12                              \n6   1.41  2.24 11.40  8.06 11.31                        \n7  10.63  8.60  1.00  2.83  2.24 10.82                  \n8  10.05  9.49  9.22  7.21 10.44 11.18  8.25            \n9  11.40 11.18 11.40  9.43 12.65 12.65 10.44  2.24      \n10 12.37 12.08 11.70 10.00 13.00 13.60 10.77  2.83  1.00\n\n\n\n\n평균연결법\n\nhc_c &lt;- hclust(D1, method = \"average\")\n\n\n\n결과 시각화\n\nplot(hc_c,\n     hang = -1, cex = 0.7, main = \"Average linkage with Euclidean distance\",\n     ylab = \"Distance\", xlab = \"observation\"\n)",
    "crumbs": [
      "군집분석",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>12장 계층적 군집방법</span>"
    ]
  },
  {
    "objectID": "chapters/ch12_cluster.html#예-12.2",
    "href": "chapters/ch12_cluster.html#예-12.2",
    "title": "12장 계층적 군집방법",
    "section": "(예 12.2)",
    "text": "(예 12.2)\n\n데이터 로드\n\ndat1 &lt;- read.csv(\"data/ch12_dat2.csv\")\ndat2 &lt;- dat1[, -1]\n\n\n\n제곱 유클리디안 거리\n\nD1 &lt;- dist(dat2)\nD1^2\n\n    1   2   3   4   5   6   7\n2 260                        \n3   5 289                    \n4 346  82 337                \n5 173  25 212 173            \n6  32 148  29 170 117        \n7 234   2 257  64  29 122    \n8 277  53 274   5 122 125  37\n\n\n\n\n워드 방법\n\nhc_c &lt;- hclust(D1, method = \"ward.D2\")\n\n\n\n결과 시각화\n\nplot(hc_c,\n     hang = -1, cex = 1, main = \"Ward's linkage with Euclidean distance\",\n     ylab = \"Distance\", xlab = \"observation\"\n)",
    "crumbs": [
      "군집분석",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>12장 계층적 군집방법</span>"
    ]
  },
  {
    "objectID": "chapters/ch12_cluster.html#예-12.3",
    "href": "chapters/ch12_cluster.html#예-12.3",
    "title": "12장 계층적 군집방법",
    "section": "(예 12.3)",
    "text": "(예 12.3)\n\n패키지 로드\n\nlibrary(cluster)\n\n\n\n데이터 로드\n\ndat1 &lt;- read.csv(\"data/ch12_dat3.csv\")\ndat2 &lt;- dat1[, -1]\n\n\n\n다이아나 방법\n\nclus_diana &lt;- diana(dat2)\n\n\n\n결과 시각화\n\nplot(clus_diana, which.plots = 2, main = \"Dendrogram of Diana\")",
    "crumbs": [
      "군집분석",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>12장 계층적 군집방법</span>"
    ]
  },
  {
    "objectID": "chapters/ch12_cluster.html#예-12.4",
    "href": "chapters/ch12_cluster.html#예-12.4",
    "title": "12장 계층적 군집방법",
    "section": "(예 12.4)",
    "text": "(예 12.4)\n\n패키지 로드\n\nlibrary(factoextra)\n\nLoading required package: ggplot2\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\n\n\n\n데이터 로드\n\ndat1 &lt;- read.csv(\"data/ch12_dat2.csv\")\ndat2 &lt;- dat1[, -1]\n\n\n\n워드 방법 군집수 결정 - average silhouette width\n\nfviz_nbclust(dat2, hcut, method = \"silhouette\", k.max = 7)",
    "crumbs": [
      "군집분석",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>12장 계층적 군집방법</span>"
    ]
  },
  {
    "objectID": "chapters/ch13_cluster_dbscan.html",
    "href": "chapters/ch13_cluster_dbscan.html",
    "title": "13장 비계층적 군집방법",
    "section": "",
    "text": "(예 13.1)",
    "crumbs": [
      "군집분석",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>13장 비계층적 군집방법</span>"
    ]
  },
  {
    "objectID": "chapters/ch13_cluster_dbscan.html#예-13.1",
    "href": "chapters/ch13_cluster_dbscan.html#예-13.1",
    "title": "13장 비계층적 군집방법",
    "section": "",
    "text": "패키지 로드\n\nlibrary(factoextra)\n\nLoading required package: ggplot2\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\n\n\n\n데이터 로드\n\ndat1 &lt;- read.csv(\"data/ch12_dat1.csv\")\ndat2 &lt;- dat1[, -1]\n\n\n\n군집 수 결정\n\n# may use scale data or raw data for the optimal k\ns_dat2 &lt;- scale(dat2)\n\nfviz_nbclust(s_dat2, kmeans, method = \"silhouette\", k.max = 5)\n\n\n\n\n\n\n\n\n\n\nK-means\n\nset.seed(123)\nkm &lt;- kmeans(dat2, 3)\nkm\n\nK-means clustering with 3 clusters of sizes 4, 3, 3\n\nCluster means:\n         X1    X2\n1 13.250000  6.75\n2  3.666667  3.00\n3  7.000000 14.00\n\nClustering vector:\n [1] 3 3 1 1 1 3 1 2 2 2\n\nWithin cluster sum of squares by cluster:\n[1] 11.500000  4.666667  4.000000\n (between_SS / total_SS =  94.6 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\n\n결과 시각화\n\nfviz_cluster(km,\n  data = dat2,\n  ellipse.type = \"convex\",\n  stand = FALSE,\n  repel = TRUE, cex = 3\n)\n\nWarning: Duplicated aesthetics after name standardisation: size",
    "crumbs": [
      "군집분석",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>13장 비계층적 군집방법</span>"
    ]
  },
  {
    "objectID": "chapters/ch13_cluster_dbscan.html#예-13.3",
    "href": "chapters/ch13_cluster_dbscan.html#예-13.3",
    "title": "13장 비계층적 군집방법",
    "section": "(예 13.3)",
    "text": "(예 13.3)\n\n패키지 로드\n\nlibrary(cluster)\nlibrary(factoextra)\n\n\n\n데이터 로드\n\ndat1 &lt;- read.csv(\"data/ch13_pam.csv\")\ndat2 &lt;- dat1[, -1]\n\n\n\nPAM\n\npam_out &lt;- pam(dat2, 2)\npam_out\n\nMedoids:\n     ID experience hours\n[1,]  2          5     4\n[2,]  5         14     6\nClustering vector:\n[1] 1 1 2 2 2 2\nObjective function:\n   build     swap \n1.383427 1.375972 \n\nAvailable components:\n [1] \"medoids\"    \"id.med\"     \"clustering\" \"objective\"  \"isolation\" \n [6] \"clusinfo\"   \"silinfo\"    \"diss\"       \"call\"       \"data\"      \n\n\n각 군집에 속한 객체 수\n\ntable(pam_out$clustering)\n\n\n1 2 \n2 4 \n\n\n\n\n결과 시각화\n\nfviz_cluster(pam_out,\n  data = dat2,\n  ellipse.type = \"convex\",\n  stand = FALSE,\n  repel = TRUE\n)",
    "crumbs": [
      "군집분석",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>13장 비계층적 군집방법</span>"
    ]
  },
  {
    "objectID": "chapters/ch13_cluster_dbscan.html#예-13.8",
    "href": "chapters/ch13_cluster_dbscan.html#예-13.8",
    "title": "13장 비계층적 군집방법",
    "section": "(예 13.8)",
    "text": "(예 13.8)\n\n패키지 로드\n\nlibrary(fpc)\nlibrary(factoextra)\n\n\n\n데이터 로드\n\ndat1 &lt;- read.csv(\"data/ch12_dat1.csv\")\ndat2 &lt;- dat1[, -1]\n\n\n\n디비스캔\n\ndb &lt;- dbscan(dat2, eps = 2.5, MinPts = 3)\ndb\n\ndbscan Pts=10 MinPts=3 eps=2.5\n       0 1 2 3\nborder 1 0 0 2\nseed   0 3 3 1\ntotal  1 3 3 3\n\n\n군집 결과\n\ndb$cluster\n\n [1] 1 1 2 0 2 1 2 3 3 3\n\n\n\n\n결과 시각화\n\nfviz_cluster(db,\n  data = dat2,\n  ellipse.type = \"convex\",\n  stand = FALSE,\n  repel = TRUE\n)",
    "crumbs": [
      "군집분석",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>13장 비계층적 군집방법</span>"
    ]
  },
  {
    "objectID": "chapters/ch15_association_rule.html",
    "href": "chapters/ch15_association_rule.html",
    "title": "15장 연관규칙",
    "section": "",
    "text": "(예 15.3 - 15.4)",
    "crumbs": [
      "연관규칙",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>15장 연관규칙</span>"
    ]
  },
  {
    "objectID": "chapters/ch15_association_rule.html#예-15.3---15.4",
    "href": "chapters/ch15_association_rule.html#예-15.3---15.4",
    "title": "15장 연관규칙",
    "section": "",
    "text": "패키지 로드\n\nlibrary(arules)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'arules'\n\n\nThe following objects are masked from 'package:base':\n\n    abbreviate, write\n\n\n\n\n데이터 로드\n\ndf &lt;- read.csv(\"data/ch15_transaction.csv\")\n\n\n\n데이터 변환\n\n# convert a data frame into a list of transactions\ntransaction_list &lt;- split(df$item, df$id)\nprint(transaction_list)\n\n$`1`\n[1] \"b\" \"c\" \"g\"\n\n$`2`\n[1] \"a\" \"b\" \"d\" \"e\" \"f\"\n\n$`3`\n[1] \"a\" \"b\" \"c\" \"g\"\n\n$`4`\n[1] \"b\" \"c\" \"e\" \"f\"\n\n$`5`\n[1] \"b\" \"c\" \"e\" \"f\" \"g\"\n\n# convert a list into an object of class \"transactions\"\ntransactions &lt;- as(transaction_list, \"transactions\")\nprint(transactions)\n\ntransactions in sparse format with\n 5 transactions (rows) and\n 7 items (columns)\n\n\n\n\n(예 15.3) 빈발항목집합 생성: 최소 지지도 0.4\n\nitemsets &lt;- apriori(\n  transactions,\n  parameter = list(\n    support = 0.4,\n    target = \"frequent itemsets\"\n  ),\n  control = list(verbose = FALSE)  # do not print progress\n)\n\ninspect(itemsets)\n\n     items        support count\n[1]  {a}          0.4     2    \n[2]  {g}          0.6     3    \n[3]  {e}          0.6     3    \n[4]  {f}          0.6     3    \n[5]  {c}          0.8     4    \n[6]  {b}          1.0     5    \n[7]  {a, b}       0.4     2    \n[8]  {c, g}       0.6     3    \n[9]  {b, g}       0.6     3    \n[10] {e, f}       0.6     3    \n[11] {c, e}       0.4     2    \n[12] {b, e}       0.6     3    \n[13] {c, f}       0.4     2    \n[14] {b, f}       0.6     3    \n[15] {b, c}       0.8     4    \n[16] {b, c, g}    0.6     3    \n[17] {c, e, f}    0.4     2    \n[18] {b, e, f}    0.6     3    \n[19] {b, c, e}    0.4     2    \n[20] {b, c, f}    0.4     2    \n[21] {b, c, e, f} 0.4     2    \n\n\n\n\n(예 15.4) 규칙 탐사: 최소 신뢰도 0.7\n\nrules &lt;- ruleInduction(\n  itemsets,\n  transactions,\n  confidence = 0.7,\n  method = \"apriori\"\n)\n\ninspect(rules)\n\n     lhs          rhs support confidence coverage lift     count\n[1]  {}        =&gt; {c} 0.8     0.80       1.0      1.000000 4    \n[2]  {}        =&gt; {b} 1.0     1.00       1.0      1.000000 5    \n[3]  {a}       =&gt; {b} 0.4     1.00       0.4      1.000000 2    \n[4]  {g}       =&gt; {c} 0.6     1.00       0.6      1.250000 3    \n[5]  {c}       =&gt; {g} 0.6     0.75       0.8      1.250000 3    \n[6]  {g}       =&gt; {b} 0.6     1.00       0.6      1.000000 3    \n[7]  {e}       =&gt; {f} 0.6     1.00       0.6      1.666667 3    \n[8]  {f}       =&gt; {e} 0.6     1.00       0.6      1.666667 3    \n[9]  {e}       =&gt; {b} 0.6     1.00       0.6      1.000000 3    \n[10] {f}       =&gt; {b} 0.6     1.00       0.6      1.000000 3    \n[11] {c}       =&gt; {b} 0.8     1.00       0.8      1.000000 4    \n[12] {b}       =&gt; {c} 0.8     0.80       1.0      1.000000 4    \n[13] {c, g}    =&gt; {b} 0.6     1.00       0.6      1.000000 3    \n[14] {b, g}    =&gt; {c} 0.6     1.00       0.6      1.250000 3    \n[15] {b, c}    =&gt; {g} 0.6     0.75       0.8      1.250000 3    \n[16] {c, e}    =&gt; {f} 0.4     1.00       0.4      1.666667 2    \n[17] {c, f}    =&gt; {e} 0.4     1.00       0.4      1.666667 2    \n[18] {e, f}    =&gt; {b} 0.6     1.00       0.6      1.000000 3    \n[19] {b, e}    =&gt; {f} 0.6     1.00       0.6      1.666667 3    \n[20] {b, f}    =&gt; {e} 0.6     1.00       0.6      1.666667 3    \n[21] {c, e}    =&gt; {b} 0.4     1.00       0.4      1.000000 2    \n[22] {c, f}    =&gt; {b} 0.4     1.00       0.4      1.000000 2    \n[23] {c, e, f} =&gt; {b} 0.4     1.00       0.4      1.000000 2    \n[24] {b, c, e} =&gt; {f} 0.4     1.00       0.4      1.666667 2    \n[25] {b, c, f} =&gt; {e} 0.4     1.00       0.4      1.666667 2    \n\n\n\n\n빈발항목집합 + 규칙탐사\n\napriori_results &lt;- apriori(\n  transactions,\n  parameter = list(\n    support = 0.4,\n    confidence = 0.7,\n    target = \"rules\"\n  ),\n  control = list(verbose = FALSE)  # do not print progress\n)\n\ninspect(apriori_results)\n\n     lhs          rhs support confidence coverage lift     count\n[1]  {}        =&gt; {c} 0.8     0.80       1.0      1.000000 4    \n[2]  {}        =&gt; {b} 1.0     1.00       1.0      1.000000 5    \n[3]  {a}       =&gt; {b} 0.4     1.00       0.4      1.000000 2    \n[4]  {g}       =&gt; {c} 0.6     1.00       0.6      1.250000 3    \n[5]  {c}       =&gt; {g} 0.6     0.75       0.8      1.250000 3    \n[6]  {g}       =&gt; {b} 0.6     1.00       0.6      1.000000 3    \n[7]  {e}       =&gt; {f} 0.6     1.00       0.6      1.666667 3    \n[8]  {f}       =&gt; {e} 0.6     1.00       0.6      1.666667 3    \n[9]  {e}       =&gt; {b} 0.6     1.00       0.6      1.000000 3    \n[10] {f}       =&gt; {b} 0.6     1.00       0.6      1.000000 3    \n[11] {c}       =&gt; {b} 0.8     1.00       0.8      1.000000 4    \n[12] {b}       =&gt; {c} 0.8     0.80       1.0      1.000000 4    \n[13] {c, g}    =&gt; {b} 0.6     1.00       0.6      1.000000 3    \n[14] {b, g}    =&gt; {c} 0.6     1.00       0.6      1.250000 3    \n[15] {b, c}    =&gt; {g} 0.6     0.75       0.8      1.250000 3    \n[16] {c, e}    =&gt; {f} 0.4     1.00       0.4      1.666667 2    \n[17] {c, f}    =&gt; {e} 0.4     1.00       0.4      1.666667 2    \n[18] {e, f}    =&gt; {b} 0.6     1.00       0.6      1.000000 3    \n[19] {b, e}    =&gt; {f} 0.6     1.00       0.6      1.666667 3    \n[20] {b, f}    =&gt; {e} 0.6     1.00       0.6      1.666667 3    \n[21] {c, e}    =&gt; {b} 0.4     1.00       0.4      1.000000 2    \n[22] {c, f}    =&gt; {b} 0.4     1.00       0.4      1.000000 2    \n[23] {c, e, f} =&gt; {b} 0.4     1.00       0.4      1.000000 2    \n[24] {b, c, e} =&gt; {f} 0.4     1.00       0.4      1.666667 2    \n[25] {b, c, f} =&gt; {e} 0.4     1.00       0.4      1.666667 2",
    "crumbs": [
      "연관규칙",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>15장 연관규칙</span>"
    ]
  },
  {
    "objectID": "chapters/ch15_association_rule.html#예-15.7---15.9",
    "href": "chapters/ch15_association_rule.html#예-15.7---15.9",
    "title": "15장 연관규칙",
    "section": "(예 15.7 - 15.9)",
    "text": "(예 15.7 - 15.9)\n본 R코드는 교재에 나온 AprioriAll이나 AprioriSome이 아닌, SPADE라는 알고리즘을 이용하여 구현한 R 패키지를 사용하였습니다. 수행 결과는 동일하며, SPADE 알고리즘이 효율성이 더 높은 것으로 알려져 있습니다.\nSPADE 알고리즘은 아래 논문에 소개되어 있습니다.\nZaki, M. J. (2001). SPADE: An efficient algorithm for mining frequent sequences. Machine learning, 42, 31-60.\n\n패키지 로드\n\nlibrary(arulesSequences)\n\n\nAttaching package: 'arulesSequences'\n\n\nThe following object is masked from 'package:arules':\n\n    itemsets\n\n\n\n\n데이터 로드\n\n# read specifically formatted data into transactions with temporal information\ntrans &lt;- read_baskets(\n  con = \"data/ch15_sequence.txt\",\n  info = c(\"sequenceID\", \"eventID\", \"SIZE\") # do not change the names\n)\n\n\n\n(예 15.7) 최대 시퀀스\n모든 시퀀스 (최소 지지도 0.4)\n\nseqs &lt;- cspade(trans, parameter = list(support = 0.4))\ninspect(seqs)\n\n   items support \n 1 &lt;{a}&gt;     0.8 \n 2 &lt;{b}&gt;     0.6 \n 3 &lt;{e}&gt;     0.4 \n 4 &lt;{g}&gt;     0.6 \n 5 &lt;{a},  \n    {g}&gt;     0.4 \n 6 &lt;{e,   \n     g}&gt;     0.4 \n 7 &lt;{a},  \n    {e,   \n     g}&gt;     0.4 \n 8 &lt;{a},  \n    {e}&gt;     0.4 \n 9 &lt;{a},  \n    {b}&gt;     0.4 \n \n\n\n최대 시퀀스 (최소 지지도 0.4)\n\nmax_seqs &lt;- subset(seqs, is.maximal(seqs))\ninspect(max_seqs)\n\n   items support \n 1 &lt;{a},  \n    {e,   \n     g}&gt;     0.4 \n 2 &lt;{a},  \n    {b}&gt;     0.4 \n \n\n\n\n\n(예 15.8) 빈발항목집합\n\nitemsets &lt;- cspade(trans, parameter = list(support = 0.4, maxlen = 1))\ninspect(itemsets)\n\n   items support \n 1 &lt;{a}&gt;     0.8 \n 2 &lt;{b}&gt;     0.6 \n 3 &lt;{e}&gt;     0.4 \n 4 &lt;{g}&gt;     0.6 \n 5 &lt;{e,   \n     g}&gt;     0.4 \n \n\n\n\n\n(예 15.9) 시퀀스 탐색\n모든 빈발 시퀀스 (최소 신뢰도 0.5)\n\nrules &lt;- ruleInduction(seqs, confidence = 0.5)\ninspect(rules)\n\n   lhs      rhs   support confidence      lift \n 1 &lt;{a}&gt; =&gt; &lt;{g}&gt;     0.4        0.5 0.8333333 \n 2 &lt;{a}&gt; =&gt; &lt;{e,      0.4        0.5 1.2500000 \n              g}&gt;    \n 3 &lt;{a}&gt; =&gt; &lt;{e}&gt;     0.4        0.5 1.2500000 \n 4 &lt;{a}&gt; =&gt; &lt;{b}&gt;     0.4        0.5 0.8333333 \n \n\n\n최대 빈발 시퀀스 (최소 신뢰도 0.5)\n\nmax_rules &lt;- ruleInduction(max_seqs, trans, confidence = 0.5)\n\nWarning in match(x@items, table@items, nomatch = nomatch, incomparables =\nincomparables): Item coding not compatible, recoding item matrices first.\n\n\nWarning in .local(x, ...): Item coding not compatible, recoding item matrices.\n\ninspect(max_rules)\n\n   lhs      rhs   support confidence      lift \n 1 &lt;{a}&gt; =&gt; &lt;{e,      0.4        0.5 1.2500000 \n              g}&gt;    \n 2 &lt;{a}&gt; =&gt; &lt;{b}&gt;     0.4        0.5 0.8333333",
    "crumbs": [
      "연관규칙",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>15장 연관규칙</span>"
    ]
  },
  {
    "objectID": "chapters/ch16_recommender_system.html",
    "href": "chapters/ch16_recommender_system.html",
    "title": "16장 추천시스템",
    "section": "",
    "text": "(예 16.2)",
    "crumbs": [
      "연관규칙",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>16장 추천시스템</span>"
    ]
  },
  {
    "objectID": "chapters/ch16_recommender_system.html#예-16.2",
    "href": "chapters/ch16_recommender_system.html#예-16.2",
    "title": "16장 추천시스템",
    "section": "",
    "text": "데이터 로드\n\ndf &lt;- read.csv(\"data/ch16_content.csv\")\nuser_weights &lt;- as.vector(df[, 2])\ndoc_weights &lt;- as.matrix(df[, -c(1, 2)])\n\n\n\n유용도 산출\n\nnumerator &lt;- t(user_weights) %*% doc_weights\ndenominator &lt;- norm(user_weights, type = \"2\") *\n  apply(doc_weights, 2, norm, type = \"2\")\nutility &lt;- numerator / denominator\nround(utility, 4)\n\n       doc1   doc2   doc3   doc4   doc5   doc6\n[1,] 0.6595 0.8409 0.9189 0.4478 0.9717 0.3728",
    "crumbs": [
      "연관규칙",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>16장 추천시스템</span>"
    ]
  },
  {
    "objectID": "chapters/ch16_recommender_system.html#예-16.3",
    "href": "chapters/ch16_recommender_system.html#예-16.3",
    "title": "16장 추천시스템",
    "section": "(예 16.3)",
    "text": "(예 16.3)\n\n패키지 로드\n\nlibrary(\"recommenderlab\")\n\nLoading required package: Matrix\n\n\nLoading required package: arules\n\n\n\nAttaching package: 'arules'\n\n\nThe following objects are masked from 'package:base':\n\n    abbreviate, write\n\n\nLoading required package: proxy\n\n\n\nAttaching package: 'proxy'\n\n\nThe following object is masked from 'package:Matrix':\n\n    as.matrix\n\n\nThe following objects are masked from 'package:stats':\n\n    as.dist, dist\n\n\nThe following object is masked from 'package:base':\n\n    as.matrix\n\n\nRegistered S3 methods overwritten by 'registry':\n  method               from \n  print.registry_field proxy\n  print.registry_entry proxy\n\n\n\n\n데이터 로드\n\ndf &lt;- read.csv(\"data/ch16_ratings.csv\")\n\n\n\n평점 행렬로 변환\n\nratings &lt;- as(df, \"realRatingMatrix\")\nprint(ratings@data)\n\n7 x 7 sparse Matrix of class \"dgCMatrix\"\n  1 2 3 4 5 6 7\n1 5 . 4 . 1 0 3\n2 4 4 4 . . . 1\n3 5 4 . 1 2 . 3\n4 1 2 1 4 3 5 2\n5 0 1 . 3 5 5 .\n6 . 2 . . 4 4 2\n7 5 . . 1 . . 2\n\n\n\n\n목표고객 설정\n\ntarget &lt;- 7\n\n\n\n목표고객과 각 고객간의 유사성 산출\n\ncentered &lt;- normalize(ratings)\nsimilarity(centered[-target], centered[target])\n\n           7\n1 0.95163875\n2 0.78234196\n3 0.98038446\n4 0.06238169\n5 0.07346149\n6 1.00000000\n\n\n\n\n협업 필터링 추천 시스템 생성\n\nrec &lt;- Recommender(ratings, method = \"UBCF\")\nrec\n\nRecommender of type 'UBCF' for 'realRatingMatrix' \nlearned using 7 users.\n\n\n\n\n평점 추정\n\npredicted &lt;- predict(rec, ratings[target], type = \"ratings\")\nprint(predicted@data)\n\n1 x 7 sparse Matrix of class \"dgCMatrix\"\n  1        2        3 4        5        6 7\n7 . 2.804412 3.680394 . 2.238142 2.110424 .\n\n\n\n\n상위-N 추천\n\nrecommended &lt;- predict(rec, ratings[target], n = 2)\nprint(recommended@items)\n\n$`0`\n[1] 3 2",
    "crumbs": [
      "연관규칙",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>16장 추천시스템</span>"
    ]
  },
  {
    "objectID": "chapters/ch16_recommender_system.html#예-16.4",
    "href": "chapters/ch16_recommender_system.html#예-16.4",
    "title": "16장 추천시스템",
    "section": "(예 16.4)",
    "text": "(예 16.4)\n\n패키지 로드\n\nlibrary(\"recommenderlab\")\n\n\n\n데이터 로드\n\ndf &lt;- read.csv(\"data/ch16_purchase.csv\")\n\n\n\n시장바구니 데이터 행렬로 변환\n\npurchases &lt;- as(df, \"binaryRatingMatrix\")\nmat &lt;- as(purchases@data, \"matrix\")\nmat\n\n      1     2     3     4     5     6     7\n1  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE\n2  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE\n3  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE\n4  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n5 FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE\n6 FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n7  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE\n\n\n\n\n목표고객 설정\n\ntarget &lt;- 7\n\n\n\n목표고객과 각 고객간의 유사성 산출\n\nsim &lt;- similarity(purchases[-target], purchases[target], method = \"jaccard\")\nround(sim, 4)\n\n       7\n1 0.4000\n2 0.4000\n3 0.6000\n4 0.5000\n5 0.1667\n6 0.1667\n\n\n\n\n평점 추정\n\ntarget_items &lt;- !mat[target, ]\npredicted &lt;- 1 / sum(sim) *\n  t(sim) %*% as(purchases[-target], \"matrix\")[, target_items]\nround(predicted, 4)\n\n       2      3     5      6\n7 0.8209 0.5821 0.597 0.3731",
    "crumbs": [
      "연관규칙",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>16장 추천시스템</span>"
    ]
  }
]