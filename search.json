[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "데이터마이닝: R 예제",
    "section": "",
    "text": "소개\n데이터마이닝 교재의 예제 R 코드입니다."
  },
  {
    "objectID": "chapters/ch02_regression.html#예-2.3---2.5-2.7-2.10---2.11",
    "href": "chapters/ch02_regression.html#예-2.3---2.5-2.7-2.10---2.11",
    "title": "2장 회귀분석",
    "section": "(예 2.3 - 2.5, 2.7, 2.10 - 2.11)",
    "text": "(예 2.3 - 2.5, 2.7, 2.10 - 2.11)\n\n데이터 로드\n\ndat1 &lt;- read.csv(\"data/ch2_reg1.csv\")\n\n\n\n(예 2.3) 회귀계수 추정\n\nlm_fit &lt;- lm(weight ~ age + height, data = dat1)\ncoef(lm_fit)\n\n (Intercept)          age       height \n-108.1671993    0.3291212    0.9552913 \n\n\n\n\n(예 2.4) 오차항 분산 추정\n\nsum(lm_fit$residuals^2) / lm_fit$df.residual\n\n[1] 7.038464\n\n\n\n\n(예 2.5) 회귀성검정\n\nanova(lm_fit)\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df  Sum Sq Mean Sq F value   Pr(&gt;F)   \nage        1 119.299 119.299   16.95 0.004476 **\nheight     1 107.831 107.831   15.32 0.005793 **\nResiduals  7  49.269   7.038                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n(예 2.7) 회귀계수검정\n\nsummary(lm_fit)\n\n\nCall:\nlm(formula = weight ~ age + height, data = dat1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3398 -1.6623 -0.3084  1.3727  4.1911 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -108.16720   42.12277  -2.568  0.03712 * \nage            0.32912    0.06924   4.753  0.00208 **\nheight         0.95529    0.24406   3.914  0.00579 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.653 on 7 degrees of freedom\nMultiple R-squared:  0.8217,    Adjusted R-squared:  0.7708 \nF-statistic: 16.13 on 2 and 7 DF,  p-value: 0.002391\n\n\n\n\n(예 2.10) 평균반응치 추정\n추정회귀계수벡터의 분산-공분산 행렬\n\nvcov(lm_fit)\n\n             (Intercept)          age        height\n(Intercept) 1774.3280624 -0.671107283 -10.264885141\nage           -0.6711073  0.004794717   0.003035476\nheight       -10.2648851  0.003035476   0.059566804\n\n\n신규 데이터\n\nnewdata &lt;- data.frame(age = 40, height = 170)\n\n신규 데이터에 대한 평균반응치 추정\n\npredict(lm_fit, newdata)\n\n       1 \n67.39718 \n\n\n신규 데이터에 대한 평균반응치의 95% 신뢰구간\n\nconf_interval &lt;- predict(lm_fit, newdata, interval = \"confidence\", level = 0.95)\nconf_interval[, c(\"lwr\", \"upr\")]\n\n     lwr      upr \n65.01701 69.77735 \n\n\n\n\n(예 2.11) 미래반응치 예측\n신규 데이터에 대한 미래반응치의 95% 예측구간\n\npred_interval &lt;- predict(lm_fit, newdata, interval = \"prediction\", level = 0.95)\npred_interval[, c(\"lwr\", \"upr\")]\n\n     lwr      upr \n60.68745 74.10690"
  },
  {
    "objectID": "chapters/ch02_regression.html#예-2.8---2.9-2.13",
    "href": "chapters/ch02_regression.html#예-2.8---2.9-2.13",
    "title": "2장 회귀분석",
    "section": "(예 2.8 - 2.9, 2.13)",
    "text": "(예 2.8 - 2.9, 2.13)\n\n패키지 로드\n\nlibrary(\"olsrr\")\n\n\nAttaching package: 'olsrr'\n\n\nThe following object is masked from 'package:datasets':\n\n    rivers\n\nlibrary(\"car\")\n\nLoading required package: carData\n\n\n\n\n데이터 로드\n\ndat_ba &lt;- read.csv(\"data/Player.csv\")\n\n\n\n다중 회귀모형 추정\n\nlm_model &lt;- lm(Salary ~ Hits + Walks + CRuns + HmRun + CWalks, data = dat_ba)\nsummary(lm_model)\n\n\nCall:\nlm(formula = Salary ~ Hits + Walks + CRuns + HmRun + CWalks, \n    data = dat_ba)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-858.19 -184.25  -40.46  120.95 2183.89 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -61.7447    57.0968  -1.081  0.28053    \nHits          1.3558     0.7568   1.791  0.07441 .  \nWalks         4.9172     1.5771   3.118  0.00203 ** \nCRuns         1.0212     0.2123   4.810 2.58e-06 ***\nHmRun         2.5378     2.9132   0.871  0.38450    \nCWalks       -0.5726     0.2821  -2.030  0.04342 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 341.1 on 257 degrees of freedom\n  (59 observations deleted due to missingness)\nMultiple R-squared:  0.4391,    Adjusted R-squared:  0.4282 \nF-statistic: 40.24 on 5 and 257 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n(예 2.8) 모든 가능한 조합의 회귀분석\n\nk &lt;- ols_step_all_possible(lm_model)\nk\n\n   Index N                    Predictors  R-Square Adj. R-Square Mallow's Cp\n3      1 1                         CRuns 0.3166062     0.3139878   54.140213\n5      2 1                        CWalks 0.2399256     0.2370135   89.276284\n2      3 1                         Walks 0.1970181     0.1939416  108.937069\n1      4 1                          Hits 0.1924355     0.1893414  111.036886\n4      5 1                         HmRun 0.1176683     0.1142877  145.296250\n7      6 2                    Hits CRuns 0.4147791     0.4102774   11.156078\n10     7 2                   Walks CRuns 0.3906078     0.3859201   22.231709\n9      8 2                   Hits CWalks 0.3853422     0.3806140   24.644480\n13     9 2                   CRuns HmRun 0.3584707     0.3535358   36.957357\n14    10 2                  CRuns CWalks 0.3240582     0.3188586   52.725631\n12    11 2                  Walks CWalks 0.3068498     0.3015179   60.610716\n15    12 2                  HmRun CWalks 0.2965559     0.2911448   65.327521\n6     13 2                    Hits Walks 0.2453786     0.2395738   88.777655\n11    14 2                   Walks HmRun 0.2240202     0.2180511   98.564374\n8     15 2                    Hits HmRun 0.2093560     0.2032741  105.283718\n16    16 3              Hits Walks CRuns 0.4287269     0.4221098    6.765004\n23    17 3            Walks CRuns CWalks 0.4269906     0.4203535    7.560585\n19    18 3              Hits CRuns HmRun 0.4178491     0.4111061   11.749353\n20    19 3             Hits CRuns CWalks 0.4147802     0.4080016   13.155578\n22    20 3             Walks CRuns HmRun 0.4021021     0.3951766   18.964848\n21    21 3             Hits HmRun CWalks 0.3874947     0.3804000   25.658175\n18    22 3             Hits Walks CWalks 0.3868621     0.3797601   25.948023\n25    23 3            CRuns HmRun CWalks 0.3647553     0.3573973   36.077637\n24    24 3            Walks HmRun CWalks 0.3290179     0.3212459   52.452998\n17    25 3              Hits Walks HmRun 0.2532017     0.2445515   87.193024\n27    26 4       Hits Walks CRuns CWalks 0.4374699     0.4287485    4.758862\n30    27 4      Walks CRuns HmRun CWalks 0.4321227     0.4233184    7.209002\n26    28 4        Hits Walks CRuns HmRun 0.4301353     0.4213001    8.119679\n29    29 4       Hits CRuns HmRun CWalks 0.4179107     0.4088860   13.721142\n28    30 4       Hits Walks HmRun CWalks 0.3886385     0.3791600   27.134046\n31    31 5 Hits Walks CRuns HmRun CWalks 0.4391260     0.4282141    6.000000\n\n\n\n\n(예 2.9) 단계별방법\n\nols_step_both_p(lm_model, details = TRUE, pent = 0.3, prem = 0.3)\n\nStepwise Selection Method   \n---------------------------\n\nCandidate Terms: \n\n1. Hits \n2. Walks \n3. CRuns \n4. HmRun \n5. CWalks \n\nWe are selecting variables based on p value...\n\n\nStepwise Selection: Step 1 \n\n- CRuns added \n\n                          Model Summary                            \n------------------------------------------------------------------\nR                       0.563       RMSE                  373.643 \nR-Squared               0.317       Coef. Var              69.719 \nAdj. R-Squared          0.314       MSE                139609.007 \nPred R-Squared          0.295       MAE                   260.516 \n------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                                   ANOVA                                    \n---------------------------------------------------------------------------\n                    Sum of                                                 \n                   Squares         DF     Mean Square       F         Sig. \n---------------------------------------------------------------------------\nRegression    16881162.032          1    16881162.032    120.917    0.0000 \nResidual      36437950.757        261      139609.007                      \nTotal         53319112.789        262                                      \n---------------------------------------------------------------------------\n\n                                    Parameter Estimates                                     \n-------------------------------------------------------------------------------------------\n      model       Beta    Std. Error    Std. Beta      t        Sig       lower      upper \n-------------------------------------------------------------------------------------------\n(Intercept)    259.082        34.127                  7.592    0.000    191.882    326.282 \n      CRuns      0.766         0.070        0.563    10.996    0.000      0.629      0.904 \n-------------------------------------------------------------------------------------------\n\n\n\nStepwise Selection: Step 2 \n\n- Hits added \n\n                          Model Summary                            \n------------------------------------------------------------------\nR                       0.644       RMSE                  346.429 \nR-Squared               0.415       Coef. Var              64.641 \nAdj. R-Squared          0.410       MSE                120013.306 \nPred R-Squared          0.389       MAE                   230.183 \n------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                                  ANOVA                                    \n--------------------------------------------------------------------------\n                    Sum of                                                \n                   Squares         DF     Mean Square      F         Sig. \n--------------------------------------------------------------------------\nRegression    22115653.209          2    11057826.604    92.138    0.0000 \nResidual      31203459.580        260      120013.306                     \nTotal         53319112.789        262                                     \n--------------------------------------------------------------------------\n\n                                    Parameter Estimates                                     \n-------------------------------------------------------------------------------------------\n      model       Beta    Std. Error    Std. Beta      t        Sig        lower     upper \n-------------------------------------------------------------------------------------------\n(Intercept)    -50.817        56.596                 -0.898    0.370    -162.262    60.627 \n      CRuns      0.661         0.067        0.486     9.939    0.000       0.530     0.792 \n       Hits      3.226         0.488        0.323     6.604    0.000       2.264     4.188 \n-------------------------------------------------------------------------------------------\n\n                          Model Summary                            \n------------------------------------------------------------------\nR                       0.644       RMSE                  346.429 \nR-Squared               0.415       Coef. Var              64.641 \nAdj. R-Squared          0.410       MSE                120013.306 \nPred R-Squared          0.389       MAE                   230.183 \n------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                                  ANOVA                                    \n--------------------------------------------------------------------------\n                    Sum of                                                \n                   Squares         DF     Mean Square      F         Sig. \n--------------------------------------------------------------------------\nRegression    22115653.209          2    11057826.604    92.138    0.0000 \nResidual      31203459.580        260      120013.306                     \nTotal         53319112.789        262                                     \n--------------------------------------------------------------------------\n\n                                    Parameter Estimates                                     \n-------------------------------------------------------------------------------------------\n      model       Beta    Std. Error    Std. Beta      t        Sig        lower     upper \n-------------------------------------------------------------------------------------------\n(Intercept)    -50.817        56.596                 -0.898    0.370    -162.262    60.627 \n      CRuns      0.661         0.067        0.486     9.939    0.000       0.530     0.792 \n       Hits      3.226         0.488        0.323     6.604    0.000       2.264     4.188 \n-------------------------------------------------------------------------------------------\n\n\n\nStepwise Selection: Step 3 \n\n- Walks added \n\n                          Model Summary                            \n------------------------------------------------------------------\nR                       0.655       RMSE                  342.936 \nR-Squared               0.429       Coef. Var              63.990 \nAdj. R-Squared          0.422       MSE                117605.308 \nPred R-Squared          0.398       MAE                   230.113 \n------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                                  ANOVA                                   \n-------------------------------------------------------------------------\n                    Sum of                                               \n                   Squares         DF    Mean Square      F         Sig. \n-------------------------------------------------------------------------\nRegression    22859338.083          3    7619779.361    64.791    0.0000 \nResidual      30459774.706        259     117605.308                     \nTotal         53319112.789        262                                    \n-------------------------------------------------------------------------\n\n                                    Parameter Estimates                                     \n-------------------------------------------------------------------------------------------\n      model       Beta    Std. Error    Std. Beta      t        Sig        lower     upper \n-------------------------------------------------------------------------------------------\n(Intercept)    -76.854        56.974                 -1.349    0.179    -189.045    35.337 \n      CRuns      0.620         0.068        0.455     9.117    0.000       0.486     0.753 \n       Hits      2.415         0.581        0.242     4.157    0.000       1.271     3.560 \n      Walks      3.126         1.243        0.151     2.515    0.013       0.678     5.575 \n-------------------------------------------------------------------------------------------\n\n                          Model Summary                            \n------------------------------------------------------------------\nR                       0.655       RMSE                  342.936 \nR-Squared               0.429       Coef. Var              63.990 \nAdj. R-Squared          0.422       MSE                117605.308 \nPred R-Squared          0.398       MAE                   230.113 \n------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                                  ANOVA                                   \n-------------------------------------------------------------------------\n                    Sum of                                               \n                   Squares         DF    Mean Square      F         Sig. \n-------------------------------------------------------------------------\nRegression    22859338.083          3    7619779.361    64.791    0.0000 \nResidual      30459774.706        259     117605.308                     \nTotal         53319112.789        262                                    \n-------------------------------------------------------------------------\n\n                                    Parameter Estimates                                     \n-------------------------------------------------------------------------------------------\n      model       Beta    Std. Error    Std. Beta      t        Sig        lower     upper \n-------------------------------------------------------------------------------------------\n(Intercept)    -76.854        56.974                 -1.349    0.179    -189.045    35.337 \n      CRuns      0.620         0.068        0.455     9.117    0.000       0.486     0.753 \n       Hits      2.415         0.581        0.242     4.157    0.000       1.271     3.560 \n      Walks      3.126         1.243        0.151     2.515    0.013       0.678     5.575 \n-------------------------------------------------------------------------------------------\n\n\n\nStepwise Selection: Step 4 \n\n- CWalks added \n\n                          Model Summary                            \n------------------------------------------------------------------\nR                       0.661       RMSE                  340.961 \nR-Squared               0.437       Coef. Var              63.621 \nAdj. R-Squared          0.429       MSE                116254.294 \nPred R-Squared          0.402       MAE                   229.259 \n------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                                  ANOVA                                   \n-------------------------------------------------------------------------\n                    Sum of                                               \n                   Squares         DF    Mean Square      F         Sig. \n-------------------------------------------------------------------------\nRegression    23325504.960          4    5831376.240    50.161    0.0000 \nResidual      29993607.829        258     116254.294                     \nTotal         53319112.789        262                                    \n-------------------------------------------------------------------------\n\n                                    Parameter Estimates                                     \n-------------------------------------------------------------------------------------------\n      model       Beta    Std. Error    Std. Beta      t        Sig        lower     upper \n-------------------------------------------------------------------------------------------\n(Intercept)    -63.651        57.028                 -1.116    0.265    -175.951    48.649 \n      CRuns      1.022         0.212        0.751     4.818    0.000       0.604     1.440 \n       Hits      1.569         0.716        0.157     2.192    0.029       0.160     2.979 \n      Walks      5.058         1.568        0.244     3.226    0.001       1.971     8.146 \n     CWalks     -0.564         0.282       -0.330    -2.002    0.046      -1.119    -0.009 \n-------------------------------------------------------------------------------------------\n\n                          Model Summary                            \n------------------------------------------------------------------\nR                       0.661       RMSE                  340.961 \nR-Squared               0.437       Coef. Var              63.621 \nAdj. R-Squared          0.429       MSE                116254.294 \nPred R-Squared          0.402       MAE                   229.259 \n------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                                  ANOVA                                   \n-------------------------------------------------------------------------\n                    Sum of                                               \n                   Squares         DF    Mean Square      F         Sig. \n-------------------------------------------------------------------------\nRegression    23325504.960          4    5831376.240    50.161    0.0000 \nResidual      29993607.829        258     116254.294                     \nTotal         53319112.789        262                                    \n-------------------------------------------------------------------------\n\n                                    Parameter Estimates                                     \n-------------------------------------------------------------------------------------------\n      model       Beta    Std. Error    Std. Beta      t        Sig        lower     upper \n-------------------------------------------------------------------------------------------\n(Intercept)    -63.651        57.028                 -1.116    0.265    -175.951    48.649 \n      CRuns      1.022         0.212        0.751     4.818    0.000       0.604     1.440 \n       Hits      1.569         0.716        0.157     2.192    0.029       0.160     2.979 \n      Walks      5.058         1.568        0.244     3.226    0.001       1.971     8.146 \n     CWalks     -0.564         0.282       -0.330    -2.002    0.046      -1.119    -0.009 \n-------------------------------------------------------------------------------------------\n\n\n\nNo more variables to be added/removed.\n\n\nFinal Model Output \n------------------\n\n                          Model Summary                            \n------------------------------------------------------------------\nR                       0.661       RMSE                  340.961 \nR-Squared               0.437       Coef. Var              63.621 \nAdj. R-Squared          0.429       MSE                116254.294 \nPred R-Squared          0.402       MAE                   229.259 \n------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                                  ANOVA                                   \n-------------------------------------------------------------------------\n                    Sum of                                               \n                   Squares         DF    Mean Square      F         Sig. \n-------------------------------------------------------------------------\nRegression    23325504.960          4    5831376.240    50.161    0.0000 \nResidual      29993607.829        258     116254.294                     \nTotal         53319112.789        262                                    \n-------------------------------------------------------------------------\n\n                                    Parameter Estimates                                     \n-------------------------------------------------------------------------------------------\n      model       Beta    Std. Error    Std. Beta      t        Sig        lower     upper \n-------------------------------------------------------------------------------------------\n(Intercept)    -63.651        57.028                 -1.116    0.265    -175.951    48.649 \n      CRuns      1.022         0.212        0.751     4.818    0.000       0.604     1.440 \n       Hits      1.569         0.716        0.157     2.192    0.029       0.160     2.979 \n      Walks      5.058         1.568        0.244     3.226    0.001       1.971     8.146 \n     CWalks     -0.564         0.282       -0.330    -2.002    0.046      -1.119    -0.009 \n-------------------------------------------------------------------------------------------\n\n\n\n                               Stepwise Selection Summary                                \n----------------------------------------------------------------------------------------\n                     Added/                   Adj.                                          \nStep    Variable    Removed     R-Square    R-Square     C(p)         AIC         RMSE      \n----------------------------------------------------------------------------------------\n   1     CRuns      addition       0.317       0.314    54.1400    3866.0101    373.6429    \n   2      Hits      addition       0.415       0.410    11.1560    3827.2236    346.4294    \n   3     Walks      addition       0.429       0.422     6.7650    3822.8795    342.9363    \n   4     CWalks     addition       0.437       0.429     4.7590    3820.8233    340.9608    \n----------------------------------------------------------------------------------------\n\n\n\n\n(예 2.13) 다중공선성\n분산팽창계수\n\nvif_value1 &lt;- vif(lm_model)\nvif_value1\n\n     Hits     Walks     CRuns     HmRun    CWalks \n 2.626281  2.641472 11.132539  1.465358 12.496169 \n\n\n변수간 상관계수\n\nvars1 &lt;- c(\"Hits\", \"Walks\", \"CRuns\", \"HmRun\", \"CWalks\")\ncor(dat_ba[vars1])\n\n            Hits     Walks     CRuns     HmRun    CWalks\nHits   1.0000000 0.6412106 0.2617869 0.5621579 0.1518182\nWalks  0.6412106 1.0000000 0.3384780 0.4810143 0.4245071\nCRuns  0.2617869 0.3384780 1.0000000 0.2623606 0.9278069\nHmRun  0.5621579 0.4810143 0.2623606 1.0000000 0.2331537\nCWalks 0.1518182 0.4245071 0.9278069 0.2331537 1.0000000\n\n\n상관계수가 높은 두 변수 간의 선형관계\n\nplot(dat_ba[, c(\"CRuns\", \"CWalks\")])\nabline(lm(CWalks ~ CRuns, data = dat_ba), col = \"red\", lwd = 2, lty = 1)\n\n\n\n\n상관계수가 높은 두 변수 중 하나의 변수를 제거\n\nlm2_model &lt;- lm(Salary ~ Hits + Walks + CRuns + HmRun, data = dat_ba)\nsummary(lm2_model)\n\n\nCall:\nlm(formula = Salary ~ Hits + Walks + CRuns + HmRun, data = dat_ba)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-902.84 -189.20  -37.78  104.27 2196.70 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -75.27564   57.04814  -1.320 0.188168    \nHits          2.23021    0.62602   3.563 0.000437 ***\nWalks         2.97024    1.25939   2.358 0.019096 *  \nCRuns         0.61299    0.06849   8.950  &lt; 2e-16 ***\nHmRun         2.33890    2.92909   0.799 0.425310    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 343.2 on 258 degrees of freedom\n  (59 observations deleted due to missingness)\nMultiple R-squared:  0.4301,    Adjusted R-squared:  0.4213 \nF-statistic: 48.68 on 4 and 258 DF,  p-value: &lt; 2.2e-16\n\nvif_value2 &lt;- vif(lm2_model)\nvif_value2\n\n    Hits    Walks    CRuns    HmRun \n1.775329 1.664286 1.144697 1.463700"
  },
  {
    "objectID": "chapters/ch02_regression.html#예-2.14-2.16",
    "href": "chapters/ch02_regression.html#예-2.14-2.16",
    "title": "2장 회귀분석",
    "section": "(예 2.14, 2.16)",
    "text": "(예 2.14, 2.16)\n\n패키지 로드\n\nlibrary(ggplot2)\n\n\n\n데이터 로드\n\ndat1 &lt;- read.csv(\"data/ch2_coil.csv\")\nsummary(dat1)\n\n      temp           thick           y        \n Min.   :530.0   Min.   :2.0   Min.   :48.70  \n 1st Qu.:562.5   1st Qu.:2.0   1st Qu.:49.52  \n Median :605.0   Median :6.0   Median :50.80  \n Mean   :616.0   Mean   :4.4   Mean   :50.54  \n 3rd Qu.:675.0   3rd Qu.:6.0   3rd Qu.:51.27  \n Max.   :710.0   Max.   :6.0   Max.   :52.50  \n\n\n\n\n지시변수 변환\n\ndat1$thick &lt;- factor(dat1$thick, levels = c(6, 2))\nsummary(dat1)\n\n      temp       thick       y        \n Min.   :530.0   6:6   Min.   :48.70  \n 1st Qu.:562.5   2:4   1st Qu.:49.52  \n Median :605.0         Median :50.80  \n Mean   :616.0         Mean   :50.54  \n 3rd Qu.:675.0         3rd Qu.:51.27  \n Max.   :710.0         Max.   :52.50  \n\n\n\n\n(예 2.14) 회귀모형 추정\n\nlm_fit &lt;- lm(y ~ temp + thick, data = dat1)\nsummary(lm_fit)\n\n\nCall:\nlm(formula = y ~ temp + thick, data = dat1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.26076 -0.18481 -0.02085  0.16208  0.29881 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 61.107966   0.703491  86.864 7.06e-12 ***\ntemp        -0.017678   0.001149 -15.380 1.18e-06 ***\nthick2       0.804153   0.149570   5.376  0.00103 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.228 on 7 degrees of freedom\nMultiple R-squared:  0.9721,    Adjusted R-squared:  0.9641 \nF-statistic: 121.8 on 2 and 7 DF,  p-value: 3.641e-06\n\n\n\n\n(예 2.16) 교호작용 추정\n\nlm_fit_interaction &lt;- lm(y ~ temp + thick + temp:thick, data = dat1)\nsummary(lm_fit_interaction)\n\n\nCall:\nlm(formula = y ~ temp + thick + temp:thick, data = dat1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.18286 -0.10707 -0.02152  0.08831  0.30949 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 60.136402   0.749951  80.187 2.53e-10 ***\ntemp        -0.016076   0.001230 -13.074 1.23e-05 ***\nthick2       3.278471   1.210446   2.708   0.0352 *  \ntemp:thick2 -0.003987   0.001940  -2.055   0.0857 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1886 on 6 degrees of freedom\nMultiple R-squared:  0.9836,    Adjusted R-squared:  0.9754 \nF-statistic:   120 on 3 and 6 DF,  p-value: 9.577e-06\n\n\n시각화\n\nggplot(dat1, aes(x = temp, y = y, shape = thick, linetype = thick)) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"grey30\") +\n  geom_point(size = 3) +\n  labs(x = \"X(temp)\", y = \"Y\", shape = \"thickness\", linetype = \"thickness\") +\n  theme_classic() +\n  theme(legend.position = c(0.2, 0.2))\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "chapters/ch03_regularized_regression.html#예-3.1---3.2",
    "href": "chapters/ch03_regularized_regression.html#예-3.1---3.2",
    "title": "3장 규제 회귀분석",
    "section": "(예 3.1 - 3.2)",
    "text": "(예 3.1 - 3.2)\n\n패키지 로드\n\nlibrary(penalized)\n\nLoading required package: survival\n\n\nWelcome to penalized. For extended examples, see vignette(\"penalized\").\n\n\n\n\n데이터 로드\n\ndat1 &lt;- read.csv(\"data/ch3_dat1.csv\")\nx &lt;- as.matrix(dat1[, 1:2])\ny &lt;- dat1$y\n\n\n\n독립변수 데이터 표준화\n\nstd_x &lt;- scale(x)\n\n\n\n(예 3.1) Lasso\n교재의 \\(\\lambda\\)값을 penalized() 함수 인자로 사용하기 위해 변환\n\nlambda &lt;- seq(0, 3, by = 1)\nlasso_lambda &lt;- lambda / 2\n\nLasso regression\n\nfor (i in lasso_lambda) {\n  # estimate Lasso regression\n  lasso &lt;- penalized(y, std_x, lambda1 = i, trace = FALSE)\n  # print coefficients\n  print(round(coef(lasso), 4))\n}\n\n(Intercept)          x1          x2 \n     0.0000      2.0201      0.1339 \n(Intercept)          x1          x2 \n     0.0000      1.9736      0.0875 \n(Intercept)          x1          x2 \n     0.0000      1.9272      0.0410 \n(Intercept)          x1 \n     0.0000      1.8764 \n\n\n\n\n(예 3.2) ridge\nL2-penality의 경우 교재의 \\(\\lambda\\)값을 그대로 사용\n\nridge_lambda &lt;- lambda\n\nridge regression\n\nfor (i in ridge_lambda) {\n  # estimate Lasso regression\n  ridge &lt;- penalized(y, std_x, lambda2 = i, trace = FALSE)\n  # print coefficients\n  print(round(coef(ridge), 4))\n}\n\n(Intercept)          x1          x2 \n     0.0000      2.0201      0.1339 \n(Intercept)          x1          x2 \n     0.0000      1.5071      0.4638 \n(Intercept)          x1          x2 \n     0.0000      1.2688      0.5477 \n(Intercept)          x1          x2 \n     0.0000      1.1177      0.5667"
  },
  {
    "objectID": "chapters/ch05_classification.html#예-5.2",
    "href": "chapters/ch05_classification.html#예-5.2",
    "title": "5장 분류분석 개요",
    "section": "(예 5.2)",
    "text": "(예 5.2)\n\n패키지 로드\n\nlibrary(class) # knn\n\n\n\n데이터 로드\n\ndat1 &lt;- read.csv(\"data/ch7_dat1.csv\")\ndat1$class &lt;- factor(dat1$class)\ntrain_x &lt;- dat1[1:7, 2:3]\ntrain_y &lt;- dat1[1:7, 4]\ntest_x &lt;- dat1[8:9, 2:3]\n\n\n\n학습데이터간 유클리디안 거리\n\ndist(train_x)\n\n         1        2        3        4        5        6\n2 4.123106                                             \n3 2.236068 5.830952                                    \n4 3.162278 5.000000 2.236068                           \n5 2.236068 3.162278 4.472136 5.000000                  \n6 3.605551 2.828427 5.830952 6.082763 1.414214         \n7 1.414214 3.605551 2.236068 2.000000 3.000000 4.123106\n\n\n\n\n3-인접객체법 - 학습데이터\n\nknn.cv(train_x, train_y, k = 3)\n\n[1] 1 1 1 1 1 1 2\nLevels: 1 2\n\n\n\n\n3-인접객체법 - 새로운 객체\n\nknn(train_x, test_x, train_y, k = 3)\n\n[1] 2 1\nLevels: 1 2"
  },
  {
    "objectID": "chapters/ch05_classification.html#예-5.3---5.4",
    "href": "chapters/ch05_classification.html#예-5.3---5.4",
    "title": "5장 분류분석 개요",
    "section": "(예 5.3 - 5.4)",
    "text": "(예 5.3 - 5.4)\n\n패키지 로드\n\nlibrary(e1071) # naive bayes\nlibrary(yardstick) # measure performance\n\n\n\n데이터 로드\n\ndat3 &lt;- read.csv(\"data/ch5_dat3.csv\")\ndat3$gender &lt;- factor(dat3$gender)\ndat3$age_gr &lt;- factor(dat3$age_gr)\ndat3$class &lt;- factor(dat3$class)\nsummary(dat3)\n\n       ID    gender age_gr class\n Min.   :1   F:5    1:1    1:5  \n 1st Qu.:3   M:4    2:4    2:4  \n Median :5          3:2         \n Mean   :5          4:2         \n 3rd Qu.:7                      \n Max.   :9                      \n\n\n\n\n(예 5.3) 나이브베이즈 분류분석\n모형 추정\n\nnb_fit &lt;- naiveBayes(class ~ gender + age_gr, data = dat3)\nprint(nb_fit)\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\n        1         2 \n0.5555556 0.4444444 \n\nConditional probabilities:\n   gender\nY      F    M\n  1 0.40 0.60\n  2 0.75 0.25\n\n   age_gr\nY      1    2    3    4\n  1 0.20 0.40 0.20 0.20\n  2 0.00 0.50 0.25 0.25\n\n\n사후확률\n\nnb_posterior &lt;- predict(nb_fit, dat3, type = \"raw\")\nround(nb_posterior, 3)\n\n          1     2\n [1,] 0.706 0.294\n [2,] 0.706 0.294\n [3,] 0.706 0.294\n [4,] 0.706 0.294\n [5,] 0.993 0.007\n [6,] 0.348 0.652\n [7,] 0.348 0.652\n [8,] 0.348 0.652\n [9,] 0.348 0.652\n\n\n추정범주\n\nnb_class &lt;- predict(nb_fit, dat3, type = \"class\")\nnb_class\n\n[1] 1 1 1 1 1 2 2 2 2\nLevels: 1 2\n\n\n추정결과 정리\n\nresults &lt;- cbind(dat3, pred_class = nb_class, posterior = round(nb_posterior, 3))\nresults\n\n  ID gender age_gr class pred_class posterior.1 posterior.2\n1  1      M      2     1          1       0.706       0.294\n2  2      M      2     2          1       0.706       0.294\n3  3      M      3     1          1       0.706       0.294\n4  4      M      4     1          1       0.706       0.294\n5  5      F      1     1          1       0.993       0.007\n6  6      F      2     2          2       0.348       0.652\n7  7      F      2     1          2       0.348       0.652\n8  8      F      3     2          2       0.348       0.652\n9  9      F      4     2          2       0.348       0.652\n\n\n\n\n(예 5.4) 성능 평가\n정오분류표\n\nconf_mat(results, truth = \"class\", estimate = \"pred_class\")\n\n          Truth\nPrediction 1 2\n         1 4 1\n         2 1 3\n\n\n민감도, 특이도 및 F1 척도\n\nmulti_metric &lt;- metric_set(accuracy, sens, spec, f_meas)\nmulti_metric(results, truth = \"class\", estimate = \"pred_class\")\n\n# A tibble: 4 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.778\n2 sens     binary         0.8  \n3 spec     binary         0.75 \n4 f_meas   binary         0.8"
  },
  {
    "objectID": "chapters/ch05_classification.html#예-5.4",
    "href": "chapters/ch05_classification.html#예-5.4",
    "title": "5장 분류분석 개요",
    "section": "(예 5.4)",
    "text": "(예 5.4)\n\n패키지 로드\n\nlibrary(yardstick)\n\n\n\n데이터 로드\n\ndat &lt;- read.csv(\"data/ch5_cm.csv\")\ndat$pred_y &lt;- factor(dat$pred_y, levels = c(1, 0))\ndat$true_y &lt;- factor(dat$true_y, levels = c(1, 0))\nsummary(dat)\n\n pred_y true_y\n 1:25   1:20  \n 0:75   0:80  \n\ntable(dat)\n\n      true_y\npred_y  1  0\n     1 15 10\n     0  5 70\n\n\n\n\n정오분류표\n\ncm &lt;- conf_mat(dat, truth = \"true_y\", estimate = \"pred_y\")\ncm\n\n          Truth\nPrediction  1  0\n         1 15 10\n         0  5 70\n\n\n\n\n정확도, 민감도, 특이도, F1 척도\n\nmulti_metric &lt;- metric_set(accuracy, sens, spec, f_meas)\nmulti_metric(dat, truth = \"true_y\", estimate = \"pred_y\")\n\n# A tibble: 4 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.85 \n2 sens     binary         0.75 \n3 spec     binary         0.875\n4 f_meas   binary         0.667\n\n\n\n\nOPTIONAL: 보다 다양한 척도들\n\nsummary(cm)\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.85 \n 2 kap                  binary         0.571\n 3 sens                 binary         0.75 \n 4 spec                 binary         0.875\n 5 ppv                  binary         0.6  \n 6 npv                  binary         0.933\n 7 mcc                  binary         0.577\n 8 j_index              binary         0.625\n 9 bal_accuracy         binary         0.812\n10 detection_prevalence binary         0.25 \n11 precision            binary         0.6  \n12 recall               binary         0.75 \n13 f_meas               binary         0.667"
  },
  {
    "objectID": "chapters/ch05_classification.html#예-5.5",
    "href": "chapters/ch05_classification.html#예-5.5",
    "title": "5장 분류분석 개요",
    "section": "(예 5.5)",
    "text": "(예 5.5)\n\n패키지 로드\n\nlibrary(yardstick)\nlibrary(ggplot2)\n\n\n\n데이터 로드\n\ndat5 &lt;- read.csv(\"data/ch5_dat5.csv\")\ndat5$class &lt;- factor(dat5$class, levels = c(1, 0))\n\n\n\n범주 분류기준: x &gt;= 40\n범주 추정\n\ndat5$pred40 &lt;- factor(ifelse(dat5$x &gt;= 40, 1, 0), levels = c(1, 0))\n\n정오분류표\n\ncm40 &lt;- conf_mat(dat5, truth = \"class\", estimate = \"pred40\")\ncm40\n\n          Truth\nPrediction 1 0\n         1 5 2\n         0 1 2\n\n\n민감도 및 특이도\n\nmulti_metric &lt;- metric_set(sens, spec)\nmetric40 &lt;- multi_metric(dat5, truth = \"class\", estimate = \"pred40\")\nmetric40\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 sens    binary         0.833\n2 spec    binary         0.5  \n\n\n\n\n범주 분류기준: x &gt;= 50\n범주 추정\n\ndat5$pred50 &lt;- factor(ifelse(dat5$x &gt;= 50, 1, 0), levels = c(1, 0))\n\n정오분류표\n\ncm50 &lt;- conf_mat(dat5, truth = \"class\", estimate = \"pred50\")\ncm50\n\n          Truth\nPrediction 1 0\n         1 4 1\n         0 2 3\n\n\n민감도 및 특이도\n\nmetric50 &lt;- multi_metric(dat5, truth = \"class\", estimate = \"pred50\")\nmetric50\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 sens    binary         0.667\n2 spec    binary         0.75"
  },
  {
    "objectID": "chapters/ch05_classification.html#예-5.6",
    "href": "chapters/ch05_classification.html#예-5.6",
    "title": "5장 분류분석 개요",
    "section": "(예 5.6)",
    "text": "(예 5.6)\n\n패키지 로드\n\nlibrary(yardstick)\nlibrary(ggplot2)\n\n\n\n데이터 로드\n\ndat &lt;- read.csv(\"data/ch5_roc.csv\")\ndat$class &lt;- factor(dat$class)\ndat$pred &lt;- factor(dat$pred)\n\n\n\nROC 곡선 데이터\n\nroc &lt;- roc_curve(dat, truth = \"class\", posterior1)\nroc\n\n# A tibble: 11 × 3\n   .threshold specificity sensitivity\n        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 -Inf               0          1   \n 2    0.00596         0          1   \n 3    0.0264          0.2        1   \n 4    0.0995          0.4        1   \n 5    0.103           0.6        1   \n 6    0.356           0.8        1   \n 7    0.728           0.8        0.75\n 8    0.921           1          0.75\n 9    0.980           1          0.5 \n10    0.981           1          0.25\n11  Inf               1          0   \n\n\n\n\nAUC\n\nauc &lt;- roc_auc(dat, truth = \"class\", posterior1)\nauc\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary          0.95\n\n\n\n\nROC 곡선 시각화\n\nautoplot(roc) +\n  labs(title = paste0(\"ROC Curve: AUC = \", auc[[\".estimate\"]]))"
  },
  {
    "objectID": "chapters/ch05_classification.html#예-5.7",
    "href": "chapters/ch05_classification.html#예-5.7",
    "title": "5장 분류분석 개요",
    "section": "(예 5.7)",
    "text": "(예 5.7)\n\n패키지 로드\n\nlibrary(yardstick)\nlibrary(ggplot2)\n\n\n\n데이터 로드\n1000개 객체의 실제범주와 범주 1에 속할 사후확률 추정 결과\n\ndat &lt;- read.csv(\"data/ch5_lift_2.csv\")\ndat$class &lt;- factor(dat$class)\nhead(dat)\n\n  class posterior1\n1     1     0.9995\n2     1     0.9985\n3     1     0.9975\n4     1     0.9965\n5     1     0.9955\n6     1     0.9945\n\ntail(dat)\n\n     class posterior1\n995      2     0.0055\n996      2     0.0045\n997      2     0.0035\n998      2     0.0025\n999      2     0.0015\n1000     2     0.0005\n\nsummary(dat)\n\n class     posterior1    \n 1:437   Min.   :0.0005  \n 2:563   1st Qu.:0.2502  \n         Median :0.5000  \n         Mean   :0.5000  \n         3rd Qu.:0.7498  \n         Max.   :0.9995  \n\n\n사후확률 내림차순으로 객체를 100개씩 묶은 10개 집단 각각의 범주 1 빈도\n\ngrouped &lt;- read.csv(\"data/ch5_lift_1.csv\")\ngrouped$group &lt;- factor(grouped$group)\ngrouped\n\n   group   n n1\n1      1 100 92\n2      2 100 78\n3      3 100 64\n4      4 100 57\n5      5 100 43\n6      6 100 35\n7      7 100 29\n8      8 100 22\n9      9 100  7\n10    10 100 10\n\n\n\n\n범주 1\n범주 1에 속한 전체 객체 수\n\ntotal_n1 &lt;- sum(grouped$n1)\nprint(total_n1)\n\n[1] 437\n\n\n전체 데이터에서 범주 1의 비율\n\nprop_n1 &lt;- sum(grouped$n1) / sum(grouped$n)\nprint(prop_n1)\n\n[1] 0.437\n\n\n\n\n이익도표 통계량 산출\n\ngrouped$response_pct &lt;- grouped$n1 / grouped$n * 100\ngrouped$captured_response_pct &lt;- grouped$n1 / total_n1 * 100\ngrouped$gain &lt;- cumsum(grouped$captured_response_pct)\ngrouped$lift &lt;- (cumsum(grouped$n1) / cumsum(grouped$n)) / prop_n1\ngrouped\n\n   group   n n1 response_pct captured_response_pct      gain     lift\n1      1 100 92           92             21.052632  21.05263 2.105263\n2      2 100 78           78             17.848970  38.90160 1.945080\n3      3 100 64           64             14.645309  53.54691 1.784897\n4      4 100 57           57             13.043478  66.59039 1.664760\n5      5 100 43           43              9.839817  76.43021 1.528604\n6      6 100 35           35              8.009153  84.43936 1.407323\n7      7 100 29           29              6.636156  91.07551 1.301079\n8      8 100 22           22              5.034325  96.10984 1.201373\n9      9 100  7            7              1.601831  97.71167 1.085685\n10    10 100 10           10              2.288330 100.00000 1.000000\n\n\n\n\nGain chart\n\nggplot(grouped, aes(x = group, y = gain)) +\n  geom_line(group = 1) +\n  geom_point() +\n  labs(y = \"% Gain\", title = \"Gain Chart for Class 1\") +\n  theme_classic()\n\n\n\n\n\n\nLift chart\n\nggplot(grouped, aes(x = group, y = lift)) +\n  geom_hline(yintercept = 1, color = \"grey30\", linetype = \"dashed\") +\n  geom_line(group = 1) +\n  geom_point() +\n  labs(y = \"Lift\", title = \"Lift Chart for Class 1\") +\n  scale_y_continuous(breaks = seq(1, 2.2, by = 0.2)) +\n  theme_classic()\n\n\n\n\n\n\nOPTIONAL: 집단화하기 전 원 데이터(1000개 객체)를 이용한 이익도표\nGain chart\n\ngain &lt;- gain_curve(dat, truth = class, posterior1)\nautoplot(gain)\n\n\n\n\n\nlift &lt;- lift_curve(dat, truth = class, posterior1)\nautoplot(lift)"
  },
  {
    "objectID": "chapters/ch06_logistic_regression.html#예-6.1-6.3",
    "href": "chapters/ch06_logistic_regression.html#예-6.1-6.3",
    "title": "6장 로지스틱 회귀분석",
    "section": "(예 6.1, 6.3)",
    "text": "(예 6.1, 6.3)\n\n패키지 로드\n\nlibrary(yardstick) # confusion matrix\n\n\n\n데이터 로드\n\ndat1 &lt;- read.csv(\"data/ch6_dat1.csv\")\ndat1$Class &lt;- factor(dat1$Class, levels = c(\"Average\", \"Excellent\"))\n\n\n\n(예 6.1)\n로짓모형 추정\n\nbin.fit &lt;- glm(\n  Class ~ Break + Sleep + Circle,\n  data = dat1,\n  family = binomial(\"logit\")\n)\n\nsummary(bin.fit)\n\n\nCall:\nglm(formula = Class ~ Break + Sleep + Circle, family = binomial(\"logit\"), \n    data = dat1)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  -30.511     18.018  -1.693   0.0904 .\nBreak          2.031      1.984   1.024   0.3058  \nSleep          3.471      2.075   1.673   0.0944 .\nCircle         2.414      1.396   1.729   0.0838 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 20.190  on 14  degrees of freedom\nResidual deviance: 10.418  on 11  degrees of freedom\nAIC: 18.418\n\nNumber of Fisher Scoring iterations: 6\n\n\n승산비 (odds ratio)\n\nexp(coef(bin.fit))\n\n (Intercept)        Break        Sleep       Circle \n5.614515e-14 7.623824e+00 3.215832e+01 1.118291e+01 \n\n\nlogit 추정\n\nlogit &lt;- predict(bin.fit, newdata = dat1, type = \"link\")\nlogit\n\n         1          2          3          4          5          6          7 \n 2.0833072 -1.7704728  0.7252047  2.0020167  4.1145853  1.0270229 -6.2161377 \n         8          9         10         11         12         13         14 \n-5.2411440 -1.3873640 -0.3310796 -8.3287064 -0.7141884 -2.4436483  0.6439141 \n        15 \n-7.2724220 \n\n\n확률 추정\n\npred.prob &lt;- predict(bin.fit, newdata = dat1, type = \"response\")\npred.prob\n\n           1            2            3            4            5            6 \n0.8892701130 0.1454835466 0.6737520946 0.8810086527 0.9839297588 0.7363383193 \n           7            8            9           10           11           12 \n0.0019929633 0.0052663161 0.1998289192 0.4179779549 0.0002414259 0.3286740111 \n          13           14           15 \n0.0799042803 0.6556377156 0.0006939461 \n\n\n범주 추정\n\npred.class &lt;- factor(\n  ifelse(pred.prob &gt; .5, \"Excellent\", \"Average\"), \n  levels = c(\"Average\", \"Excellent\")\n)\npred.class\n\n        1         2         3         4         5         6         7         8 \nExcellent   Average Excellent Excellent Excellent Excellent   Average   Average \n        9        10        11        12        13        14        15 \n  Average   Average   Average   Average   Average Excellent   Average \nLevels: Average Excellent\n\n\n결과 요약표\n\nresults &lt;- cbind(dat1, logit = logit, P = pred.prob, pred_class = pred.class)\nresults\n\n   Break Sleep Circle     Class      logit            P pred_class\n1      0     8      2 Excellent  2.0833072 0.8892701130  Excellent\n2      1     7      1 Excellent -1.7704728 0.1454835466    Average\n3      0     9      0 Excellent  0.7252047 0.6737520946  Excellent\n4      1     6      4 Excellent  2.0020167 0.8810086527  Excellent\n5      1     8      2 Excellent  4.1145853 0.9839297588  Excellent\n6      0     7      3 Excellent  1.0270229 0.7363383193  Excellent\n7      0     7      0   Average -6.2161377 0.0019929633    Average\n8      1     6      1   Average -5.2411440 0.0052663161    Average\n9      0     7      2   Average -1.3873640 0.1998289192    Average\n10     0     8      1   Average -0.3310796 0.4179779549    Average\n11     0     5      2   Average -8.3287064 0.0002414259    Average\n12     1     8      0   Average -0.7141884 0.3286740111    Average\n13     0     6      3   Average -2.4436483 0.0799042803    Average\n14     1     7      2   Average  0.6439141 0.6556377156  Excellent\n15     0     6      1   Average -7.2724220 0.0006939461    Average\n\n\n정오분류표\n\nconf_mat(results, truth = \"Class\", estimate = \"pred_class\")\n\n           Truth\nPrediction  Average Excellent\n  Average         8         1\n  Excellent       1         5\n\n\n\n\n(예 6.3)\n곰핏모형 추정\n\ngompit.fit &lt;- glm(\n  Class ~ Break + Sleep + Circle, \n  data = dat1, \n  family = binomial(\"cloglog\")\n)\n\nsummary(gompit.fit)\n\n\nCall:\nglm(formula = Class ~ Break + Sleep + Circle, family = binomial(\"cloglog\"), \n    data = dat1)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  -26.350     14.979  -1.759   0.0786 .\nBreak          1.350      1.503   0.898   0.3692  \nSleep          2.967      1.711   1.734   0.0828 .\nCircle         2.040      1.071   1.905   0.0567 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 20.190  on 14  degrees of freedom\nResidual deviance:  9.676  on 11  degrees of freedom\nAIC: 17.676\n\nNumber of Fisher Scoring iterations: 10\n\n\n노밋모형 추정\n\nnormit.fit &lt;- glm(\n  Class ~ Break + Sleep + Circle, \n  data = dat1, \n  family = binomial(\"probit\")\n)\n\nsummary(normit.fit)\n\n\nCall:\nglm(formula = Class ~ Break + Sleep + Circle, family = binomial(\"probit\"), \n    data = dat1)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept) -17.8505     9.9127  -1.801   0.0717 .\nBreak         1.2607     1.1288   1.117   0.2641  \nSleep         2.0313     1.1424   1.778   0.0754 .\nCircle        1.4144     0.7798   1.814   0.0697 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 20.190  on 14  degrees of freedom\nResidual deviance: 10.381  on 11  degrees of freedom\nAIC: 18.381\n\nNumber of Fisher Scoring iterations: 8"
  },
  {
    "objectID": "chapters/ch06_logistic_regression.html#예-6.4",
    "href": "chapters/ch06_logistic_regression.html#예-6.4",
    "title": "6장 로지스틱 회귀분석",
    "section": "(예 6.4)",
    "text": "(예 6.4)\n\n패키지 로드\n\nlibrary(nnet) # nominal logistic regression\nlibrary(yardstick) # confusion matrix\n\n\n\n데이터 로드\n\ndat2 &lt;- read.csv(\"data/ch6_dat2.csv\")\ndat2$Y &lt;- factor(dat2$Y, levels = c(1, 2, 3))\n\n\n\n모형 추정\n\nnom.fit &lt;- multinom(\n  Y ~ X1 + X2,\n  data = dat2\n)\n\n# weights:  12 (6 variable)\ninitial  value 19.775021 \niter  10 value 17.466403\niter  20 value 15.284871\niter  30 value 14.370416\niter  40 value 14.229073\niter  50 value 14.169298\niter  60 value 14.124675\niter  70 value 14.113683\niter  80 value 14.109933\niter  90 value 14.107827\nfinal  value 14.106976 \nconverged\n\nsummary(nom.fit)\n\nCall:\nmultinom(formula = Y ~ X1 + X2, data = dat2)\n\nCoefficients:\n  (Intercept)         X1        X2\n2   -74.29739  0.8048018  12.98658\n3    85.80346 -1.4767302 -14.26795\n\nStd. Errors:\n  (Intercept)        X1        X2\n2    52.49867 0.7464051  9.467255\n3    69.11561 0.8853856 13.019163\n\nResidual Deviance: 28.21395 \nAIC: 40.21395 \n\n\n\n\n확률 추정\n\npred.prob &lt;- predict(nom.fit, newdata = dat2, type = \"probs\")\npred.prob\n\n           1          2           3\n1  0.3710259 0.07521250 0.553761557\n2  0.4962993 0.14091316 0.362787538\n3  0.6000149 0.33506283 0.064922267\n4  0.1847598 0.81270788 0.002532356\n5  0.3926307 0.59109035 0.016278920\n6  0.5215422 0.45340905 0.025048770\n7  0.3391730 0.63640377 0.024423244\n8  0.2067844 0.01156141 0.781654190\n9  0.4143635 0.52411629 0.061520246\n10 0.5638953 0.13097391 0.305130814\n11 0.4915098 0.08398930 0.424500880\n12 0.1948346 0.02303418 0.782131197\n13 0.4180557 0.09174773 0.490196581\n14 0.1782425 0.01299234 0.808765136\n15 0.3853146 0.57648020 0.038205222\n16 0.5840208 0.37054351 0.045435719\n17 0.4504473 0.10280271 0.446749976\n18 0.2048577 0.02380630 0.771336031\n\n\n\n\n범주 추정\n\npred.class &lt;- predict(nom.fit, newdata = dat2, type = \"class\")\npred.class\n\n [1] 3 1 1 2 2 1 2 3 2 1 1 3 3 3 2 1 1 3\nLevels: 1 2 3\n\n\n\n\n결과 요약표\n\nresults &lt;- cbind(dat2, P = pred.prob, pred_class = pred.class)\nresults\n\n      X1   X2 Y       P.1        P.2         P.3 pred_class\n1   9.33 5.02 1 0.3710259 0.07521250 0.553761557          3\n2   9.91 5.01 1 0.4962993 0.14091316 0.362787538          1\n3  11.88 4.94 1 0.6000149 0.33506283 0.064922267          1\n4  11.54 5.12 1 0.1847598 0.81270788 0.002532356          2\n5  11.66 5.03 1 0.3926307 0.59109035 0.016278920          2\n6  12.43 4.94 2 0.5215422 0.45340905 0.025048770          1\n7  10.32 5.13 2 0.3391730 0.63640377 0.024423244          2\n8  10.15 4.87 1 0.2067844 0.01156141 0.781654190          3\n9   9.83 5.13 2 0.4143635 0.52411629 0.061520246          2\n10 10.79 4.94 3 0.5638953 0.13097391 0.305130814          1\n11 10.57 4.93 3 0.4915098 0.08398930 0.424500880          1\n12  8.66 5.02 3 0.1948346 0.02303418 0.782131197          3\n13  9.59 5.01 3 0.4180557 0.09174773 0.490196581          3\n14  9.35 4.94 3 0.1782425 0.01299234 0.808765136          3\n15 10.20 5.12 2 0.3853146 0.57648020 0.038205222          2\n16 12.20 4.93 2 0.5840208 0.37054351 0.045435719          1\n17  9.80 5.00 1 0.4504473 0.10280271 0.446749976          1\n18  8.80 5.01 3 0.2048577 0.02380630 0.771336031          3\n\n\n\n\n정오분류표\n\nconf_mat(results, truth = \"Y\", estimate = \"pred_class\")\n\n          Truth\nPrediction 1 2 3\n         1 3 2 2\n         2 2 3 0\n         3 2 0 4"
  },
  {
    "objectID": "chapters/ch06_logistic_regression.html#예-6.5---6.6",
    "href": "chapters/ch06_logistic_regression.html#예-6.5---6.6",
    "title": "6장 로지스틱 회귀분석",
    "section": "(예 6.5 - 6.6)",
    "text": "(예 6.5 - 6.6)\n\n패키지 로드\n\nlibrary(VGAM) # ordinal logistic regression\n\nLoading required package: stats4\n\n\nLoading required package: splines\n\nlibrary(yardstick) # confusion matrix\n\n\n\n데이터 로드\n\ndat3 &lt;- read.csv(\"data/ch6_dat3.csv\")\ndat3$Y &lt;- factor(dat3$Y, levels = c(1, 2, 3), ordered = TRUE)\n\n\n\n(예 16.5)\n누적 로짓모형 추정\n\ncum.fit &lt;- vglm(\n  Y ~ N + L,\n  data = dat3,\n  family = cumulative(parallel = TRUE, reverse = FALSE)\n)\n\nsummary(cum.fit, HDEtest = FALSE)\n\n\nCall:\nvglm(formula = Y ~ N + L, family = cumulative(parallel = TRUE, \n    reverse = FALSE), data = dat3)\n\nCoefficients: \n              Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept):1 -13.0353     6.7831  -1.922   0.0546 .\n(Intercept):2 -11.3991     6.4017  -1.781   0.0750 .\nN               0.2236     0.1467   1.525   0.1273  \nL               0.2999     0.1383   2.169   0.0301 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNames of linear predictors: logitlink(P[Y&lt;=1]), logitlink(P[Y&lt;=2])\n\nResidual deviance: 12.8825 on 20 degrees of freedom\n\nLog-likelihood: -6.4413 on 20 degrees of freedom\n\nNumber of Fisher scoring iterations: 9 \n\n\nExponentiated coefficients:\n       N        L \n1.250617 1.349690 \n\n\n확률 추정\n\npred.prob &lt;- predict(cum.fit, dat3, type = \"response\")\npred.prob\n\n             1           2            3\n1  0.002612032 0.010659596 0.9867283719\n2  0.011593638 0.045225094 0.9431812685\n3  0.190480692 0.356717172 0.4528021357\n4  0.825181904 0.135202201 0.0396158953\n5  0.012375914 0.048089960 0.9395341263\n6  0.053142270 0.170610178 0.7762475514\n7  0.529608187 0.322951084 0.1474407283\n8  0.957601516 0.033851255 0.0085472293\n9  0.104970550 0.270942513 0.6240869366\n10 0.344387383 0.385182621 0.2704299967\n11 0.913326457 0.068531057 0.0181424862\n12 0.995291622 0.003788121 0.0009202566\n\n\n범주 추정\n\npred.class &lt;- factor(\n  apply(pred.prob, 1, which.max),\n  levels = c(1, 2, 3),\n  ordered = TRUE\n)\npred.class\n\n 1  2  3  4  5  6  7  8  9 10 11 12 \n 3  3  3  1  3  3  1  1  3  2  1  1 \nLevels: 1 &lt; 2 &lt; 3\n\n\n결과 요약표\n\nresults &lt;- cbind(dat3, P = pred.prob, pred_class = pred.class)\nresults\n\n    N  L Y         P.1         P.2          P.3 pred_class\n1  25  5 3 0.002612032 0.010659596 0.9867283719          3\n2  25 10 3 0.011593638 0.045225094 0.9431812685          3\n3  25 20 2 0.190480692 0.356717172 0.4528021357          3\n4  25 30 1 0.825181904 0.135202201 0.0396158953          1\n5  32  5 3 0.012375914 0.048089960 0.9395341263          3\n6  32 10 3 0.053142270 0.170610178 0.7762475514          3\n7  32 20 2 0.529608187 0.322951084 0.1474407283          1\n8  32 30 1 0.957601516 0.033851255 0.0085472293          1\n9  42  5 1 0.104970550 0.270942513 0.6240869366          3\n10 42 10 3 0.344387383 0.385182621 0.2704299967          2\n11 42 20 1 0.913326457 0.068531057 0.0181424862          1\n12 42 30 1 0.995291622 0.003788121 0.0009202566          1\n\n\n정오분류표\n\nconf_mat(results, truth = \"Y\", estimate = \"pred_class\")\n\n          Truth\nPrediction 1 2 3\n         1 4 1 0\n         2 0 0 1\n         3 1 1 4\n\n\n\n\n(예 6-6) - 인근범주 로짓모형\n인근범주 로짓모형 추정\n\nadj.fit &lt;- vglm(\n  Y ~ N + L,\n  data = dat3,\n  family = acat(parallel = FALSE, reverse = FALSE)\n)\n\nsummary(adj.fit, HDEtest = FALSE)\n\n\nCall:\nvglm(formula = Y ~ N + L, family = acat(parallel = FALSE, reverse = FALSE), \n    data = dat3)\n\nCoefficients: \n              Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept):1 12.94227   10.91281   1.186    0.236\n(Intercept):2  6.10598    8.12750   0.751    0.452\nN:1           -0.31432    0.24616  -1.277    0.202\nN:2           -0.04643    0.20835  -0.223    0.824\nL:1           -0.17500    0.18967  -0.923    0.356\nL:2           -0.29578    0.21017  -1.407    0.159\n\nNames of linear predictors: loglink(P[Y=2]/P[Y=1]), loglink(P[Y=3]/P[Y=2])\n\nResidual deviance: 11.6777 on 18 degrees of freedom\n\nLog-likelihood: -5.8388 on 18 degrees of freedom\n\nNumber of Fisher scoring iterations: 6 \n\n\n확률 추정\n\npred.prob &lt;- predict(adj.fit, dat3, type = \"response\")\npred.prob\n\n             1           2            3\n1  0.000449727 0.030272428 9.692778e-01\n2  0.004277162 0.120015544 8.757073e-01\n3  0.129475890 0.631302720 2.392214e-01\n4  0.536500470 0.454554350 8.945179e-03\n5  0.005525946 0.041206436 9.532676e-01\n6  0.048790213 0.151661243 7.995485e-01\n7  0.592407688 0.319984941 8.760737e-02\n8  0.913080653 0.085700815 1.218532e-03\n9  0.166660093 0.053620769 7.797191e-01\n10 0.633490909 0.084962109 2.815470e-01\n11 0.973410484 0.022685475 3.904041e-03\n12 0.995930771 0.004033183 3.604585e-05\n\n\n범주 추정\n\npred.class &lt;- factor(\n  apply(pred.prob, 1, which.max),\n  levels = c(1, 2, 3),\n  ordered = TRUE\n)\npred.class\n\n 1  2  3  4  5  6  7  8  9 10 11 12 \n 3  3  2  1  3  3  1  1  3  1  1  1 \nLevels: 1 &lt; 2 &lt; 3\n\n\n결과 요약표\n\nresults &lt;- cbind(dat3, P = pred.prob, pred_class = pred.class)\nresults\n\n    N  L Y         P.1         P.2          P.3 pred_class\n1  25  5 3 0.000449727 0.030272428 9.692778e-01          3\n2  25 10 3 0.004277162 0.120015544 8.757073e-01          3\n3  25 20 2 0.129475890 0.631302720 2.392214e-01          2\n4  25 30 1 0.536500470 0.454554350 8.945179e-03          1\n5  32  5 3 0.005525946 0.041206436 9.532676e-01          3\n6  32 10 3 0.048790213 0.151661243 7.995485e-01          3\n7  32 20 2 0.592407688 0.319984941 8.760737e-02          1\n8  32 30 1 0.913080653 0.085700815 1.218532e-03          1\n9  42  5 1 0.166660093 0.053620769 7.797191e-01          3\n10 42 10 3 0.633490909 0.084962109 2.815470e-01          1\n11 42 20 1 0.973410484 0.022685475 3.904041e-03          1\n12 42 30 1 0.995930771 0.004033183 3.604585e-05          1\n\n\n정오분류표\n\nconf_mat(results, truth = \"Y\", estimate = \"pred_class\")\n\n          Truth\nPrediction 1 2 3\n         1 4 1 1\n         2 0 1 0\n         3 1 0 4\n\n\n\n\n(예 6-6) 명목 로지스틱 회귀모형\n\nnom.fit &lt;- vglm(\n  Y ~ N + L,\n  data = dat3,\n  family = multinomial(refLevel = 1)\n)\n\nWarning in eval(slot(family, \"initialize\")): response should be nominal, not\nordinal\n\nsummary(nom.fit, HDEtest = FALSE)\n\n\nCall:\nvglm(formula = Y ~ N + L, family = multinomial(refLevel = 1), \n    data = dat3)\n\nCoefficients: \n              Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept):1  12.9423    10.9128   1.186   0.2356  \n(Intercept):2  19.0482    12.2270   1.558   0.1193  \nN:1            -0.3143     0.2462  -1.277   0.2016  \nN:2            -0.3607     0.2631  -1.371   0.1704  \nL:1            -0.1750     0.1897  -0.923   0.3562  \nL:2            -0.4708     0.2642  -1.782   0.0748 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNames of linear predictors: log(mu[,2]/mu[,1]), log(mu[,3]/mu[,1])\n\nResidual deviance: 11.6777 on 18 degrees of freedom\n\nLog-likelihood: -5.8388 on 18 degrees of freedom\n\nNumber of Fisher scoring iterations: 6 \n\n\nReference group is level  1  of the response"
  },
  {
    "objectID": "chapters/ch07_da.html#예-7.4",
    "href": "chapters/ch07_da.html#예-7.4",
    "title": "7장 판별분석",
    "section": "(예 7.4)",
    "text": "(예 7.4)\n\n패키지 로드\n\nlibrary(MASS)\nlibrary(yardstick)\n\n\n\n데이터 로드\n\ndat1 &lt;- read.csv(\"data/ch7_dat1.csv\")\ndat1 &lt;- dat1[, -1]\ndat1$class &lt;- as.factor(dat1$class)\n\n\n\n선형 판별분석 (사전확률 = 0.5)\n\nlda_fit &lt;- lda(class ~ ., data = dat1, prior = c(1 / 2, 1 / 2))\nlda_fit\n\nCall:\nlda(class ~ ., data = dat1, prior = c(1/2, 1/2))\n\nPrior probabilities of groups:\n  1   2 \n0.5 0.5 \n\nGroup means:\n   X1  X2\n1 4.0 6.0\n2 6.6 5.4\n\nCoefficients of linear discriminants:\n          LD1\nX1  0.6850490\nX2 -0.7003859\n\n\n\n\n각 객체에 대한 추정\n추정 실행\n\npred &lt;- predict(lda_fit, dat1)\n\n추정된 사후확률\n\npred$posterior\n\n            1          2\n1 0.921053841 0.07894616\n2 0.099534138 0.90046586\n3 0.727599210 0.27240079\n4 0.026360768 0.97363923\n5 0.980754199 0.01924580\n6 0.980106474 0.01989353\n7 0.355926909 0.64407309\n8 0.005957065 0.99404294\n9 0.102601311 0.89739869\n\n\n추정된 범주\n\npred$class\n\n[1] 1 2 1 2 1 1 2 2 2\nLevels: 1 2\n\n\n추정결과 요약\n\nresults &lt;- cbind(dat1, posterior = pred$posterior, pred_class = pred$class)\nresults\n\n  X1 X2 class posterior.1 posterior.2 pred_class\n1  5  7     1 0.921053841  0.07894616          1\n2  4  3     2 0.099534138  0.90046586          2\n3  7  8     2 0.727599210  0.27240079          1\n4  8  6     2 0.026360768  0.97363923          2\n5  3  6     1 0.980754199  0.01924580          1\n6  2  5     1 0.980106474  0.01989353          1\n7  6  6     1 0.355926909  0.64407309          2\n8  9  6     2 0.005957065  0.99404294          2\n9  5  4     2 0.102601311  0.89739869          2\n\n\n\n\n추정 성능\n\nmetrics_multi &lt;- metric_set(accuracy, sens, spec, f_meas, roc_auc)\nmetrics_multi(results, truth = \"class\", estimate = \"pred_class\", posterior.1)\n\n# A tibble: 5 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.778\n2 sens     binary         0.75 \n3 spec     binary         0.8  \n4 f_meas   binary         0.75 \n5 roc_auc  binary         0.95"
  },
  {
    "objectID": "chapters/ch07_da.html#예-7.6",
    "href": "chapters/ch07_da.html#예-7.6",
    "title": "7장 판별분석",
    "section": "(예 7.6)",
    "text": "(예 7.6)\n\n패키지 로드\n\nlibrary(MASS)\nlibrary(yardstick)\n\n\n\n데이터 로드\n\ndat1 &lt;- read.csv(\"data/ch7_dat1.csv\")\ndat1 &lt;- dat1[, -1]\ndat1$class &lt;- as.factor(dat1$class)\n\n\n\n이차 판별분석 (사전확률 = 0.5)\n\nqda_fit &lt;- qda(class ~ ., data = dat1, prior = c(1 / 2, 1 / 2))\nqda_fit\n\nCall:\nqda(class ~ ., data = dat1, prior = c(1/2, 1/2))\n\nPrior probabilities of groups:\n  1   2 \n0.5 0.5 \n\nGroup means:\n   X1  X2\n1 4.0 6.0\n2 6.6 5.4\n\n\n\n\n각 객체에 대한 추정\n추정 실행\n\npred &lt;- predict(qda_fit, dat1)\n\n추정된 사후확률\n\npred$posterior\n\n             1           2\n1 9.201462e-01 0.079853771\n2 2.855742e-05 0.999971443\n3 3.675551e-01 0.632444929\n4 3.981233e-02 0.960187670\n5 9.918634e-01 0.008136619\n6 9.909543e-01 0.009045678\n7 5.387326e-01 0.461267351\n8 7.218143e-03 0.992781857\n9 2.184825e-03 0.997815175\n\n\n추정된 범주\n\npred$class\n\n[1] 1 2 2 2 1 1 1 2 2\nLevels: 1 2\n\n\n추정결과 요약\n\nresults &lt;- cbind(dat1, posterior = pred$posterior, pred_class = pred$class)\nresults\n\n  X1 X2 class  posterior.1 posterior.2 pred_class\n1  5  7     1 9.201462e-01 0.079853771          1\n2  4  3     2 2.855742e-05 0.999971443          2\n3  7  8     2 3.675551e-01 0.632444929          2\n4  8  6     2 3.981233e-02 0.960187670          2\n5  3  6     1 9.918634e-01 0.008136619          1\n6  2  5     1 9.909543e-01 0.009045678          1\n7  6  6     1 5.387326e-01 0.461267351          1\n8  9  6     2 7.218143e-03 0.992781857          2\n9  5  4     2 2.184825e-03 0.997815175          2\n\n\n\n\n추정 성능\n\nmetrics_multi &lt;- metric_set(accuracy, sens, spec, f_meas, roc_auc)\nmetrics_multi(results, truth = \"class\", estimate = \"pred_class\", posterior.1)\n\n# A tibble: 5 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary             1\n2 sens     binary             1\n3 spec     binary             1\n4 f_meas   binary             1\n5 roc_auc  binary             1"
  },
  {
    "objectID": "chapters/ch07_da.html#예-7.7",
    "href": "chapters/ch07_da.html#예-7.7",
    "title": "7장 판별분석",
    "section": "(예 7.7)",
    "text": "(예 7.7)\n\n패키지 로드\n\nlibrary(MASS)\nlibrary(yardstick)\n\n\n\n데이터 로드\n\niris &lt;- read.csv(\"data/iris.csv\", stringsAsFactors = TRUE)\nnames(iris) &lt;- c(\"x1\", \"x2\", \"x3\", \"x4\", \"class\")\n\n\n\n학습 데이터 생성\n\ntr_idx &lt;- c(1:30, 51:80, 101:130)\ntrain &lt;- iris[tr_idx, ]\n\n\n\n선형 판별분석\n\niris_lda &lt;- lda(class ~ ., data = train, prior = c(1 / 3, 1 / 3, 1 / 3))\niris_lda\n\nCall:\nlda(class ~ ., data = train, prior = c(1/3, 1/3, 1/3))\n\nPrior probabilities of groups:\n    setosa versicolor  virginica \n 0.3333333  0.3333333  0.3333333 \n\nGroup means:\n                 x1       x2       x3        x4\nsetosa     5.026667 3.450000 1.473333 0.2466667\nversicolor 6.070000 2.790000 4.333333 1.3533333\nvirginica  6.583333 2.933333 5.603333 2.0066667\n\nCoefficients of linear discriminants:\n          LD1        LD2\nx1  0.5711419  1.2397647\nx2  1.8752911 -3.0223980\nx3 -1.7361767 -0.3159667\nx4 -3.4672646 -1.3954748\n\nProportion of trace:\n   LD1    LD2 \n0.9929 0.0071 \n\n\n\n\n객체 분류 - 각 범주별 첫 번째 객체\n\npred_idx &lt;- c(1, 51, 101)\npred &lt;- predict(iris_lda, newdata = iris[pred_idx, -5])\npred\n\n$class\n[1] setosa     versicolor virginica \nLevels: setosa versicolor virginica\n\n$posterior\n          setosa   versicolor    virginica\n1   1.000000e+00 5.571341e-22 6.338484e-42\n51  4.294887e-18 9.999507e-01 4.932755e-05\n101 1.375820e-50 1.381378e-08 1.000000e+00\n\n$x\n          LD1        LD2\n1    8.023770 -0.1621690\n51  -1.343748  0.3828433\n101 -7.627039 -2.7330107\n\n\n요약표\n\nresults &lt;- cbind(iris[pred_idx, ], pred_class = pred$class)\nresults\n\n     x1  x2  x3  x4      class pred_class\n1   5.1 3.5 1.4 0.2     setosa     setosa\n51  7.0 3.2 4.7 1.4 versicolor versicolor\n101 6.3 3.3 6.0 2.5  virginica  virginica\n\n\n\n\nOPTIONAL: 테스트 데이터에 대해 추정 수행\n\ntest &lt;- iris[-tr_idx, ]\npred_test &lt;- predict(iris_lda, newdata = iris[-tr_idx, -5])\nresults_test &lt;- cbind(test, pred_class = pred_test$class)\n\n정오분류표\n\ncm &lt;- conf_mat(results_test, truth = \"class\", estimate = \"pred_class\")\ncm\n\n            Truth\nPrediction   setosa versicolor virginica\n  setosa         20          0         0\n  versicolor      0         19         1\n  virginica       0          1        19"
  },
  {
    "objectID": "chapters/ch08_tree.html#예-8.2---8.7",
    "href": "chapters/ch08_tree.html#예-8.2---8.7",
    "title": "8장 트리기반 기법",
    "section": "(예 8.2 - 8.7)",
    "text": "(예 8.2 - 8.7)\n\n패키지 로드\n\nlibrary(rpart)\nlibrary(yardstick)\n\n\n\n데이터 로드\n\ndf_train &lt;- read.csv(\"data/ch8_dat1.csv\")\ndf_train$class &lt;- factor(df_train$class, levels = c(1, 2))\n\ndf_test &lt;- read.csv(\"data/ch8_dat2.csv\")\ndf_test$class &lt;- factor(df_test$class, levels = c(1, 2))\n\n\n\n(예 8.2)\nOne-level tree (sometime called decision stump)\n\ncontrol_depth1 &lt;- rpart.control(\n  minsplit = 2, # attempt split when two or more observations are in a node\n  cp = 0, # attempt split even if does not decrease misclassification cost\n  xval = 0, # no cross-validation\n  maxdepth = 1 # make only one split from root node\n)\n\ntree_depth1 &lt;- rpart(\n  class ~ x1 + x2,\n  data = df_train,\n  control = control_depth1\n)\n\nprint(tree_depth1)\n\nn= 10 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 10 5 1 (0.5000000 0.5000000)  \n  2) x2&gt;=5.5 3 0 1 (1.0000000 0.0000000) *\n  3) x2&lt; 5.5 7 2 2 (0.2857143 0.7142857) *\n\n\n\n\n(예 8.3)\nTwo-levels decision tree\n\ncontrol_depth2 &lt;- rpart.control(\n  minsplit = 2, # attempt split when two or more observations are in a node\n  cp = 0, # attempt split even if does not decrease misclassification cost\n  xval = 0, # no cross-validation\n  maxdepth = 2 # make only one split from root node\n)\n\ntree_depth2 &lt;- rpart(\n  class ~ x1 + x2,\n  data = df_train,\n  control = control_depth2\n)\n\nprint(tree_depth2)\n\nn= 10 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 10 5 1 (0.5000000 0.5000000)  \n  2) x2&gt;=5.5 3 0 1 (1.0000000 0.0000000) *\n  3) x2&lt; 5.5 7 2 2 (0.2857143 0.7142857)  \n    6) x1&lt; 1.5 1 0 1 (1.0000000 0.0000000) *\n    7) x1&gt;=1.5 6 1 2 (0.1666667 0.8333333) *\n\n\n최대 트리\n\ncontrol_maximal &lt;- rpart.control(\n  minsplit = 2, # attempt split when two or more observations are in a node\n  cp = 0, # attempt split even if does not decrease misclassification cost\n  xval = 0 # no cross-validation\n)\n\ntree_maximal &lt;- rpart(\n  class ~ x1 + x2,\n  data = df_train,\n  control = control_maximal\n)\n\nprint(tree_maximal)\n\nn= 10 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 10 5 1 (0.5000000 0.5000000)  \n   2) x2&gt;=5.5 3 0 1 (1.0000000 0.0000000) *\n   3) x2&lt; 5.5 7 2 2 (0.2857143 0.7142857)  \n     6) x1&lt; 1.5 1 0 1 (1.0000000 0.0000000) *\n     7) x1&gt;=1.5 6 1 2 (0.1666667 0.8333333)  \n      14) x2&gt;=4.5 2 1 1 (0.5000000 0.5000000)  \n        28) x1&lt; 3 1 0 1 (1.0000000 0.0000000) *\n        29) x1&gt;=3 1 0 2 (0.0000000 1.0000000) *\n      15) x2&lt; 4.5 4 0 2 (0.0000000 1.0000000) *\n\n\n\n\n(예 8.4)\n\nrel error: Root 노드만 지닌 트리에 비해, 각 단계별 분지된 트리의 상대적 오분류비용\nnsplit: 해당 트리까지의 분지 횟수\n\n\n# \"rel error\" is R(T)/R(Root), where Root means a tree with no split\n# \"nsplit\" means a number of splits made to produce tree T\nprintcp(tree_maximal)\n\n\nClassification tree:\nrpart(formula = class ~ x1 + x2, data = df_train, control = control_maximal)\n\nVariables actually used in tree construction:\n[1] x1 x2\n\nRoot node error: 5/10 = 0.5\n\nn= 10 \n\n   CP nsplit rel error\n1 0.6      0       1.0\n2 0.2      1       0.4\n3 0.1      2       0.2\n4 0.0      4       0.0\n\n\n\n\n(예 8.5 - 8.6)\n\nT1: 최대 트리\n\n\nT1 &lt;- tree_maximal\nprint(T1)\n\nn= 10 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 10 5 1 (0.5000000 0.5000000)  \n   2) x2&gt;=5.5 3 0 1 (1.0000000 0.0000000) *\n   3) x2&lt; 5.5 7 2 2 (0.2857143 0.7142857)  \n     6) x1&lt; 1.5 1 0 1 (1.0000000 0.0000000) *\n     7) x1&gt;=1.5 6 1 2 (0.1666667 0.8333333)  \n      14) x2&gt;=4.5 2 1 1 (0.5000000 0.5000000)  \n        28) x1&lt; 3 1 0 1 (1.0000000 0.0000000) *\n        29) x1&gt;=3 1 0 2 (0.0000000 1.0000000) *\n      15) x2&lt; 4.5 4 0 2 (0.0000000 1.0000000) *\n\n\n\nT2: 첫 번째 가지치기된 트리\n\n\nT2 &lt;- snip.rpart(T1, toss = 7) # node number corresponding to t5 in the book\nprint(T2)\n\nn= 10 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 10 5 1 (0.5000000 0.5000000)  \n  2) x2&gt;=5.5 3 0 1 (1.0000000 0.0000000) *\n  3) x2&lt; 5.5 7 2 2 (0.2857143 0.7142857)  \n    6) x1&lt; 1.5 1 0 1 (1.0000000 0.0000000) *\n    7) x1&gt;=1.5 6 1 2 (0.1666667 0.8333333) *\n\n\n\nT3: 두 번째 가지치기된 트리\n\n\nT3 &lt;- snip.rpart(T2, toss = 3) \nprint(T3)\n\nn= 10 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 10 5 1 (0.5000000 0.5000000)  \n  2) x2&gt;=5.5 3 0 1 (1.0000000 0.0000000) *\n  3) x2&lt; 5.5 7 2 2 (0.2857143 0.7142857) *\n\n\n\n\n(예 8.7)\n테스트 표본에 대한 분류오류율\n\npredict(T1, df_test, type = \"class\")\n\n1 2 3 4 5 6 \n1 1 2 2 1 1 \nLevels: 1 2\n\n1 - accuracy_vec(df_test$class, predict(T1, df_test, type = \"class\"))\n\n[1] 0.1666667\n\npredict(T2, df_test, type = \"class\")\n\n1 2 3 4 5 6 \n1 1 2 2 1 1 \nLevels: 1 2\n\n1 - accuracy_vec(df_test$class, predict(T2, df_test, type = \"class\"))\n\n[1] 0.1666667\n\npredict(T3, df_test, type = \"class\")\n\n1 2 3 4 5 6 \n2 2 2 2 1 2 \nLevels: 1 2\n\n1 - accuracy_vec(df_test$class, predict(T3, df_test, type = \"class\"))\n\n[1] 0.3333333\n\n\n\n\nOPTIONAL: {rpart}에서의 가지치기\n{rpart}의 함수 prune()은 인자 cp가 주어졌을 때 다음과 같이 크기가 조정된(scaled) 비용-복잡도 척도를 최소화하는 가지치기를 수행한다.\n\\[\n\\frac{R(T)}{R(Root)} + \\text{cp} \\times |T|\n\\]\n여기에서 \\(R(Root)\\)는 분지가 전혀 없이 뿌리 노드만 존재하는 트리의 오분류 비용이다.\n\nprune(tree_maximal, cp = 0.15)\n\nn= 10 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 10 5 1 (0.5000000 0.5000000)  \n  2) x2&gt;=5.5 3 0 1 (1.0000000 0.0000000) *\n  3) x2&lt; 5.5 7 2 2 (0.2857143 0.7142857)  \n    6) x1&lt; 1.5 1 0 1 (1.0000000 0.0000000) *\n    7) x1&gt;=1.5 6 1 2 (0.1666667 0.8333333) *"
  },
  {
    "objectID": "chapters/ch09_svm.html#예-9.1",
    "href": "chapters/ch09_svm.html#예-9.1",
    "title": "9장 서포트 벡터 머신",
    "section": "(예 9.1)",
    "text": "(예 9.1)\n\n패키지 로드\n\nlibrary(kernlab)\n\n\n\n데이터 로드\n\ndat &lt;- read.csv(\"data/ch9_dat1.csv\")\ndat$class &lt;- factor(dat$class)\ndat\n\n  x1 x2 class\n1  5  7     1\n2  4  3    -1\n3  7  8     1\n4  8  6     1\n5  3  6    -1\n6  2  5    -1\n7  6  6     1\n8  9  6     1\n9  5  4    -1\n\n\n\n\nSVM 학습\n\nmodel &lt;- ksvm(\n  class ~ x1 + x2,\n  data = dat,\n  scaled = FALSE,\n  kernel = \"vanilladot\"\n)\n\n Setting default kernel parameters  \n\n\n\n\n서포트 벡터 객체\n\nmodel@alphaindex[[1]]\n\n[1] 1 5 7 9\n\n\n\n\n최적해: \\(\\alpha\\)값 (서포트 벡터)\n\nmodel@alpha[[1]]\n\n[1] 0.1555556 0.1888889 0.2888889 0.2555556\n\n\n\n\n목적함수값\n\n-model@obj\n\n[1] 0.4444444\n\n\n\n\n하이퍼플레인 추정\n계수 (w)\n\nw &lt;- model@coef[[1]] %*% model@xmatrix[[1]]\nw\n\n            x1        x2\n[1,] 0.6666667 0.6666667\n\n\n절편 (b)\n\n-model@b\n\n[1] -7\n\n\n\n\n시각화\n\nplot(model, data = dat)"
  },
  {
    "objectID": "chapters/ch09_svm.html#예-9.2",
    "href": "chapters/ch09_svm.html#예-9.2",
    "title": "9장 서포트 벡터 머신",
    "section": "(예 9.2)",
    "text": "(예 9.2)\n\n패키지 로드\n\nlibrary(kernlab)\n\n\n\n데이터 로드\n\ndat &lt;- read.csv(\"data/ch9_dat2.csv\")\ndat$class &lt;- factor(dat$class)\ndat\n\n   x1 x2 class\n1   5  7     1\n2   4  3    -1\n3   7  8     1\n4   8  6     1\n5   3  6    -1\n6   2  5    -1\n7   6  6     1\n8   9  6     1\n9   5  4    -1\n10  7  6    -1\n\n\n\n\nSVM 학습\n\nmodel &lt;- ksvm(\n  class ~ x1 + x2,\n  data = dat,\n  scaled = FALSE,\n  kernel = \"vanilladot\",\n  C = 1\n)\n\n Setting default kernel parameters  \n\n\n\n\n서포트 벡터 객체\n\nmodel@alphaindex[[1]]\n\n[1]  1  5  7 10\n\n\n\n\n최적해: \\(\\alpha\\)값 (서포트 벡터)\n\nmodel@alpha[[1]]\n\n[1] 0.8 0.8 1.0 1.0\n\n\n\n\n목적함수값\n\n-model@obj\n\n[1] 3.1\n\n\n\n\n하이퍼플레인 추정\n계수 (w)\n\nw &lt;- model@coef[[1]] %*% model@xmatrix[[1]]\nw\n\n      x1  x2\n[1,] 0.6 0.8\n\n\n절편 (b)\n\n-model@b\n\n[1] -7.6\n\n\n\n\n오분류 객체\n\nwhich(model@ymatrix != as.integer(model@fitted))\n\n[1] 10\n\n\n\n\n시각화\n\nplot(model, data = dat)\n\n\n\n\n\n\n패널티 단가(C) 변경: 1, 5, 100\n\n# Try with C = 1, C = 5 and C = 100\nCs &lt;- c(1, 5, 100)\nmodels &lt;- vector(\"list\", length = length(Cs))\n\nfor (i in seq_along(Cs)) {\n  message(paste0(\"-------\\nC = \", Cs[i], \"\\n-------\"))\n\n  # SVM with 2nd order polynomial kernel\n  models[[i]] &lt;- ksvm(\n    class ~ x1 + x2,\n    data = dat,\n    scaled = FALSE,\n    kernel = \"vanilladot\",\n    C = Cs[i]\n  )\n\n  # support vectors\n  message(\"Support vectors:\")\n  print(models[[i]]@alphaindex[[1]])\n\n  # alpha values for support vectors\n  message(\"Alpha values for support vectors:\")\n  print(round(models[[i]]@alpha[[1]], 4))\n\n  # objective value\n  # note that we placed minus(-) sign\n  message(\"Objective value:\")\n  print(-models[[i]]@obj)\n\n  # hyperplane coefficient vector w and intercept b\n  message(\"Hyperplane coefficients (w):\")\n  w &lt;- models[[i]]@coef[[1]] %*% models[[i]]@xmatrix[[1]]\n  print(w)\n\n  # hyperplane's intercept b\n  # Note that we placed minus(-) sign\n  message(\"Hyperplane intercept (b):\")\n  print(-models[[i]]@b)\n\n  # misclassified objects\n  message(\"Misclassified instances:\")\n  print(which(models[[i]]@ymatrix != as.integer(models[[i]]@fitted)))\n}\n\n-------\nC = 1\n-------\n\n\n Setting default kernel parameters  \n\n\nSupport vectors:\n\n\n[1]  1  5  7 10\n\n\nAlpha values for support vectors:\n\n\n[1] 0.8 0.8 1.0 1.0\n\n\nObjective value:\n\n\n[1] 3.1\n\n\nHyperplane coefficients (w):\n\n\n      x1  x2\n[1,] 0.6 0.8\n\n\nHyperplane intercept (b):\n\n\n[1] -7.6\n\n\nMisclassified instances:\n\n\n[1] 10\n\n\n-------\nC = 5\n-------\n\n\n Setting default kernel parameters  \n\n\nSupport vectors:\n\n\n[1]  1  4  5  7 10\n\n\nAlpha values for support vectors:\n\n\n[1] 1.1991 0.6004 1.7995 5.0000 5.0000\n\n\nObjective value:\n\n\n[1] 12.8\n\n\nHyperplane coefficients (w):\n\n\n      x1       x2\n[1,] 0.4 1.199115\n\n\nHyperplane intercept (b):\n\n\n[1] -9.394396\n\n\nMisclassified instances:\n\n\n[1] 10\n\n\n-------\nC = 100\n-------\n\n\n Setting default kernel parameters  \n\n\nSupport vectors:\n\n\n[1]  1  4  5  7 10\n\n\nAlpha values for support vectors:\n\n\n[1]   1.2004  19.5999  20.8003 100.0000 100.0000\n\n\nObjective value:\n\n\n[1] 240.8\n\n\nHyperplane coefficients (w):\n\n\n            x1       x2\n[1,] 0.4001396 1.200419\n\n\nHyperplane intercept (b):\n\n\n[1] -9.403397\n\n\nMisclassified instances:\n\n\n[1] 10"
  },
  {
    "objectID": "chapters/ch09_svm.html#예-9.3",
    "href": "chapters/ch09_svm.html#예-9.3",
    "title": "9장 서포트 벡터 머신",
    "section": "(예 9.3)",
    "text": "(예 9.3)\n\n패키지 로드\n\nlibrary(kernlab)\n\n\n\n데이터 로드\n\ndat &lt;- matrix(c(1, 2, 2, 2, 2, -1), nrow = 3, byrow = TRUE)\ndat\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    2    2\n[3,]    2   -1\n\n\n\n\n가우시안 커널\n\nkernelMatrix(rbfdot(sigma = 1 / 2), dat)\n\nAn object of class \"kernelMatrix\"\n            [,1]      [,2]        [,3]\n[1,] 1.000000000 0.6065307 0.006737947\n[2,] 0.606530660 1.0000000 0.011108997\n[3,] 0.006737947 0.0111090 1.000000000\n\n\n\n\n이차 커널\n\nkernelMatrix(polydot(degree = 2), dat)\n\nAn object of class \"kernelMatrix\"\n     [,1] [,2] [,3]\n[1,]   36   49    1\n[2,]   49   81    9\n[3,]    1    9   36\n\n\n\n\n시그모이드 커널 (하이퍼볼릭 탄젠트 커널)\n\nkernelMatrix(tanhdot(offset = 0), dat)\n\nAn object of class \"kernelMatrix\"\n          [,1]      [,2]      [,3]\n[1,] 0.9999092 0.9999877 0.0000000\n[2,] 0.9999877 0.9999998 0.9640276\n[3,] 0.0000000 0.9640276 0.9999092\n\n\n\n\n선형 커널\n\nkernelMatrix(vanilladot(), dat)\n\nAn object of class \"kernelMatrix\"\n     [,1] [,2] [,3]\n[1,]    5    6    0\n[2,]    6    8    2\n[3,]    0    2    5"
  },
  {
    "objectID": "chapters/ch09_svm.html#예-9.6",
    "href": "chapters/ch09_svm.html#예-9.6",
    "title": "9장 서포트 벡터 머신",
    "section": "(예 9.6)",
    "text": "(예 9.6)\n\n패키지 로드\n\nlibrary(kernlab)\n\n\n\n데이터 로드\n\ndat &lt;- data.frame(\n  x1 = c(-1, -1, 1, 1),\n  x2 = c(-1, 1, -1, 1),\n  class = factor(c(-1, 1, 1, -1))\n)\ndat\n\n  x1 x2 class\n1 -1 -1    -1\n2 -1  1     1\n3  1 -1     1\n4  1  1    -1\n\n\n\n\n이차 커널\n\nX &lt;- as.matrix(dat[, 1:2])\nK &lt;- kernelMatrix(polydot(degree = 2), X)\nK\n\nAn object of class \"kernelMatrix\"\n     [,1] [,2] [,3] [,4]\n[1,]    9    1    1    1\n[2,]    1    9    1    1\n[3,]    1    1    9    1\n[4,]    1    1    1    9\n\n\n\n\nSVM 학습\n\nmodel &lt;- ksvm(\n  class ~ x1 + x2,\n  data = dat,\n  scaled = FALSE,\n  kernel = polydot(degree = 2)\n)\n\n\n\n서포트 벡터 객체\n\nmodel@alphaindex[[1]]\n\n[1] 1 2 3 4\n\n\n\n\n최적해: \\(\\alpha\\)값 (서포트 벡터)\n\nmodel@alpha[[1]]\n\n[1] 0.125 0.125 0.125 0.125\n\n\n\n\n하이퍼플레인 추정\n계수 (beta)\n\nbeta1 &lt;- 2 * sum(model@coef[[1]] * model@xmatrix[[1]][, \"x1\"])\nbeta2 &lt;- 2 * sum(model@coef[[1]] * model@xmatrix[[1]][, \"x2\"])\nbeta11 &lt;- sum(model@coef[[1]] * model@xmatrix[[1]][, \"x1\"]^2)\nbeta22 &lt;- sum(model@coef[[1]] * model@xmatrix[[1]][, \"x2\"]^2)\nbeta12 &lt;- 2 * sum(model@coef[[1]] * apply(model@xmatrix[[1]], 1, prod))\nbetas &lt;- c(beta1, beta2, beta11, beta22, beta12)\nnames(betas) &lt;- c(\"beta1\", \"beta2\", \"beta11\", \"beta22\", \"beta12\")\nround(betas, 4)\n\n beta1  beta2 beta11 beta22 beta12 \n     0      0      0      0     -1 \n\n\n절편 (b)\n\n-model@b\n\n[1] 0"
  },
  {
    "objectID": "chapters/ch09_svm.html#예-9.7",
    "href": "chapters/ch09_svm.html#예-9.7",
    "title": "9장 서포트 벡터 머신",
    "section": "(예 9.7)",
    "text": "(예 9.7)\n\n패키지 로드\n\nlibrary(kernlab)\n\n\n\n데이터 로드\n\ndat &lt;- read.csv(\"data/ch9_dat3.csv\")\ndat$class &lt;- factor(dat$class)\ndat\n\n  x1 x2 class\n1  5  7     1\n2  4  3    -1\n3  7  8    -1\n4  8  6    -1\n5  3  6     1\n6  2  5     1\n7  6  6     1\n8  9  6    -1\n9  5  4    -1\n\n\n\n\nSVM 학습\n\nmodel &lt;- ksvm(\n  class ~ x1 + x2,\n  data = dat,\n  scaled = FALSE,\n  kernel = polydot(degree = 2),\n  C = 1\n)\n\n\n\n서포트 벡터 객체\n\nmodel@alphaindex[[1]]\n\n[1] 1 2 3 7 9\n\n\n\n\n최적해: \\(\\alpha\\)값 (서포트 벡터)\n\nmodel@alpha[[1]]\n\n[1] 0.23555183 0.50094882 0.65316473 1.00000000 0.08143829\n\n\n\n\n하이퍼플레인 추정\n\nbeta1 &lt;- 2 * sum(model@coef[[1]] * model@xmatrix[[1]][, \"x1\"])\nbeta2 &lt;- 2 * sum(model@coef[[1]] * model@xmatrix[[1]][, \"x2\"])\nbeta11 &lt;- sum(model@coef[[1]] * model@xmatrix[[1]][, \"x1\"]^2)\nbeta22 &lt;- sum(model@coef[[1]] * model@xmatrix[[1]][, \"x2\"]^2)\nbeta12 &lt;- 2 * sum(model@coef[[1]] * apply(model@xmatrix[[1]], 1, prod))\nhyperplane &lt;- c(-model@b, beta1, beta2, beta11, beta22, beta12)\nnames(hyperplane) &lt;- c(\"b\", \"beta1\", \"beta2\", \"beta11\", \"beta22\", \"beta12\")\nround(hyperplane, 4)\n\n      b   beta1   beta2  beta11  beta22  beta12 \n-3.4455  0.3892  1.1899 -0.1674 -0.0721  0.0539 \n\n\n\n\n오분류 객체\n\nwhich(model@ymatrix != as.integer(model@fitted))\n\n[1] 7\n\n\n\n\n패널티 단가(C) 변경: 1, 5, 100\n\n# Try with C = 1, C = 5 and C = 100\nCs &lt;- c(1, 5, 100)\nmodels &lt;- vector(\"list\", length = length(Cs))\n\nfor (i in seq_along(Cs)) {\n  message(paste0(\"-------\\nC = \", Cs[i], \"\\n-------\"))\n\n  # SVM with 2nd order polynomial kernel\n  models[[i]] &lt;- ksvm(\n    class ~ x1 + x2,\n    data = dat,\n    scaled = FALSE,\n    kernel = polydot(degree = 2),\n    C = Cs[i]\n  )\n\n  # support vectors\n  message(\"Support vectors:\")\n  print(models[[i]]@alphaindex[[1]])\n\n  # alpha values for support vectors\n  message(\"Alpha values for support vectors:\")\n  print(round(models[[i]]@alpha[[1]], 4))\n\n  # hyperplane\n  beta1 &lt;- 2 * sum(models[[i]]@coef[[1]] * models[[i]]@xmatrix[[1]][, \"x1\"])\n  beta2 &lt;- 2 * sum(models[[i]]@coef[[1]] * models[[i]]@xmatrix[[1]][, \"x2\"])\n  beta11 &lt;- sum(models[[i]]@coef[[1]] * models[[i]]@xmatrix[[1]][, \"x1\"]^2)\n  beta22 &lt;- sum(models[[i]]@coef[[1]] * models[[i]]@xmatrix[[1]][, \"x2\"]^2)\n  beta12 &lt;- 2 * sum(models[[i]]@coef[[1]] * apply(models[[i]]@xmatrix[[1]], 1, prod))\n  hyperplane &lt;- c(-models[[i]]@b, beta1, beta2, beta11, beta22, beta12)\n  names(hyperplane) &lt;- c(\"b\", \"beta1\", \"beta2\", \"beta11\", \"beta22\", \"beta12\")\n  message(\"Hyperplane coefficients:\")\n  print(round(hyperplane, 4))\n\n  # misclassified objects\n  message(\"Misclassified instances:\")\n  print(which(models[[i]]@ymatrix != as.integer(models[[i]]@fitted)))\n}\n\n-------\nC = 1\n-------\n\n\nSupport vectors:\n\n\n[1] 1 2 3 7 9\n\n\nAlpha values for support vectors:\n\n\n[1] 0.2356 0.5009 0.6532 1.0000 0.0814\n\n\nHyperplane coefficients:\n\n\n      b   beta1   beta2  beta11  beta22  beta12 \n-3.4455  0.3892  1.1899 -0.1674 -0.0721  0.0539 \n\n\nMisclassified instances:\n\n\n[1] 7\n\n\n-------\nC = 5\n-------\nSupport vectors:\n\n\n[1] 2 3 6 7 9\n\n\nAlpha values for support vectors:\n\n\n[1] 0.0865 2.1573 0.2357 5.0000 2.9919\n\n\nHyperplane coefficients:\n\n\n      b   beta1   beta2  beta11  beta22  beta12 \n-5.2716  0.1297  3.3860 -0.9460 -0.8234  1.3451 \n\n\nMisclassified instances:\n\n\ninteger(0)\n\n\n-------\nC = 100\n-------\nSupport vectors:\n\n\n[1] 2 3 6 7 9\n\n\nAlpha values for support vectors:\n\n\n[1] 0.0152 2.7412 0.2814 6.3923 3.9173\n\n\nHyperplane coefficients:\n\n\n      b   beta1   beta2  beta11  beta22  beta12 \n-6.2803  0.1622  4.2330 -1.2444 -1.0915  1.8056 \n\n\nMisclassified instances:\n\n\ninteger(0)"
  },
  {
    "objectID": "chapters/ch09_svm.html#예-9.8",
    "href": "chapters/ch09_svm.html#예-9.8",
    "title": "9장 서포트 벡터 머신",
    "section": "(예 9.8)",
    "text": "(예 9.8)\n\n패키지 로드\n\nlibrary(kernlab)\nlibrary(yardstick)\n\n\n\n데이터 로드\n파일 읽기\n\ndat &lt;- read.csv(\"data/breast-cancer-wisconsin.csv\")\ndat$class &lt;- factor(dat$class)\n\n결측값 존재 객체 제외\n\ndat &lt;- na.omit(dat)\n\n학습표본 / 테스트 표본 분리\n\ntrain_idx &lt;- which(dat$X1 %% 3 &lt; 2)\ntrain_dat &lt;- dat[train_idx, ]\ntest_dat &lt;- dat[-train_idx, ]\n\n\nSVM (가우시안 커널, \\(\\sigma = 1\\), \\(C = 10\\))\n모형 학습\n\nmodel1 &lt;- ksvm(\n  class ~ . - X1,\n  data = train_dat,\n  kernel = \"rbfdot\",\n  kpar = list(sigma = 1),\n  C = 10\n)\n\nprint(model1)\n\nSupport Vector Machine object of class \"ksvm\" \n\nSV type: C-svc  (classification) \n parameter : cost C = 10 \n\nGaussian Radial Basis kernel function. \n Hyperparameter : sigma =  1 \n\nNumber of Support Vectors : 206 \n\nObjective Function Value : -49.7701 \nTraining error : 0 \n\n\n학습표본 정오분류표\n\ntrain_results1 &lt;- data.frame(\n  class = train_dat$class,\n  pred_class = predict(model1, train_dat)\n)\nconf_mat(train_results1, truth = \"class\", estimate = \"pred_class\")\n\n          Truth\nPrediction   2   4\n         2 298   0\n         4   0 157\n\n\n테스트 표본 정오분류표\n\ntest_results1 &lt;- data.frame(\n  class = test_dat$class,\n  pred_class = predict(model1, test_dat)\n)\nconf_mat(test_results1, truth = \"class\", estimate = \"pred_class\")\n\n          Truth\nPrediction   2   4\n         2 141   2\n         4   5  80\n\n\n\n\nSVM (가우시안 커널, \\(\\sigma = 2\\), \\(C = 10\\))\n모형 학습\n\nmodel2 &lt;- ksvm(\n  class ~ . - X1,\n  data = train_dat,\n  kernel = \"rbfdot\",\n  kpar = list(sigma = 2),\n  C = 10\n)\n\nprint(model2)\n\nSupport Vector Machine object of class \"ksvm\" \n\nSV type: C-svc  (classification) \n parameter : cost C = 10 \n\nGaussian Radial Basis kernel function. \n Hyperparameter : sigma =  2 \n\nNumber of Support Vectors : 240 \n\nObjective Function Value : -66.1847 \nTraining error : 0 \n\n\n학습표본 정오분류표\n\ntrain_results2 &lt;- data.frame(\n  class = train_dat$class,\n  pred_class = predict(model2, train_dat)\n)\nconf_mat(train_results2, truth = \"class\", estimate = \"pred_class\")\n\n          Truth\nPrediction   2   4\n         2 298   0\n         4   0 157\n\n\n테스트 표본 정오분류표\n\ntest_results2 &lt;- data.frame(\n  class = test_dat$class,\n  pred_class = predict(model2, test_dat)\n)\nconf_mat(test_results2, truth = \"class\", estimate = \"pred_class\")\n\n          Truth\nPrediction   2   4\n         2 137   0\n         4   9  82"
  },
  {
    "objectID": "chapters/ch11_cluster.html#예-11.1-11.3",
    "href": "chapters/ch11_cluster.html#예-11.1-11.3",
    "title": "11장 군집분석 개요",
    "section": "(예 11.1, 11.3)",
    "text": "(예 11.1, 11.3)\n\n데이터 읽기\n\ndat1 &lt;- read.csv(\"data/ch11_dat1.csv\")\ndat2 &lt;- dat1[, -1]\n\n\n\n(예 11.1)\n유클리디안 거리\n\nD1 &lt;- dist(dat2)\nround(D1, 2)\n\n       1     2     3     4     5     6     7     8     9\n2   8.31                                                \n3  24.74 16.76                                          \n4  16.91  9.11  7.87                                    \n5  15.17  9.43 12.08  6.48                              \n6  10.10  3.00 16.55  9.49 11.31                        \n7  27.17 19.05  3.16 10.39 15.17 18.49                  \n8  27.87 20.35 10.05 13.15 19.10 19.52  8.31            \n9  33.03 25.57 14.53 18.57 24.52 24.52 12.04  5.48      \n10 24.37 17.75 11.75 11.66 17.03 17.49 11.49  5.74 10.05\n\n\n행렬로 변환\n\nD1_mat &lt;- as.matrix(D1)\nround(D1_mat, 2)\n\n       1     2     3     4     5     6     7     8     9    10\n1   0.00  8.31 24.74 16.91 15.17 10.10 27.17 27.87 33.03 24.37\n2   8.31  0.00 16.76  9.11  9.43  3.00 19.05 20.35 25.57 17.75\n3  24.74 16.76  0.00  7.87 12.08 16.55  3.16 10.05 14.53 11.75\n4  16.91  9.11  7.87  0.00  6.48  9.49 10.39 13.15 18.57 11.66\n5  15.17  9.43 12.08  6.48  0.00 11.31 15.17 19.10 24.52 17.03\n6  10.10  3.00 16.55  9.49 11.31  0.00 18.49 19.52 24.52 17.49\n7  27.17 19.05  3.16 10.39 15.17 18.49  0.00  8.31 12.04 11.49\n8  27.87 20.35 10.05 13.15 19.10 19.52  8.31  0.00  5.48  5.74\n9  33.03 25.57 14.53 18.57 24.52 24.52 12.04  5.48  0.00 10.05\n10 24.37 17.75 11.75 11.66 17.03 17.49 11.49  5.74 10.05  0.00\n\n\n2번째 객체와 4번째 객체 간 거리\n\nD1_mat[2, 4]\n\n[1] 9.110434\n\n\n2번째 객체와 5번째 객체 간 거리\n\nD1_mat[2, 5]\n\n[1] 9.433981\n\n\n데이터 표준화\n\nstd_dat2 &lt;- scale(dat2)\nattr(std_dat2, \"scaled:center\") # mean\n\n  X1   X2   X3 \n36.8  8.5  7.8 \n\nattr(std_dat2, \"scaled:scale\") # standard deviation\n\n      X1       X2       X3 \n9.738811 4.478343 4.661902 \n\n\n표준화된 유클리디안 거리\n\nstd_D1 &lt;- dist(std_dat2)\nstd_D1_mat &lt;- as.matrix(std_D1)\nround(std_D1_mat, 2)\n\n      1    2    3    4    5    6    7    8    9   10\n1  0.00 0.96 3.35 2.30 2.71 1.07 3.46 3.43 4.02 3.42\n2  0.96 0.00 2.47 1.45 2.04 0.53 2.56 2.76 3.38 2.93\n3  3.35 2.47 0.00 1.07 1.27 2.77 0.38 2.10 2.70 2.60\n4  2.30 1.45 1.07 0.00 1.05 1.82 1.20 1.95 2.65 2.29\n5  2.71 2.04 1.27 1.05 0.00 2.48 1.62 2.85 3.54 3.10\n6  1.07 0.53 2.77 1.82 2.48 0.00 2.81 2.91 3.47 3.14\n7  3.46 2.56 0.38 1.20 1.62 2.81 0.00 1.84 2.40 2.43\n8  3.43 2.76 2.10 1.95 2.85 2.91 1.84 0.00 0.71 0.80\n9  4.02 3.38 2.70 2.65 3.54 3.47 2.40 0.71 0.00 1.05\n10 3.42 2.93 2.60 2.29 3.10 3.14 2.43 0.80 1.05 0.00\n\n\n2번째 객체와 4번째 객체 간 거리\n\nstd_D1_mat[2, 4]\n\n[1] 1.454543\n\n\n2번째 객체와 5번째 객체 간 거리\n\nstd_D1_mat[2, 5]\n\n[1] 2.035148\n\n\n\n\n(예 11.3)\n상관계수\n\nrow_cor &lt;- cor(t(dat2))\nround(row_cor, 2)\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,] 1.00 0.93 0.68 0.76 0.58 0.97 0.71 0.81 0.82  0.81\n [2,] 0.93 1.00 0.90 0.94 0.83 0.99 0.92 0.97 0.97  0.97\n [3,] 0.68 0.90 1.00 0.99 0.99 0.85 1.00 0.98 0.98  0.98\n [4,] 0.76 0.94 0.99 1.00 0.97 0.90 1.00 1.00 0.99  1.00\n [5,] 0.58 0.83 0.99 0.97 1.00 0.77 0.98 0.95 0.94  0.95\n [6,] 0.97 0.99 0.85 0.90 0.77 1.00 0.87 0.93 0.94  0.93\n [7,] 0.71 0.92 1.00 1.00 0.98 0.87 1.00 0.99 0.99  0.99\n [8,] 0.81 0.97 0.98 1.00 0.95 0.93 0.99 1.00 1.00  1.00\n [9,] 0.82 0.97 0.98 0.99 0.94 0.94 0.99 1.00 1.00  1.00\n[10,] 0.81 0.97 0.98 1.00 0.95 0.93 0.99 1.00 1.00  1.00\n\n\n1번째 객체와 6번째 객체 간 상관계수\n\nrow_cor[1, 6]\n\n[1] 0.9673518\n\n\n1번째 객체와 8번째 객체 간 상관계수\n\nrow_cor[1, 8]\n\n[1] 0.8099343\n\n\n표준화 이후 상관계수\n\nstd_row_cor &lt;- cor(t(std_dat2))\nround(std_row_cor, 2)\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]\n [1,]  1.00  1.00 -0.67  0.17  0.10  0.97 -0.88 -0.80 -0.75 -0.80\n [2,]  1.00  1.00 -0.67  0.18  0.11  0.97 -0.87 -0.81 -0.76 -0.80\n [3,] -0.67 -0.67  1.00  0.62  0.67 -0.83  0.94  0.10  0.01  0.09\n [4,]  0.17  0.18  0.62  1.00  1.00 -0.07  0.32 -0.72 -0.78 -0.73\n [5,]  0.10  0.11  0.67  1.00  1.00 -0.13  0.38 -0.68 -0.74 -0.68\n [6,]  0.97  0.97 -0.83 -0.07 -0.13  1.00 -0.97 -0.64 -0.57 -0.63\n [7,] -0.88 -0.87  0.94  0.32  0.38 -0.97  1.00  0.42  0.34  0.41\n [8,] -0.80 -0.81  0.10 -0.72 -0.68 -0.64  0.42  1.00  1.00  1.00\n [9,] -0.75 -0.76  0.01 -0.78 -0.74 -0.57  0.34  1.00  1.00  1.00\n[10,] -0.80 -0.80  0.09 -0.73 -0.68 -0.63  0.41  1.00  1.00  1.00\n\n\n표준화 이후 1번째 객체와 6번째 객체 간 상관계수\n\nstd_row_cor[1, 6]\n\n[1] 0.9721348\n\n\n표준화 이후 1번째 객체와 8번째 객체 간 상관계수\n\nstd_row_cor[1, 8]\n\n[1] -0.8006556\n\n\n\n\nOPTIONAL: 민코프스키 거리 (p = 3)\n\nstd_D2 &lt;- dist(std_dat2, method = \"minkowski\", p = 3)\nround(std_D2, 2)\n\n      1    2    3    4    5    6    7    8    9\n2  0.87                                        \n3  2.81 2.06                                   \n4  1.93 1.24 0.91                              \n5  2.33 1.81 1.24 0.95                         \n6  1.03 0.46 2.34 1.62 2.21                    \n7  2.95 2.16 0.34 1.08 1.55 2.37               \n8  3.07 2.40 2.02 1.65 2.51 2.60 1.79          \n9  3.57 2.90 2.51 2.25 3.10 3.03 2.27 0.62     \n10 3.01 2.57 2.49 2.01 2.80 2.88 2.28 0.67 1.03"
  },
  {
    "objectID": "chapters/ch11_cluster.html#예-11.2",
    "href": "chapters/ch11_cluster.html#예-11.2",
    "title": "11장 군집분석 개요",
    "section": "(예 11.2)",
    "text": "(예 11.2)\n\n데이터 로드\n\nx &lt;- c(3, 4)\norigin &lt;- c(0, 0)\n\n\n\n거리 계산\n유클리디안 거리\n\ndist(rbind(x, origin))\n\n       x\norigin 5\n\n\n표준 유클리디안 거리 (s1 = 1, s2 = 2)\n\ndist(rbind(x / c(1, 2), origin / c(1, 2)))\n\n         1\n2 3.605551\n\n\n표준 유클리디안 거리 (s1 = 1, s2 = 0.5)\n\ndist(rbind(x / c(1, 0.5), origin / c(1, 0.5)))\n\n         1\n2 8.544004\n\n\n마할라노비스 거리 (s1 = 1, s2 = 2, r = 0.8)\n\ncov_mat &lt;- matrix(c(1^2, 0.8 * 1 * 2, 0.8 * 1 * 2, 2^2), nrow = 2)\nsqrt(mahalanobis(x, center = origin, cov_mat))\n\n[1] 3.073181\n\n\n마할라노비스 거리 (s1 = 1, s2 = 2, r = -0.8)\n\ncov_mat &lt;- matrix(c(1^2, -0.8 * 1 * 2, -0.8 * 1 * 2, 2^2), nrow = 2)\nsqrt(mahalanobis(x, center = origin, cov_mat))\n\n[1] 7.923243"
  },
  {
    "objectID": "chapters/ch11_cluster.html#예-11.4",
    "href": "chapters/ch11_cluster.html#예-11.4",
    "title": "11장 군집분석 개요",
    "section": "(예 11.4)",
    "text": "(예 11.4)\n\n데이터 로드\n\ndf &lt;- read.csv(\"data/ch11_dat2.csv\")\n\n\n\nJaccard 유사성 척도 계산\n\njaccard_sim &lt;- dist(df, method = \"binary\")\njaccard_sim\n\n    1   2\n2 0.5    \n3 0.8 1.0\n\njaccard_sim_mat &lt;- as.matrix(jaccard_sim)\n\n1번째 객체와 2번째 객체 간 유사성\n\njaccard_sim_mat[1, 2]\n\n[1] 0.5"
  },
  {
    "objectID": "chapters/ch12_cluster.html#예-12.1",
    "href": "chapters/ch12_cluster.html#예-12.1",
    "title": "12장 계층적 군집방법",
    "section": "(예 12.1)",
    "text": "(예 12.1)\n\n데이터 로드\n\ndat1 &lt;- read.csv(\"data/ch12_dat1.csv\")\ndat2 &lt;- dat1[, -1]\n\n\n\n유클리디안 거리\n\nD1 &lt;- dist(dat2)\nround(D1, 2)\n\n       1     2     3     4     5     6     7     8     9\n2   2.24                                                \n3  11.31  9.22                                          \n4   7.81  5.83  3.61                                    \n5  11.40  9.22  1.41  4.12                              \n6   1.41  2.24 11.40  8.06 11.31                        \n7  10.63  8.60  1.00  2.83  2.24 10.82                  \n8  10.05  9.49  9.22  7.21 10.44 11.18  8.25            \n9  11.40 11.18 11.40  9.43 12.65 12.65 10.44  2.24      \n10 12.37 12.08 11.70 10.00 13.00 13.60 10.77  2.83  1.00\n\n\n\n\n평균연결법\n\nhc_c &lt;- hclust(D1, method = \"average\")\n\n\n\n결과 시각화\n\nplot(hc_c,\n     hang = -1, cex = 0.7, main = \"Average linkage with Euclidean distance\",\n     ylab = \"Distance\", xlab = \"observation\"\n)"
  },
  {
    "objectID": "chapters/ch12_cluster.html#예-12.2",
    "href": "chapters/ch12_cluster.html#예-12.2",
    "title": "12장 계층적 군집방법",
    "section": "(예 12.2)",
    "text": "(예 12.2)\n\n데이터 로드\n\ndat1 &lt;- read.csv(\"data/ch12_dat2.csv\")\ndat2 &lt;- dat1[, -1]\n\n\n\n제곱 유클리디안 거리\n\nD1 &lt;- dist(dat2)\nD1^2\n\n    1   2   3   4   5   6   7\n2 260                        \n3   5 289                    \n4 346  82 337                \n5 173  25 212 173            \n6  32 148  29 170 117        \n7 234   2 257  64  29 122    \n8 277  53 274   5 122 125  37\n\n\n\n\n워드 방법\n\nhc_c &lt;- hclust(D1, method = \"ward.D2\")\n\n\n\n결과 시각화\n\nplot(hc_c,\n     hang = -1, cex = 1, main = \"Ward's linkage with Euclidean distance\",\n     ylab = \"Distance\", xlab = \"observation\"\n)"
  },
  {
    "objectID": "chapters/ch12_cluster.html#예-12.3",
    "href": "chapters/ch12_cluster.html#예-12.3",
    "title": "12장 계층적 군집방법",
    "section": "(예 12.3)",
    "text": "(예 12.3)\n\n패키지 로드\n\nlibrary(cluster)\n\n\n\n데이터 로드\n\ndat1 &lt;- read.csv(\"data/ch12_dat3.csv\")\ndat2 &lt;- dat1[, -1]\n\n\n\n다이아나 방법\n\nclus_diana &lt;- diana(dat2)\n\n\n\n결과 시각화\n\nplot(clus_diana, which.plots = 2, main = \"Dendrogram of Diana\")"
  },
  {
    "objectID": "chapters/ch12_cluster.html#예-12.5",
    "href": "chapters/ch12_cluster.html#예-12.5",
    "title": "12장 계층적 군집방법",
    "section": "(예 12.5)",
    "text": "(예 12.5)\n\n패키지 로드\n\nlibrary(factoextra)\n\nLoading required package: ggplot2\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\n\n\n\n데이터 로드\n\ndat1 &lt;- read.csv(\"data/ch12_dat2.csv\")\ndat2 &lt;- dat1[, -1]\n\n\n\n워드 방법 군집수 결정 - average silhouette width\n\nfviz_nbclust(dat2, hcut, method = \"silhouette\", k.max = 7)"
  },
  {
    "objectID": "chapters/ch13_cluster_dbscan.html#예-13.1",
    "href": "chapters/ch13_cluster_dbscan.html#예-13.1",
    "title": "13장 비계층적 군집방법",
    "section": "(예 13.1)",
    "text": "(예 13.1)\n\n패키지 로드\n\nlibrary(factoextra)\n\nLoading required package: ggplot2\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\n\n\n\n데이터 로드\n\ndat1 &lt;- read.csv(\"data/ch12_dat1.csv\")\ndat2 &lt;- dat1[, -1]\n\n\n\n군집 수 결정\n\n# may use scale data or raw data for the optimal k\ns_dat2 &lt;- scale(dat2)\n\nfviz_nbclust(s_dat2, kmeans, method = \"silhouette\", k.max = 5)\n\n\n\n\n\n\nK-means\n\nset.seed(123)\nkm &lt;- kmeans(dat2, 3)\nkm\n\nK-means clustering with 3 clusters of sizes 4, 3, 3\n\nCluster means:\n         X1    X2\n1 13.250000  6.75\n2  3.666667  3.00\n3  7.000000 14.00\n\nClustering vector:\n [1] 3 3 1 1 1 3 1 2 2 2\n\nWithin cluster sum of squares by cluster:\n[1] 11.500000  4.666667  4.000000\n (between_SS / total_SS =  94.6 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\n\n결과 시각화\n\nfviz_cluster(km,\n  data = dat2,\n  ellipse.type = \"convex\",\n  stand = FALSE,\n  repel = TRUE, cex = 3\n)\n\nWarning: Duplicated aesthetics after name standardisation: size"
  },
  {
    "objectID": "chapters/ch13_cluster_dbscan.html#예-13.3",
    "href": "chapters/ch13_cluster_dbscan.html#예-13.3",
    "title": "13장 비계층적 군집방법",
    "section": "(예 13.3)",
    "text": "(예 13.3)\n\n패키지 로드\n\nlibrary(cluster)\nlibrary(factoextra)\n\n\n\n데이터 로드\n\ndat1 &lt;- read.csv(\"data/ch13_pam.csv\")\ndat2 &lt;- dat1[, -1]\n\n\n\nPAM\n\npam_out &lt;- pam(dat2, 2)\npam_out\n\nMedoids:\n     ID experience hours\n[1,]  2          5     4\n[2,]  5         14     6\nClustering vector:\n[1] 1 1 2 2 2 2\nObjective function:\n   build     swap \n1.383427 1.375972 \n\nAvailable components:\n [1] \"medoids\"    \"id.med\"     \"clustering\" \"objective\"  \"isolation\" \n [6] \"clusinfo\"   \"silinfo\"    \"diss\"       \"call\"       \"data\"      \n\n\n각 군집에 속한 객체 수\n\ntable(pam_out$clustering)\n\n\n1 2 \n2 4 \n\n\n\n\n결과 시각화\n\nfviz_cluster(pam_out,\n  data = dat2,\n  ellipse.type = \"convex\",\n  stand = FALSE,\n  repel = TRUE\n)"
  },
  {
    "objectID": "chapters/ch13_cluster_dbscan.html#예-13.8",
    "href": "chapters/ch13_cluster_dbscan.html#예-13.8",
    "title": "13장 비계층적 군집방법",
    "section": "(예 13.8)",
    "text": "(예 13.8)\n\n패키지 로드\n\nlibrary(fpc)\nlibrary(factoextra)\n\n\n\n데이터 로드\n\ndat1 &lt;- read.csv(\"data/ch12_dat1.csv\")\ndat2 &lt;- dat1[, -1]\n\n\n\n디비스캔\n\ndb &lt;- dbscan(dat2, eps = 2.5, MinPts = 3)\ndb\n\ndbscan Pts=10 MinPts=3 eps=2.5\n       0 1 2 3\nborder 1 0 0 2\nseed   0 3 3 1\ntotal  1 3 3 3\n\n\n군집 결과\n\ndb$cluster\n\n [1] 1 1 2 0 2 1 2 3 3 3\n\n\n\n\n결과 시각화\n\nfviz_cluster(db,\n  data = dat2,\n  ellipse.type = \"convex\",\n  stand = FALSE,\n  repel = TRUE\n)"
  },
  {
    "objectID": "chapters/ch14_cluster_validation.html#예-14.1-14.3",
    "href": "chapters/ch14_cluster_validation.html#예-14.1-14.3",
    "title": "14장 군집해의 평가 및 해석",
    "section": "(예 14.1, 14.3)",
    "text": "(예 14.1, 14.3)\n\n패키지 로드\n\nlibrary(clv)\n\nLoading required package: cluster\n\n\nLoading required package: class\n\n\n\n\n데이터 로드\n\nsol_1 &lt;- as.integer(c(1, 1, 2, 2, 2, 3, 3))\nsol_2 &lt;- as.integer(c(1, 1, 2, 3, 2, 1, 3))\n\n\n\n카운팅\n\nstd &lt;- std.ext(sol_1, sol_2)\nstd\n\n$SS\n[1] 2\n\n$SD\n[1] 3\n\n$DS\n[1] 3\n\n$DD\n[1] 13\n\n\n\n\n(예 14.1) 랜드지수\n\nclv.Rand(std)\n\n[1] 0.7142857\n\n\n\n\n(예 14.3) 수정랜드지수\n\nclv.Phi(std)\n\n[1] 0.2125"
  },
  {
    "objectID": "chapters/ch14_cluster_validation.html#예-14.4",
    "href": "chapters/ch14_cluster_validation.html#예-14.4",
    "title": "14장 군집해의 평가 및 해석",
    "section": "(예 14.4)",
    "text": "(예 14.4)\n\n패키지 로드\n\nlibrary(clv)\n\n\n\n데이터 로드\n\nsol_1 &lt;- as.integer(c(1, 1, 1, 1, 2, 2, 2, 3, 3, 3))\nsol_2 &lt;- as.integer(c(1, 1, 2, 3, 1, 2, 3, 1, 2, 3))\n\n\n\n카운팅\n\nstd &lt;- std.ext(sol_1, sol_2)\nstd\n\n$SS\n[1] 1\n\n$SD\n[1] 11\n\n$DS\n[1] 11\n\n$DD\n[1] 22\n\n\n\n\n랜드지수\n\nclv.Rand(std)\n\n[1] 0.5111111\n\n\n\n\n수정랜드지수\n\nclv.Phi(std)\n\n[1] -0.25"
  },
  {
    "objectID": "chapters/ch14_cluster_validation.html#예-14.5",
    "href": "chapters/ch14_cluster_validation.html#예-14.5",
    "title": "14장 군집해의 평가 및 해석",
    "section": "(예 14.5)",
    "text": "(예 14.5)\n\n패키지 로드\n\nlibrary(clv) # Dunn, connectivity, silhouette\nlibrary(fpc) # Calinski-Harabasz(CH)\n\n\n\n데이터 로드\n데이터\n\ndf &lt;- read.csv(\"data/ch14_dat1.csv\", colClasses = \"numeric\")\nmat &lt;- as.matrix(df)\nD &lt;- dist(mat)\n\n군집해\n\nsol_1 &lt;- as.integer(c(1, 2, 1, 3, 2, 1, 2, 3))\nsol_2 &lt;- as.integer(c(1, 2, 1, 2, 2, 1, 2, 2))\n\n\n\n군집해 분석\n\nclust_1 &lt;- cls.scatt.data(df, sol_1)\nclust_2 &lt;- cls.scatt.data(df, sol_2)\n\n첫 번째 군집해 분석\n\nprint(clust_1)\n\n$intracls.complete\n           c1       c2       c3\n[1,] 5.656854 5.385165 2.236068\n\n$intracls.average\n           c1       c2       c3\n[1,] 4.426029 3.933126 2.236068\n\n$intracls.centroid\n           c1       c2       c3\n[1,] 2.613873 2.375377 1.118034\n\n$intercls.single\n         c1        c2        c3\nc1  0.00000 10.816654 11.180340\nc2 10.81665  0.000000  6.082763\nc3 11.18034  6.082763  0.000000\n\n$intercls.complete\n         c1       c2       c3\nc1  0.00000 17.00000 18.60108\nc2 17.00000  0.00000 13.15295\nc3 18.60108 13.15295  0.00000\n\n$intercls.average\n         c1        c2        c3\nc1  0.00000 14.021500 15.728940\nc2 14.02150  0.000000  9.102761\nc3 15.72894  9.102761  0.000000\n\n$intercls.centroid\n         c1        c2        c3\nc1  0.00000 13.703203 15.692355\nc2 13.70320  0.000000  9.001543\nc3 15.69235  9.001543  0.000000\n\n$intercls.ave_to_cent\n         c1        c2        c3\nc1  0.00000 13.860760 15.711641\nc2 13.86076  0.000000  9.056783\nc3 15.71164  9.056783  0.000000\n\n$intercls.hausdorff\n         c1       c2       c3\nc1  0.00000 14.56022 16.64332\nc2 12.16553  0.00000 11.04536\nc3 13.03840  8.00000  0.00000\n\n$cluster.center\n       [,1] [,2]\nc1  5.00000   13\nc2 18.66667   14\nc3 18.50000    5\n\n$cluster.size\n[1] 3 3 2\n\nattr(,\"class\")\n[1] \"cls.list\"\n\n\n두 번째 군집해 분석\n\nprint(clust_2)\n\n$intracls.complete\n           c1       c2\n[1,] 5.656854 13.15295\n\n$intracls.average\n           c1       c2\n[1,] 4.426029 6.865201\n\n$intracls.centroid\n           c1       c2\n[1,] 2.613873 4.449317\n\n$intercls.single\n         c1       c2\nc1  0.00000 10.81665\nc2 10.81665  0.00000\n\n$intercls.complete\n         c1       c2\nc1  0.00000 18.60108\nc2 18.60108  0.00000\n\n$intercls.average\n         c1       c2\nc1  0.00000 14.70448\nc2 14.70448  0.00000\n\n$intercls.centroid\n        c1      c2\nc1  0.0000 13.8463\nc2 13.8463  0.0000\n\n$intercls.ave_to_cent\n         c1       c2\nc1  0.00000 14.35407\nc2 14.35407  0.00000\n\n$intercls.hausdorff\n        c1       c2\nc1  0.0000 14.56022\nc2 13.0384  0.00000\n\n$cluster.center\n   [,1] [,2]\nc1  5.0 13.0\nc2 18.6 10.4\n\n$cluster.size\n[1] 3 5\n\nattr(,\"class\")\n[1] \"cls.list\"\n\n\n\n\nDunn index\n\nclv.Dunn(clust_1, intracls = \"complete\", intercls = \"single\")\n\n        comp\nsin 1.075291\n\nclv.Dunn(clust_2, intracls = \"complete\", intercls = \"single\")\n\n        comp\nsin 0.822375\n\n\n\n\nCH index\n\ncalinhara(df, sol_1)\n\n[1] 26.45029\n\ncalinhara(df, sol_2)\n\n[1] 15.36218\n\n\n\n\nConnectivity (BI)\n\nconnectivity(df, sol_1, neighbour.num = 1)\n\n[1] 0\n\nconnectivity(df, sol_2, neighbour.num = 1)\n\n[1] 0\n\n\n\n\n실루엣\n\nsummary(silhouette(sol_1, dist = D))[[\"avg.width\"]]\n\n[1] 0.6507364\n\nsummary(silhouette(sol_2, dist = D))[[\"avg.width\"]]\n\n[1] 0.5864226\n\n\n\n\n객체별 실루엣\n군집해 1\n\nsilhouette(sol_1, dist = D)\n\n     cluster neighbor sil_width\n[1,]       1        2 0.7343912\n[2,]       2        3 0.6073450\n[3,]       1        2 0.7597919\n[4,]       3        2 0.7779353\n[5,]       2        3 0.5708309\n[6,]       1        2 0.5132464\n[7,]       2        3 0.5171843\n[8,]       3        2 0.7251663\nattr(,\"Ordered\")\n[1] FALSE\nattr(,\"call\")\nsilhouette.default(x = sol_1, dist = D)\nattr(,\"class\")\n[1] \"silhouette\"\n\n\n군집해 2\n\nsilhouette(sol_2, dist = D)\n\n     cluster neighbor sil_width\n[1,]       1        2 0.7527866\n[2,]       2        1 0.6232664\n[3,]       1        2 0.7690590\n[4,]       2        1 0.5133052\n[5,]       2        1 0.3268174\n[6,]       1        2 0.5260634\n[7,]       2        1 0.6303927\n[8,]       2        1 0.5496900\nattr(,\"Ordered\")\n[1] FALSE\nattr(,\"call\")\nsilhouette.default(x = sol_2, dist = D)\nattr(,\"class\")\n[1] \"silhouette\""
  },
  {
    "objectID": "chapters/ch15_association_rule.html#예-15.3---15.4",
    "href": "chapters/ch15_association_rule.html#예-15.3---15.4",
    "title": "15장 연관규칙",
    "section": "(예 15.3 - 15.4)",
    "text": "(예 15.3 - 15.4)\n\n패키지 로드\n\nlibrary(arules)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'arules'\n\n\nThe following objects are masked from 'package:base':\n\n    abbreviate, write\n\n\n\n\n데이터 로드\n\ndf &lt;- read.csv(\"data/ch15_transaction.csv\")\n\n\n\n데이터 변환\n\n# convert a data frame into a list of transactions\ntransaction_list &lt;- split(df$item, df$id)\nprint(transaction_list)\n\n$`1`\n[1] \"b\" \"c\" \"g\"\n\n$`2`\n[1] \"a\" \"b\" \"d\" \"e\" \"f\"\n\n$`3`\n[1] \"a\" \"b\" \"c\" \"g\"\n\n$`4`\n[1] \"b\" \"c\" \"e\" \"f\"\n\n$`5`\n[1] \"b\" \"c\" \"e\" \"f\" \"g\"\n\n# convert a list into an object of class \"transactions\"\ntransactions &lt;- as(transaction_list, \"transactions\")\nprint(transactions)\n\ntransactions in sparse format with\n 5 transactions (rows) and\n 7 items (columns)\n\n\n\n\n(예 15.3) 빈발항목집합 생성: 최소 지지도 0.4\n\nitemsets &lt;- apriori(\n  transactions,\n  parameter = list(\n    support = 0.4,\n    target = \"frequent itemsets\"\n  ),\n  control = list(verbose = FALSE)  # do not print progress\n)\n\ninspect(itemsets)\n\n     items        support count\n[1]  {a}          0.4     2    \n[2]  {g}          0.6     3    \n[3]  {e}          0.6     3    \n[4]  {f}          0.6     3    \n[5]  {c}          0.8     4    \n[6]  {b}          1.0     5    \n[7]  {a, b}       0.4     2    \n[8]  {c, g}       0.6     3    \n[9]  {b, g}       0.6     3    \n[10] {e, f}       0.6     3    \n[11] {c, e}       0.4     2    \n[12] {b, e}       0.6     3    \n[13] {c, f}       0.4     2    \n[14] {b, f}       0.6     3    \n[15] {b, c}       0.8     4    \n[16] {b, c, g}    0.6     3    \n[17] {c, e, f}    0.4     2    \n[18] {b, e, f}    0.6     3    \n[19] {b, c, e}    0.4     2    \n[20] {b, c, f}    0.4     2    \n[21] {b, c, e, f} 0.4     2    \n\n\n\n\n(예 15.4) 규칙 탐사: 최소 신뢰도 0.7\n\nrules &lt;- ruleInduction(\n  itemsets,\n  transactions,\n  confidence = 0.7,\n  method = \"apriori\"\n)\n\ninspect(rules)\n\n     lhs          rhs support confidence coverage lift     count\n[1]  {}        =&gt; {c} 0.8     0.80       1.0      1.000000 4    \n[2]  {}        =&gt; {b} 1.0     1.00       1.0      1.000000 5    \n[3]  {a}       =&gt; {b} 0.4     1.00       0.4      1.000000 2    \n[4]  {g}       =&gt; {c} 0.6     1.00       0.6      1.250000 3    \n[5]  {c}       =&gt; {g} 0.6     0.75       0.8      1.250000 3    \n[6]  {g}       =&gt; {b} 0.6     1.00       0.6      1.000000 3    \n[7]  {e}       =&gt; {f} 0.6     1.00       0.6      1.666667 3    \n[8]  {f}       =&gt; {e} 0.6     1.00       0.6      1.666667 3    \n[9]  {e}       =&gt; {b} 0.6     1.00       0.6      1.000000 3    \n[10] {f}       =&gt; {b} 0.6     1.00       0.6      1.000000 3    \n[11] {c}       =&gt; {b} 0.8     1.00       0.8      1.000000 4    \n[12] {b}       =&gt; {c} 0.8     0.80       1.0      1.000000 4    \n[13] {c, g}    =&gt; {b} 0.6     1.00       0.6      1.000000 3    \n[14] {b, g}    =&gt; {c} 0.6     1.00       0.6      1.250000 3    \n[15] {b, c}    =&gt; {g} 0.6     0.75       0.8      1.250000 3    \n[16] {c, e}    =&gt; {f} 0.4     1.00       0.4      1.666667 2    \n[17] {c, f}    =&gt; {e} 0.4     1.00       0.4      1.666667 2    \n[18] {e, f}    =&gt; {b} 0.6     1.00       0.6      1.000000 3    \n[19] {b, e}    =&gt; {f} 0.6     1.00       0.6      1.666667 3    \n[20] {b, f}    =&gt; {e} 0.6     1.00       0.6      1.666667 3    \n[21] {c, e}    =&gt; {b} 0.4     1.00       0.4      1.000000 2    \n[22] {c, f}    =&gt; {b} 0.4     1.00       0.4      1.000000 2    \n[23] {c, e, f} =&gt; {b} 0.4     1.00       0.4      1.000000 2    \n[24] {b, c, e} =&gt; {f} 0.4     1.00       0.4      1.666667 2    \n[25] {b, c, f} =&gt; {e} 0.4     1.00       0.4      1.666667 2    \n\n\n\n\n빈발항목집합 + 규칙탐사\n\napriori_results &lt;- apriori(\n  transactions,\n  parameter = list(\n    support = 0.4,\n    confidence = 0.7,\n    target = \"rules\"\n  ),\n  control = list(verbose = FALSE)  # do not print progress\n)\n\ninspect(apriori_results)\n\n     lhs          rhs support confidence coverage lift     count\n[1]  {}        =&gt; {c} 0.8     0.80       1.0      1.000000 4    \n[2]  {}        =&gt; {b} 1.0     1.00       1.0      1.000000 5    \n[3]  {a}       =&gt; {b} 0.4     1.00       0.4      1.000000 2    \n[4]  {g}       =&gt; {c} 0.6     1.00       0.6      1.250000 3    \n[5]  {c}       =&gt; {g} 0.6     0.75       0.8      1.250000 3    \n[6]  {g}       =&gt; {b} 0.6     1.00       0.6      1.000000 3    \n[7]  {e}       =&gt; {f} 0.6     1.00       0.6      1.666667 3    \n[8]  {f}       =&gt; {e} 0.6     1.00       0.6      1.666667 3    \n[9]  {e}       =&gt; {b} 0.6     1.00       0.6      1.000000 3    \n[10] {f}       =&gt; {b} 0.6     1.00       0.6      1.000000 3    \n[11] {c}       =&gt; {b} 0.8     1.00       0.8      1.000000 4    \n[12] {b}       =&gt; {c} 0.8     0.80       1.0      1.000000 4    \n[13] {c, g}    =&gt; {b} 0.6     1.00       0.6      1.000000 3    \n[14] {b, g}    =&gt; {c} 0.6     1.00       0.6      1.250000 3    \n[15] {b, c}    =&gt; {g} 0.6     0.75       0.8      1.250000 3    \n[16] {c, e}    =&gt; {f} 0.4     1.00       0.4      1.666667 2    \n[17] {c, f}    =&gt; {e} 0.4     1.00       0.4      1.666667 2    \n[18] {e, f}    =&gt; {b} 0.6     1.00       0.6      1.000000 3    \n[19] {b, e}    =&gt; {f} 0.6     1.00       0.6      1.666667 3    \n[20] {b, f}    =&gt; {e} 0.6     1.00       0.6      1.666667 3    \n[21] {c, e}    =&gt; {b} 0.4     1.00       0.4      1.000000 2    \n[22] {c, f}    =&gt; {b} 0.4     1.00       0.4      1.000000 2    \n[23] {c, e, f} =&gt; {b} 0.4     1.00       0.4      1.000000 2    \n[24] {b, c, e} =&gt; {f} 0.4     1.00       0.4      1.666667 2    \n[25] {b, c, f} =&gt; {e} 0.4     1.00       0.4      1.666667 2"
  },
  {
    "objectID": "chapters/ch15_association_rule.html#예-15.7---15.9",
    "href": "chapters/ch15_association_rule.html#예-15.7---15.9",
    "title": "15장 연관규칙",
    "section": "(예 15.7 - 15.9)",
    "text": "(예 15.7 - 15.9)\n본 R코드는 교재에 나온 AprioriAll이나 AprioriSome이 아닌, SPADE라는 알고리즘을 이용하여 구현한 R 패키지를 사용하였습니다. 수행 결과는 동일하며, SPADE 알고리즘이 효율성이 더 높은 것으로 알려져 있습니다.\nSPADE 알고리즘은 아래 논문에 소개되어 있습니다.\nZaki, M. J. (2001). SPADE: An efficient algorithm for mining frequent sequences. Machine learning, 42, 31-60.\n\n패키지 로드\n\nlibrary(arulesSequences)\n\n\nAttaching package: 'arulesSequences'\n\n\nThe following object is masked from 'package:arules':\n\n    itemsets\n\n\n\n\n데이터 로드\n\n# read specifically formatted data into transactions with temporal information\ntrans &lt;- read_baskets(\n  con = \"data/ch15_sequence.txt\",\n  info = c(\"sequenceID\", \"eventID\", \"SIZE\") # do not change the names\n)\n\n\n\n(예 15.7) 최대 시퀀스\n모든 시퀀스 (최소 지지도 0.4)\n\nseqs &lt;- cspade(trans, parameter = list(support = 0.4))\ninspect(seqs)\n\n   items support \n 1 &lt;{a}&gt;     0.8 \n 2 &lt;{b}&gt;     0.6 \n 3 &lt;{e}&gt;     0.4 \n 4 &lt;{g}&gt;     0.6 \n 5 &lt;{a},  \n    {g}&gt;     0.4 \n 6 &lt;{e,   \n     g}&gt;     0.4 \n 7 &lt;{a},  \n    {e,   \n     g}&gt;     0.4 \n 8 &lt;{a},  \n    {e}&gt;     0.4 \n 9 &lt;{a},  \n    {b}&gt;     0.4 \n \n\n\n최대 시퀀스 (최소 지지도 0.4)\n\nmax_seqs &lt;- subset(seqs, is.maximal(seqs))\ninspect(max_seqs)\n\n   items support \n 1 &lt;{a},  \n    {e,   \n     g}&gt;     0.4 \n 2 &lt;{a},  \n    {b}&gt;     0.4 \n \n\n\n\n\n(예 15.8) 빈발항목집합\n\nitemsets &lt;- cspade(trans, parameter = list(support = 0.4, maxlen = 1))\ninspect(itemsets)\n\n   items support \n 1 &lt;{a}&gt;     0.8 \n 2 &lt;{b}&gt;     0.6 \n 3 &lt;{e}&gt;     0.4 \n 4 &lt;{g}&gt;     0.6 \n 5 &lt;{e,   \n     g}&gt;     0.4 \n \n\n\n\n\n(예 15.9) 시퀀스 탐색\n모든 빈발 시퀀스 (최소 신뢰도 0.5)\n\nrules &lt;- ruleInduction(seqs, confidence = 0.5)\ninspect(rules)\n\n   lhs      rhs   support confidence      lift \n 1 &lt;{a}&gt; =&gt; &lt;{g}&gt;     0.4        0.5 0.8333333 \n 2 &lt;{a}&gt; =&gt; &lt;{e,      0.4        0.5 1.2500000 \n              g}&gt;    \n 3 &lt;{a}&gt; =&gt; &lt;{e}&gt;     0.4        0.5 1.2500000 \n 4 &lt;{a}&gt; =&gt; &lt;{b}&gt;     0.4        0.5 0.8333333 \n \n\n\n최대 빈발 시퀀스 (최소 신뢰도 0.5)\n\nmax_rules &lt;- ruleInduction(max_seqs, trans, confidence = 0.5)\n\nWarning in match(x@items, table@items, nomatch = nomatch, incomparables =\nincomparables): Item coding not compatible, recoding item matrices first.\n\n\nWarning in .local(x, ...): Item coding not compatible, recoding item matrices.\n\ninspect(max_rules)\n\n   lhs      rhs   support confidence      lift \n 1 &lt;{a}&gt; =&gt; &lt;{e,      0.4        0.5 1.2500000 \n              g}&gt;    \n 2 &lt;{a}&gt; =&gt; &lt;{b}&gt;     0.4        0.5 0.8333333"
  },
  {
    "objectID": "chapters/ch16_recommender_system.html#예-16.2",
    "href": "chapters/ch16_recommender_system.html#예-16.2",
    "title": "16장 추천시스템",
    "section": "(예 16.2)",
    "text": "(예 16.2)\n\n데이터 로드\n\ndf &lt;- read.csv(\"data/ch16_content.csv\")\nuser_weights &lt;- as.vector(df[, 2])\ndoc_weights &lt;- as.matrix(df[, -c(1, 2)])\n\n\n\n유용도 산출\n\nnumerator &lt;- t(user_weights) %*% doc_weights\ndenominator &lt;- norm(user_weights, type = \"2\") *\n  apply(doc_weights, 2, norm, type = \"2\")\nutility &lt;- numerator / denominator\nround(utility, 4)\n\n       doc1   doc2   doc3   doc4   doc5   doc6\n[1,] 0.6595 0.8409 0.9189 0.4478 0.9717 0.3728"
  },
  {
    "objectID": "chapters/ch16_recommender_system.html#예-16.3",
    "href": "chapters/ch16_recommender_system.html#예-16.3",
    "title": "16장 추천시스템",
    "section": "(예 16.3)",
    "text": "(예 16.3)\n\n패키지 로드\n\nlibrary(\"recommenderlab\")\n\nLoading required package: Matrix\n\n\nLoading required package: arules\n\n\n\nAttaching package: 'arules'\n\n\nThe following objects are masked from 'package:base':\n\n    abbreviate, write\n\n\nLoading required package: proxy\n\n\n\nAttaching package: 'proxy'\n\n\nThe following object is masked from 'package:Matrix':\n\n    as.matrix\n\n\nThe following objects are masked from 'package:stats':\n\n    as.dist, dist\n\n\nThe following object is masked from 'package:base':\n\n    as.matrix\n\n\nRegistered S3 methods overwritten by 'registry':\n  method               from \n  print.registry_field proxy\n  print.registry_entry proxy\n\n\n\n\n데이터 로드\n\ndf &lt;- read.csv(\"data/ch16_ratings.csv\")\n\n\n\n평점 행렬로 변환\n\nratings &lt;- as(df, \"realRatingMatrix\")\nprint(ratings@data)\n\n7 x 7 sparse Matrix of class \"dgCMatrix\"\n  1 2 3 4 5 6 7\n1 5 . 4 . 1 0 3\n2 4 4 4 . . . 1\n3 5 4 . 1 2 . 3\n4 1 2 1 4 3 5 2\n5 0 1 . 3 5 5 .\n6 . 2 . . 4 4 2\n7 5 . . 1 . . 2\n\n\n\n\n목표고객 설정\n\ntarget &lt;- 7\n\n\n\n목표고객과 각 고객간의 유사성 산출\n\ncentered &lt;- normalize(ratings)\nsimilarity(centered[-target], centered[target])\n\n           7\n1 0.95163875\n2 0.78234196\n3 0.98038446\n4 0.06238169\n5 0.07346149\n6 1.00000000\n\n\n\n\n협업 필터링 추천 시스템 생성\n\nrec &lt;- Recommender(ratings, method = \"UBCF\")\nrec\n\nRecommender of type 'UBCF' for 'realRatingMatrix' \nlearned using 7 users.\n\n\n\n\n평점 추정\n\npredicted &lt;- predict(rec, ratings[target], type = \"ratings\")\nprint(predicted@data)\n\n1 x 7 sparse Matrix of class \"dgCMatrix\"\n  1        2        3 4        5        6 7\n7 . 2.804412 3.680394 . 2.238142 2.110424 .\n\n\n\n\n상위-N 추천\n\nrecommended &lt;- predict(rec, ratings[target], n = 2)\nprint(recommended@items)\n\n$`0`\n[1] 3 2"
  },
  {
    "objectID": "chapters/ch16_recommender_system.html#예-16.4",
    "href": "chapters/ch16_recommender_system.html#예-16.4",
    "title": "16장 추천시스템",
    "section": "(예 16.4)",
    "text": "(예 16.4)\n\n패키지 로드\n\nlibrary(\"recommenderlab\")\n\n\n\n데이터 로드\n\ndf &lt;- read.csv(\"data/ch16_purchase.csv\")\n\n\n\n시장바구니 데이터 행렬로 변환\n\npurchases &lt;- as(df, \"binaryRatingMatrix\")\nmat &lt;- as(purchases@data, \"matrix\")\nmat\n\n      1     2     3     4     5     6     7\n1  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE\n2  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE\n3  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE\n4  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n5 FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE\n6 FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n7  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE\n\n\n\n\n목표고객 설정\n\ntarget &lt;- 7\n\n\n\n목표고객과 각 고객간의 유사성 산출\n\nsim &lt;- similarity(purchases[-target], purchases[target], method = \"jaccard\")\nround(sim, 4)\n\n       7\n1 0.4000\n2 0.4000\n3 0.6000\n4 0.5000\n5 0.1667\n6 0.1667\n\n\n\n\n평점 추정\n\ntarget_items &lt;- !mat[target, ]\npredicted &lt;- 1 / sum(sim) *\n  t(sim) %*% as(purchases[-target], \"matrix\")[, target_items]\nround(predicted, 4)\n\n       2      3     5      6\n7 0.8209 0.5821 0.597 0.3731"
  }
]